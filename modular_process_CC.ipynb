{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Converting center coordinates (27.783611, -97.414779) to UTM...\n",
      "Corpus Christi center in UTM (EPSG:32614): (656184.519263486, 3074239.5261650323)\n",
      "- Checking cache for blocks...\n",
      "Loading data from cache: cache\\blocks.pkl\n",
      "- Checking cache for buildings...\n",
      "Loading data from cache: cache\\buildings.pkl\n",
      "- Checking cache for roads...\n",
      "Loading data from cache: cache\\roads.pkl\n",
      "- Subsampling datasets...\n",
      "- Subsampling data using bounding box...\n",
      "-- Bounding box geometry: POLYGON ((656489.3192634861 3073934.7261650325, 656489.3192634861 3074544.326165032, 655879.719263486 3074544.326165032, 655879.719263486 3073934.7261650325, 656489.3192634861 3073934.7261650325))\n",
      "-- Blocks CRS: EPSG:32614\n",
      "-- Buildings CRS before reprojection: EPSG:32614\n",
      "-- Roads CRS: EPSG:32614\n",
      "-- Buildings CRS after reprojection: EPSG:32614\n",
      "-- Buildings subset shape after intersects: (405, 198)\n",
      "-- Subsampled 35 blocks, 405 buildings, and 126 roads.\n",
      "- Data loading complete.\n",
      "Calculating building setbacks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\2730296908.py:191: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Blocks subset:\n",
      "       STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "316275        48        355    001000      2012  483550010002012   \n",
      "359470        48        355    001000      1017  483550010001017   \n",
      "359630        48        355    001000      3008  483550010003008   \n",
      "359631        48        355    001000      2022  483550010002022   \n",
      "367172        48        355    001000      1019  483550010001019   \n",
      "\n",
      "                       GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20 FUNCSTAT20  \\\n",
      "316275  1000000US483550010002012  Block2012   G5040    U  20287          S   \n",
      "359470  1000000US483550010001017  Block1017   G5040    U  20287          S   \n",
      "359630  1000000US483550010003008  Block3008   G5040    U  20287          S   \n",
      "359631  1000000US483550010002022  Block2022   G5040    U  20287          S   \n",
      "367172  1000000US483550010001019  Block1019   G5040    U  20287          S   \n",
      "\n",
      "        ALAND20  AWATER20   INTPTLAT20    INTPTLON20  HOUSING20  POP20  \\\n",
      "316275     5968         0  +27.7861080  -097.4140292          4      9   \n",
      "359470     8667         0  +27.7832089  -097.4156547         22     40   \n",
      "359630     9406         0  +27.7809086  -097.4147305         23     48   \n",
      "359631    11640         0  +27.7814295  -097.4128967         15     47   \n",
      "367172     8522         0  +27.7831790  -097.4176849         14     25   \n",
      "\n",
      "                                                 geometry  \n",
      "316275  POLYGON ((656199.608 3074521.411, 656199.421 3...  \n",
      "359470  POLYGON ((656049.45 3074166.753, 656048.773 30...  \n",
      "359630  POLYGON ((656055.843 3073953.629, 656110.426 3...  \n",
      "359631  POLYGON ((656327.708 3073935.194, 656328.032 3...  \n",
      "367172  POLYGON ((655847.886 3074229.204, 655867.887 3...  \n",
      "-- Buildings subset:\n",
      "                                                            geometry  \\\n",
      "element id                                                             \n",
      "way     530186194  POLYGON ((656076.241 3074294.955, 656065.422 3...   \n",
      "        530186213  POLYGON ((656091.511 3074296.194, 656081.776 3...   \n",
      "        530186246  POLYGON ((656061.354 3074266.262, 656054.262 3...   \n",
      "        530186263  POLYGON ((656115.404 3074279.115, 656115.533 3...   \n",
      "        530186328  POLYGON ((656117.974 3074309.833, 656118.235 3...   \n",
      "\n",
      "                  addr:state building  ele gnis:feature_id name source  \\\n",
      "element id                                                               \n",
      "way     530186194        NaN      yes  NaN             NaN  NaN    NaN   \n",
      "        530186213        NaN      yes  NaN             NaN  NaN    NaN   \n",
      "        530186246        NaN      yes  NaN             NaN  NaN    NaN   \n",
      "        530186263        NaN      yes  NaN             NaN  NaN    NaN   \n",
      "        530186328        NaN      yes  NaN             NaN  NaN    NaN   \n",
      "\n",
      "                  addr:city addr:housename addr:housenumber  ...  \\\n",
      "element id                                                   ...   \n",
      "way     530186194       NaN            NaN              NaN  ...   \n",
      "        530186213       NaN            NaN              NaN  ...   \n",
      "        530186246       NaN            NaN              NaN  ...   \n",
      "        530186263       NaN            NaN              NaN  ...   \n",
      "        530186328       NaN            NaN              NaN  ...   \n",
      "\n",
      "                  fuel:octane_87 fuel:octane_89 fuel:octane_93 female male  \\\n",
      "element id                                                                   \n",
      "way     530186194            NaN            NaN            NaN    NaN  NaN   \n",
      "        530186213            NaN            NaN            NaN    NaN  NaN   \n",
      "        530186246            NaN            NaN            NaN    NaN  NaN   \n",
      "        530186263            NaN            NaN            NaN    NaN  NaN   \n",
      "        530186328            NaN            NaN            NaN    NaN  NaN   \n",
      "\n",
      "                  portable toilets:handwashing capacity size  setback_ft  \n",
      "element id                                                                \n",
      "way     530186194      NaN                 NaN      NaN  NaN   35.848671  \n",
      "        530186213      NaN                 NaN      NaN  NaN   44.730535  \n",
      "        530186246      NaN                 NaN      NaN  NaN   25.398193  \n",
      "        530186263      NaN                 NaN      NaN  NaN  103.380038  \n",
      "        530186328      NaN                 NaN      NaN  NaN   41.409583  \n",
      "\n",
      "[5 rows x 199 columns]\n",
      "-- Roads subset:\n",
      "                            osmid      highway lanes               name  \\\n",
      "u         v         key                                                   \n",
      "227494180 227525355 0    21153170  residential   NaN  Marguerite Street   \n",
      "227494197 227533820 0    21154066  residential   NaN        Mary Street   \n",
      "227494201 227572969 0    21157903  residential   NaN      Morris Street   \n",
      "227494205 227563929 0    89958916  residential   NaN     Coleman Avenue   \n",
      "227494208 227502399 0    21150792     tertiary   NaN        Ruth Street   \n",
      "\n",
      "                         oneway  ref reversed      length maxspeed  \\\n",
      "u         v         key                                              \n",
      "227494180 227525355 0     False  NaN     True  281.132210      NaN   \n",
      "227494197 227533820 0     False  NaN     True   98.376559      NaN   \n",
      "227494201 227572969 0     False  NaN    False  101.324854      NaN   \n",
      "227494205 227563929 0     False  NaN     True  100.119788      NaN   \n",
      "227494208 227502399 0     False  NaN     True  100.178653      NaN   \n",
      "\n",
      "                                                                  geometry  \\\n",
      "u         v         key                                                      \n",
      "227494180 227525355 0    LINESTRING (655810.207 3074514.941, 656046.746...   \n",
      "227494197 227533820 0    LINESTRING (655842.677 3074420.28, 655941.207 ...   \n",
      "227494201 227572969 0    LINESTRING (655843.445 3074315.449, 655944.914...   \n",
      "227494205 227563929 0    LINESTRING (655846.291 3074230.635, 655946.584...   \n",
      "227494208 227502399 0    LINESTRING (655848.507 3074145.016, 655948.821...   \n",
      "\n",
      "                        bridge junction width access  \n",
      "u         v         key                               \n",
      "227494180 227525355 0      NaN      NaN   NaN    NaN  \n",
      "227494197 227533820 0      NaN      NaN   NaN    NaN  \n",
      "227494201 227572969 0      NaN      NaN   NaN    NaN  \n",
      "227494205 227563929 0      NaN      NaN   NaN    NaN  \n",
      "227494208 227502399 0      NaN      NaN   NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "### Utility Functions ###\n",
    "def read_path_from_file(file_path: str) -> str:\n",
    "    \"\"\"Read OneDrive path from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            path = file.readline().strip()  # Read the first line and strip whitespace\n",
    "        return path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_cache(data, filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Save a GeoDataFrame or Python object to a pickle file in the cache directory.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    print(f\"Saving data to cache: {cache_path}\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_from_cache(filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Load a GeoDataFrame or Python object from a pickle file in the cache directory.\"\"\"\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading data from cache: {cache_path}\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def latlon_to_utm(lat: float, lon: float, epsg: int = 32614):\n",
    "    \"\"\"\n",
    "    Convert latitude and longitude (EPSG:4326) to UTM coordinates (e.g., EPSG:32614).\n",
    "    \"\"\"\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg}\", always_xy=True)\n",
    "    x, y = transformer.transform(lon, lat)  # Transform to target coordinates\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def subsample_data(center_point: tuple, scale: float, blocks: gpd.GeoDataFrame, buildings: gpd.GeoDataFrame, roads: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Subsample blocks, buildings, and roads using a bounding box around a central point.\n",
    "    Ensures proper alignment of CRS before subsampling.\n",
    "    \"\"\"\n",
    "    print(\"- Subsampling data using bounding box...\")\n",
    "\n",
    "    # Default bounding box size: ~10,000 feet (~3,048 meters) on each side\n",
    "    default_bbox_size = 3048  # Half the total size on each dimension, in meters\n",
    "    scaled_bbox_size = default_bbox_size * scale\n",
    "\n",
    "    # Calculate bounding box geometry based on scaled size\n",
    "    center_x, center_y = center_point\n",
    "    bbox = box(\n",
    "        center_x - scaled_bbox_size,  # Min X\n",
    "        center_y - scaled_bbox_size,  # Min Y\n",
    "        center_x + scaled_bbox_size,  # Max X\n",
    "        center_y + scaled_bbox_size   # Max Y\n",
    "    )\n",
    "    print(\"-- Bounding box geometry:\", bbox)\n",
    "\n",
    "    # Debugging: Check CRS alignment for all datasets\n",
    "    print(\"-- Blocks CRS:\", blocks.crs)\n",
    "    print(\"-- Buildings CRS before reprojection:\", buildings.crs)\n",
    "    print(\"-- Roads CRS:\", roads.crs)\n",
    "\n",
    "    # Reproject buildings to match the CRS of the bounding box (EPSG:32614)\n",
    "    if buildings.crs.to_string().lower() != \"epsg:32614\":\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "    print(\"-- Buildings CRS after reprojection:\", buildings.crs)\n",
    "\n",
    "    # Filter each GeoDataFrame by the bounding box intersection\n",
    "    blocks_subset = blocks[blocks.geometry.intersects(bbox)]\n",
    "    buildings_subset = buildings[buildings.geometry.intersects(bbox)]\n",
    "    roads_subset = roads[roads.geometry.intersects(bbox)]\n",
    "\n",
    "    # Debugging: Check if any buildings are retained in the subset\n",
    "    print(\"-- Buildings subset shape after intersects:\", buildings_subset.shape)\n",
    "    if len(buildings_subset) == 0:\n",
    "        print(\"-- Warning: No buildings found within the bounding box.\")\n",
    "        print(\"-- Original buildings dataset sample (after reprojection):\")\n",
    "        print(buildings.geometry.head())  # Inspect original geometries\n",
    "\n",
    "    print(f\"-- Subsampled {len(blocks_subset)} blocks, {len(buildings_subset)} buildings, and {len(roads_subset)} roads.\")\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "def load_data(block_path, osm_boundary_place, cache_dir=\"cache\", subsample_scale=1.0):\n",
    "    \"\"\"\n",
    "    Load blocks, buildings, and roads data from cache (if available), process, and optionally subsample using a scaled bounding box.\n",
    "    Uses only the 'height' column for building heights and outputs the percentage of valid values.\n",
    "    \"\"\"\n",
    "    # Corpus Christi center point from Google Maps\n",
    "    corpus_christi_lat = 27.783611  # Latitude\n",
    "    corpus_christi_lon = -97.414779  # Longitude\n",
    "\n",
    "    print(f\"Converting center coordinates ({corpus_christi_lat}, {corpus_christi_lon}) to UTM...\")\n",
    "    corpus_christi_center_utm = latlon_to_utm(corpus_christi_lat, corpus_christi_lon)  # Convert to UTM (EPSG:32614)\n",
    "    CC_Padre_center_utm = latlon_to_utm(27.614956, -97.269562)\n",
    "    print(f\"Corpus Christi center in UTM (EPSG:32614): {corpus_christi_center_utm}\")\n",
    "\n",
    "    # Check cache for blocks\n",
    "    print(\"- Checking cache for blocks...\")\n",
    "    blocks = load_from_cache(\"blocks.pkl\", cache_dir)\n",
    "    if blocks is None:\n",
    "        print(\"-- Loading block data...\")\n",
    "        blocks = gpd.read_file(block_path).to_crs(epsg=32614)\n",
    "        blocks.loc[:, \"POP20\"] = pd.to_numeric(blocks[\"POP20\"], errors=\"coerce\")\n",
    "        save_to_cache(blocks, \"blocks.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for buildings\n",
    "    print(\"- Checking cache for buildings...\")\n",
    "    buildings = load_from_cache(\"buildings.pkl\", cache_dir)\n",
    "    if buildings is None:\n",
    "        print(\"-- Fetching building data...\")\n",
    "        buildings = ox.features_from_place(\n",
    "            osm_boundary_place,\n",
    "            tags={\"building\": True, \"building:height\": True, \"building:levels\": True}\n",
    "        )\n",
    "\n",
    "        print(\"-- Raw buildings dataset columns:\")\n",
    "        print(buildings.columns)  # Print available columns for validation\n",
    "        print(\"-- Preview of raw building dataset:\")\n",
    "        print(buildings.head())  # Inspect raw data for possible errors\n",
    "\n",
    "        if \"height\" in buildings.columns:\n",
    "            print(\"-- 'height' column found. Attempting numerical conversion...\")\n",
    "            buildings.loc[:, \"height\"] = pd.to_numeric(buildings[\"height\"], errors=\"coerce\")\n",
    "\n",
    "            # Debugging step to Calculate percentage of valid values (>0 and numeric)\n",
    "            total_height_values = len(buildings)\n",
    "            valid_height_values = buildings[\"height\"].dropna().gt(0).sum()\n",
    "            percentage_valid = (valid_height_values / total_height_values) * 100 if total_height_values > 0 else 0\n",
    "            print(f\"-- Valid height values: {valid_height_values} / Total: {total_height_values} ({percentage_valid:.2f}%)\")\n",
    "\n",
    "            print(\"-- Converted 'height' values to numeric. Example values:\")\n",
    "            print(buildings[\"height\"].head())\n",
    "        else:\n",
    "            print(\"-- ERROR: 'height' column is missing in the dataset! Setting height to None.\")\n",
    "            buildings.loc[:, \"height\"] = None\n",
    "\n",
    "        # Reproject buildings to CRS: EPSG:32614\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "        # Save the buildings dataset to cache\n",
    "        save_to_cache(buildings, \"buildings.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for roads\n",
    "    print(\"- Checking cache for roads...\")\n",
    "    roads = load_from_cache(\"roads.pkl\", cache_dir)\n",
    "    if roads is None:\n",
    "        print(\"-- Fetching road data...\")\n",
    "        roads = ox.graph_to_gdfs(ox.graph_from_place(osm_boundary_place, network_type=\"drive\"), nodes=False)\n",
    "        roads = roads.to_crs(epsg=32614)\n",
    "        save_to_cache(roads, \"roads.pkl\", cache_dir)\n",
    "\n",
    "    # Subsample datasets using the bounding box around Corpus Christi center in UTM\n",
    "    print(\"- Subsampling datasets...\")\n",
    "    # CC Padre Island as center of box\n",
    "    #blocks_subset, buildings_subset, roads_subset = subsample_data(CC_Padre_center_utm, subsample_scale, blocks, buildings, roads)\n",
    "    # CC Corpus Christi Downtown as the center of box\n",
    "    blocks_subset, buildings_subset, roads_subset = subsample_data(corpus_christi_center_utm, subsample_scale, blocks, buildings, roads)\n",
    "    print(\"- Data loading complete.\")\n",
    "\n",
    "    '''\n",
    "    # IF you want to calculate all of Corpus Christi, uncomment the lines below and comment out the lines above for subsampling\n",
    "    blocks_subset = blocks\n",
    "    building_subset = buildings\n",
    "    road_subset = roads\n",
    "    '''\n",
    "    \n",
    "    # Calculate building setbacks (distance to nearest road)\n",
    "    print(\"Calculating building setbacks...\")\n",
    "    roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
    "    buildings_subset[\"setback_ft\"] = buildings_subset.geometry.apply(\n",
    "        lambda building_geom: building_geom.distance(roads_union) * 3.28084  # Convert meters to feet\n",
    "    )\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "# Define paths to block shapefile and Corpus Christi boundary\n",
    "one_drive_path = read_path_from_file(\"OneDrive.txt\")\n",
    "block_path = os.path.join(one_drive_path, \"Data\", \"tl_2023_48_tabblock20\", \"tl_2023_48_tabblock20.shp\")\n",
    "osm_boundary_place = \"Corpus Christi, Texas, USA\"\n",
    "\n",
    "# Load data with subsampling\n",
    "print(\"Starting data loading...\")\n",
    "blocks_subset, buildings_subset, roads_subset = load_data(block_path, osm_boundary_place, subsample_scale=0.1) # CHANGE SCALE OF BOUNDING BOX HERE\n",
    "\n",
    "# Inspect subsampled data\n",
    "print(\"-- Blocks subset:\")\n",
    "print(blocks_subset.head())\n",
    "print(\"-- Buildings subset:\")\n",
    "print(buildings_subset.head())\n",
    "print(\"-- Roads subset:\")\n",
    "print(roads_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in buildings dataset:\n",
      "Index(['geometry', 'addr:state', 'building', 'ele', 'gnis:feature_id', 'name',\n",
      "       'source', 'addr:city', 'addr:housename', 'addr:housenumber',\n",
      "       ...\n",
      "       'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male',\n",
      "       'portable', 'toilets:handwashing', 'capacity', 'size', 'setback_ft'],\n",
      "      dtype='object', length=199)\n",
      "Calculating block metrics...\n",
      "Calculating building area coverage (sq-mile)...\n",
      "Processing building heights...\n",
      "Associating buildings with blocks via spatial join...\n",
      "Aggregating building metrics per block...\n",
      "Calculating population density and other metrics for blocks...\n",
      "Calculating additional block metrics: Building count per square mile, building area percentage, and households per building count...\n",
      "Calculating Households per Building Count (HHpBLD)...\n",
      "Merging building metrics into block dataset...\n",
      "Filling missing values for block metrics...\n",
      "Block metrics calculated successfully.\n",
      "Block metrics successfully calculated:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001000      2012  483550010002012   \n",
      "1        48        355    001000      1017  483550010001017   \n",
      "2        48        355    001000      3008  483550010003008   \n",
      "3        48        355    001000      2022  483550010002022   \n",
      "4        48        355    001000      1019  483550010001019   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... BLOCK_ID  \\\n",
      "0  1000000US483550010002012  Block2012   G5040    U  20287  ...   316275   \n",
      "1  1000000US483550010001017  Block1017   G5040    U  20287  ...   359470   \n",
      "2  1000000US483550010003008  Block3008   G5040    U  20287  ...   359630   \n",
      "3  1000000US483550010002022  Block2022   G5040    U  20287  ...   359631   \n",
      "4  1000000US483550010001019  Block1019   G5040    U  20287  ...   367172   \n",
      "\n",
      "    area_sm       pop_den   avg_hght   avg_sback  bld_area  bld_cnt  \\\n",
      "0  0.002304   3906.448145  32.020998   54.403313  0.000178      4.0   \n",
      "1  0.003346  11956.064635  28.935079   36.253966  0.000724     17.0   \n",
      "2  0.003631  13219.247586  33.207569  179.262686  0.000613      6.0   \n",
      "3  0.004493  10460.225254  32.515175   57.344657  0.001073     16.0   \n",
      "4  0.003290   7599.159707  28.808758   40.860291  0.000402     11.0   \n",
      "\n",
      "      bld_ctsm    bld_prc    HHpBLD  \n",
      "0  1736.199176   7.719694  1.000000  \n",
      "1  5081.327470  21.644454  1.294118  \n",
      "2  1652.405948  16.873311  3.833333  \n",
      "3  3560.927746  23.886743  0.937500  \n",
      "4  3343.630271  12.214263  1.272727  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_block_metrics(blocks_subset: gpd.GeoDataFrame, buildings_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate block-level metrics, including a new HHpBLD metric based on households per building count.\n",
    "    \"\"\"\n",
    "    print(\"Available columns in buildings dataset:\")\n",
    "    print(buildings_subset.columns)\n",
    "    # Check if required columns exist in the buildings dataset\n",
    "    required_columns = [\"geometry\", \"height\"]\n",
    "    missing_columns = [col for col in required_columns if col not in buildings_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing required columns in buildings dataset: {missing_columns}\")\n",
    "        print(\"Preview of buildings dataset:\")\n",
    "        print(buildings_subset.head())\n",
    "        raise KeyError(f\"Required columns {missing_columns} are not found in buildings dataset.\")\n",
    "\n",
    "    print(\"Calculating block metrics...\")\n",
    "    blocks_subset[\"BLOCK_ID\"] = blocks_subset.index  # Assign unique ID to each block\n",
    "    buildings_subset[\"build_idx\"] = buildings_subset.index  # Assign unique ID to each building\n",
    "    \n",
    "    # Calculate building area (square meters -> square miles)\n",
    "    print(\"Calculating building area coverage (sq-mile)...\")\n",
    "    buildings_subset[\"build_area_sm\"] = buildings_subset.geometry.area * 0.00000038610215855  # Convert area to sq-mile\n",
    "    \n",
    "    # Handle building height (convert meters to feet and filter valid heights)\n",
    "    print(\"Processing building heights...\")\n",
    "    buildings_subset[\"height_m\"] = pd.to_numeric(buildings_subset[\"height\"], errors=\"coerce\")\n",
    "    buildings_subset[\"height_ft\"] = buildings_subset[\"height_m\"] * 3.28084  # Convert height to feet\n",
    "    buildings_subset = buildings_subset[buildings_subset[\"height_ft\"] > 0]  # Filter buildings with positive heights\n",
    "    \n",
    "    # Spatial join to associate buildings with blocks\n",
    "    print(\"Associating buildings with blocks via spatial join...\")\n",
    "    buildings_with_blocks = gpd.sjoin(buildings_subset, blocks_subset, how=\"inner\", predicate=\"intersects\")\n",
    "    \n",
    "    # Aggregate metrics per block\n",
    "    print(\"Aggregating building metrics per block...\")\n",
    "    building_metrics = buildings_with_blocks.groupby(\"BLOCK_ID\").agg(\n",
    "        avg_hght=(\"height_ft\", \"mean\"),  # Average building height\n",
    "        avg_sback=(\"setback_ft\", \"mean\"),  # Average setback distance\n",
    "        bld_area=(\"build_area_sm\", \"sum\"),  # Sum building areas\n",
    "        bld_cnt=(\"build_idx\", \"count\"),  # Count number of buildings\n",
    "    )\n",
    "    \n",
    "    # Calculate population density for blocks\n",
    "    print(\"Calculating population density and other metrics for blocks...\")\n",
    "    blocks_subset[\"area_sm\"] = blocks_subset.geometry.area * 0.00000038610215855  # Convert block area to sq-mile\n",
    "    blocks_subset[\"pop_den\"] = blocks_subset[\"POP20\"] / blocks_subset[\"area_sm\"]  # Population density per sq-mile\n",
    "    \n",
    "    # additional metrics\n",
    "    print(\"Calculating additional block metrics: Building count per square mile, building area percentage, and households per building count...\")\n",
    "    building_metrics[\"bld_ctsm\"] = building_metrics[\"bld_cnt\"] / blocks_subset[\"area_sm\"]  # Building count per sq-mile\n",
    "    building_metrics[\"bld_prc\"] = (building_metrics[\"bld_area\"] / blocks_subset[\"area_sm\"]) * 100  # Building area as percentage of block area\n",
    "    \n",
    "    # Add Households per Building Count (HHpBLD)\n",
    "    print(\"Calculating Households per Building Count (HHpBLD)...\")\n",
    "    building_metrics[\"HHpBLD\"] = blocks_subset[\"HOUSING20\"] / building_metrics[\"bld_cnt\"]\n",
    "    building_metrics[\"HHpBLD\"] = building_metrics[\"HHpBLD\"].fillna(0)  # Fill missing values with 0 (e.g., no buildings)\n",
    "\n",
    "    # Merge metrics into block dataset\n",
    "    print(\"Merging building metrics into block dataset...\")\n",
    "    blocks_subset = blocks_subset.merge(building_metrics, on=\"BLOCK_ID\", how=\"left\")\n",
    "    \n",
    "    # Fill missing values explicitly for columns\n",
    "    print(\"Filling missing values for block metrics...\")\n",
    "    blocks_subset[\"avg_hght\"] = blocks_subset[\"avg_hght\"].fillna(0)  \n",
    "    blocks_subset[\"avg_sback\"] = blocks_subset[\"avg_sback\"].fillna(0)  \n",
    "    blocks_subset[\"bld_area\"] = blocks_subset[\"bld_area\"].fillna(0)  \n",
    "    blocks_subset[\"bld_ctsm\"] = blocks_subset[\"bld_ctsm\"].fillna(0)  \n",
    "    blocks_subset[\"bld_prc\"] = blocks_subset[\"bld_prc\"].fillna(0)  \n",
    "    blocks_subset[\"bld_cnt\"] = blocks_subset[\"bld_cnt\"].fillna(0)  \n",
    "    blocks_subset[\"HHpBLD\"] = blocks_subset[\"HHpBLD\"].fillna(0)  \n",
    "\n",
    "    print(\"Block metrics calculated successfully.\")\n",
    "    return blocks_subset\n",
    "\n",
    "# Calculate block-level metrics\n",
    "blocks_processed = calculate_block_metrics(blocks_subset, buildings_subset, roads_subset)\n",
    "print(\"Block metrics successfully calculated:\")\n",
    "print(blocks_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48683f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculating road metrics with buffer size (for spatial analysis): 50 feet...\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001000      2012  483550010002012   \n",
      "1        48        355    001000      1017  483550010001017   \n",
      "2        48        355    001000      3008  483550010003008   \n",
      "3        48        355    001000      2022  483550010002022   \n",
      "4        48        355    001000      1019  483550010001019   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... BLOCK_ID  \\\n",
      "0  1000000US483550010002012  Block2012   G5040    U  20287  ...   316275   \n",
      "1  1000000US483550010001017  Block1017   G5040    U  20287  ...   359470   \n",
      "2  1000000US483550010003008  Block3008   G5040    U  20287  ...   359630   \n",
      "3  1000000US483550010002022  Block2022   G5040    U  20287  ...   359631   \n",
      "4  1000000US483550010001019  Block1019   G5040    U  20287  ...   367172   \n",
      "\n",
      "    area_sm       pop_den   avg_hght   avg_sback  bld_area  bld_cnt  \\\n",
      "0  0.002304   3906.448145  32.020998   54.403313  0.000178      4.0   \n",
      "1  0.003346  11956.064635  28.935079   36.253966  0.000724     17.0   \n",
      "2  0.003631  13219.247586  33.207569  179.262686  0.000613      6.0   \n",
      "3  0.004493  10460.225254  32.515175   57.344657  0.001073     16.0   \n",
      "4  0.003290   7599.159707  28.808758   40.860291  0.000402     11.0   \n",
      "\n",
      "      bld_ctsm    bld_prc    HHpBLD  \n",
      "0  1736.199176   7.719694  1.000000  \n",
      "1  5081.327470  21.644454  1.294118  \n",
      "2  1652.405948  16.873311  3.833333  \n",
      "3  3560.927746  23.886743  0.937500  \n",
      "4  3343.630271  12.214263  1.272727  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "- Creating road buffers...\n",
      "-- Road buffers structure after buffering:\n",
      "Index(['osmid', 'highway', 'lanes', 'name', 'oneway', 'ref', 'reversed',\n",
      "       'length', 'maxspeed', 'geometry', 'bridge', 'junction', 'width',\n",
      "       'access', 'road_id', 'buffer_area'],\n",
      "      dtype='object')\n",
      "- Performing spatial join of blocks with road buffers...\n",
      "-- Intersections structure after spatial join:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001000      2012  483550010002012   \n",
      "1        48        355    001000      1005  483550010001005   \n",
      "2        48        355    001000      2010  483550010002010   \n",
      "3        48        355    001000      1006  483550010001006   \n",
      "4        48        355    001000      2011  483550010002011   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... reversed  \\\n",
      "0  1000000US483550010002012  Block2012   G5040    U  20287  ...    False   \n",
      "1  1000000US483550010001005  Block1005   G5040    U  20287  ...    False   \n",
      "2  1000000US483550010002010  Block2010   G5040    U  20287  ...    False   \n",
      "3  1000000US483550010001006  Block1006   G5040    U  20287  ...    False   \n",
      "4  1000000US483550010002011  Block2011   G5040    U  20287  ...    False   \n",
      "\n",
      "       length  maxspeed bridge junction  width  access  road_id  buffer_area  \\\n",
      "0  109.886879       NaN    NaN      NaN    NaN     NaN       42  4083.696159   \n",
      "1  109.886879       NaN    NaN      NaN    NaN     NaN       42  4083.696159   \n",
      "2  109.886879       NaN    NaN      NaN    NaN     NaN       42  4083.696159   \n",
      "3  109.886879       NaN    NaN      NaN    NaN     NaN       42  4083.696159   \n",
      "4  109.886879       NaN    NaN      NaN    NaN     NaN       42  4083.696159   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((656199.421 3074543.571, 656221.397 3...  \n",
      "1  POLYGON ((656199.421 3074543.571, 656199.556 3...  \n",
      "2  POLYGON ((656309.181 3074545.764, 656308.835 3...  \n",
      "3  POLYGON ((656199.076 3074554.981, 656199.421 3...  \n",
      "4  POLYGON ((656308.835 3074557.284, 656309.181 3...  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "-- Adding road buffer areas for overlap proportion calculation...\n",
      "-- Intersection overlap proportions sample:\n",
      "   BLOCK_ID  road_id  overlap_area  road_buffer_area  overlap_proportion\n",
      "0    316275       42   1706.451215       4083.696159            0.417869\n",
      "1    476007       42    223.849946       4083.696159            0.054816\n",
      "2    591277       42    306.474134       4083.696159            0.075048\n",
      "3    608956       42    206.784240       4083.696159            0.050637\n",
      "4    608958       42   1640.136624       4083.696159            0.401630\n",
      "- Creating overlap dictionary for roads...\n",
      "-- Adding block IDs and overlap percentages columns...\n",
      "- Loading evacuation routes shapefile...\n",
      "-- Flagging roads based on evacuation route overlap...\n",
      "- Weighting metrics for roads...\n",
      "- Aggregating weighted metrics across roads...\n",
      "-- Aggregated road metrics structure:\n",
      "        agg_pop  agg_area     agg_ctsm  agg_bldprc   agg_hght  agg_sback  \\\n",
      "0   6724.699007  0.000664  1586.279553    6.526563  28.843399  44.494374   \n",
      "1   8429.342498  0.000831  2490.019889   11.494276  29.689891  57.409532   \n",
      "2  10204.453983  0.000503  3011.494163   13.612558  29.594486  50.970519   \n",
      "3  10437.723912  0.000445  3195.825095   13.529357  29.671367  43.016041   \n",
      "4   8017.932741  0.000875  3160.983300   14.012185  29.979459  78.388945   \n",
      "\n",
      "   agg_HHpBLD  \n",
      "0    1.851652  \n",
      "1    1.593379  \n",
      "2    1.347512  \n",
      "3    1.429428  \n",
      "4    1.359001  \n",
      "- Merging metrics into roads dataset...\n",
      "-- Adding calculated road segment lengths as length_cal...\n",
      "-- Final roads structure:\n",
      "      osmid      highway lanes               name  oneway  ref reversed  \\\n",
      "0  21153170  residential   NaN  Marguerite Street   False  NaN     True   \n",
      "1  21154066  residential   NaN        Mary Street   False  NaN     True   \n",
      "2  21157903  residential   NaN      Morris Street   False  NaN    False   \n",
      "3  89958916  residential   NaN     Coleman Avenue   False  NaN     True   \n",
      "4  21150792     tertiary   NaN        Ruth Street   False  NaN     True   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  281.132210      NaN  LINESTRING (655810.207 3074514.941, 656046.746...   \n",
      "1   98.376559      NaN  LINESTRING (655842.677 3074420.28, 655941.207 ...   \n",
      "2  101.324854      NaN  LINESTRING (655843.445 3074315.449, 655944.914...   \n",
      "3  100.119788      NaN  LINESTRING (655846.291 3074230.635, 655946.584...   \n",
      "4  100.178653      NaN  LINESTRING (655848.507 3074145.016, 655948.821...   \n",
      "\n",
      "   ...                                      overlap_percs evac_flag  \\\n",
      "0  ...  [0.016350909618299395, 0.47395796680445423, 0....         0   \n",
      "1  ...  [0.043017128296961527, 0.5227961978336852, 0.3...         0   \n",
      "2  ...  [0.05004595171806484, 0.3711329404436099, 0.43...         0   \n",
      "3  ...  [0.39427144805278186, 0.04722269267436038, 0.4...         0   \n",
      "4  ...  [0.431614559566586, 0.04550757082571523, 0.372...         0   \n",
      "\n",
      "        agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0   6724.699007  0.000664  1586.279553   6.526563  28.843399  44.494374   \n",
      "1   8429.342498  0.000831  2490.019889  11.494276  29.689891  57.409532   \n",
      "2  10204.453983  0.000503  3011.494163  13.612558  29.594486  50.970519   \n",
      "3  10437.723912  0.000445  3195.825095  13.529357  29.671367  43.016041   \n",
      "4   8017.932741  0.000875  3160.983300  14.012185  29.979459  78.388945   \n",
      "\n",
      "   agg_HHpBLD  length_cal  \n",
      "0    1.851652  281.594984  \n",
      "1    1.593379   98.548456  \n",
      "2    1.347512  101.501837  \n",
      "3    1.429428  100.294721  \n",
      "4    1.359001  100.353592  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "- Loading the shapefile with polygons for 'UrbanArea' flagging...\n",
      "- Adding 'UrbanArea' flag to roads based on intersection or touching polygons...\n",
      "-- Reprojecting urban area polygons to match roads CRS...\n",
      "-- Added 'UrbanArea' column to roads GeoDataFrame:\n",
      "-- Final roads with 'UrbanArea' flag sample:\n",
      "      osmid      highway lanes               name  oneway  ref reversed  \\\n",
      "0  21153170  residential   NaN  Marguerite Street   False  NaN     True   \n",
      "1  21154066  residential   NaN        Mary Street   False  NaN     True   \n",
      "2  21157903  residential   NaN      Morris Street   False  NaN    False   \n",
      "3  89958916  residential   NaN     Coleman Avenue   False  NaN     True   \n",
      "4  21150792     tertiary   NaN        Ruth Street   False  NaN     True   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  281.132210      NaN  LINESTRING (655810.207 3074514.941, 656046.746...   \n",
      "1   98.376559      NaN  LINESTRING (655842.677 3074420.28, 655941.207 ...   \n",
      "2  101.324854      NaN  LINESTRING (655843.445 3074315.449, 655944.914...   \n",
      "3  100.119788      NaN  LINESTRING (655846.291 3074230.635, 655946.584...   \n",
      "4  100.178653      NaN  LINESTRING (655848.507 3074145.016, 655948.821...   \n",
      "\n",
      "   ... evac_flag       agg_pop  agg_area     agg_ctsm  agg_bldprc   agg_hght  \\\n",
      "0  ...         0   6724.699007  0.000664  1586.279553    6.526563  28.843399   \n",
      "1  ...         0   8429.342498  0.000831  2490.019889   11.494276  29.689891   \n",
      "2  ...         0  10204.453983  0.000503  3011.494163   13.612558  29.594486   \n",
      "3  ...         0  10437.723912  0.000445  3195.825095   13.529357  29.671367   \n",
      "4  ...         0   8017.932741  0.000875  3160.983300   14.012185  29.979459   \n",
      "\n",
      "   agg_sback  agg_HHpBLD  length_cal  UrbanArea  \n",
      "0  44.494374    1.851652  281.594984          1  \n",
      "1  57.409532    1.593379   98.548456          1  \n",
      "2  50.970519    1.347512  101.501837          1  \n",
      "3  43.016041    1.429428  100.294721          1  \n",
      "4  78.388945    1.359001  100.353592          1  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "- Ensuring CRS consistency and clipping polygons...\n",
      "-- CRS mismatch between polygons and city limits.\n",
      "--- Converting both city limits and polygons CRS to match roads CRS...\n",
      "-- CRS match between polygons and city limits. Clipping polygons to city limits...\n",
      "-- Sample of clipped polygons:\n",
      "           D1C    Shape__Are     Shape__Len  \\\n",
      "9186  0.001412  1.740383e+09  702468.025945   \n",
      "8826  0.035271  1.493616e+08   60699.525117   \n",
      "9187  0.000000  3.860989e+04   80104.012519   \n",
      "8890  0.163972  1.769925e+06    9368.110551   \n",
      "8908  0.068378  1.677014e+06    5987.189241   \n",
      "\n",
      "                                    GlobalID  \\\n",
      "9186  {0a7bd5fb-24b3-4230-bf36-c08cd27e2735}   \n",
      "8826  {6f535ab8-7e82-4204-b21c-14621919ede8}   \n",
      "9187  {d458cafe-3ddf-46b0-a80e-11e7d8fa4b15}   \n",
      "8890  {7f90fd59-28bc-470f-afd8-534cbb41bf3f}   \n",
      "8908  {e366b08b-6529-4a18-9e11-3ff58403e32f}   \n",
      "\n",
      "                                               geometry  \n",
      "9186  MULTIPOLYGON (((669560.837 3048792.708, 669596...  \n",
      "8826  POLYGON ((649059.12 3068886.087, 649181.187 30...  \n",
      "9187  POLYGON ((675438.745 3051553.313, 675426.238 3...  \n",
      "8890  POLYGON ((668192.606 3055870.206, 668179.696 3...  \n",
      "8908  POLYGON ((667863.538 3057007.098, 667945.252 3...  \n",
      "- Converting D1C from jobs/acre to jobs/square mile...\n",
      "-- Updated density column with jobs/square mile (after clipping):\n",
      "         emp_den\n",
      "9186    0.903962\n",
      "8826   22.573742\n",
      "9187    0.000000\n",
      "8890  104.942059\n",
      "8908   43.761863\n",
      "- Performing spatial join to associate roads with employment density polygons...\n",
      "-- Results of spatial join sample:\n",
      "         D1C       emp_den\n",
      "0   2.028361   1298.151013\n",
      "1   2.028361   1298.151013\n",
      "2   2.028361   1298.151013\n",
      "3   2.028361   1298.151013\n",
      "4  21.134731  13526.227945\n",
      "- Aggregating road intersections to take the highest employment density for each road...\n",
      "-- Aggregated density by road (max jobs/sq mile):\n",
      "   road_id       emp_den\n",
      "0        0   1298.151013\n",
      "1        1   1298.151013\n",
      "2        2   1298.151013\n",
      "3        3   1298.151013\n",
      "4        4  13526.227945\n",
      "- Merging maximum density values back into the roads GeoDataFrame...\n",
      "-- Final roads sample with employment density:\n",
      "      osmid      highway lanes               name  oneway  ref reversed  \\\n",
      "0  21153170  residential   NaN  Marguerite Street   False  NaN     True   \n",
      "1  21154066  residential   NaN        Mary Street   False  NaN     True   \n",
      "2  21157903  residential   NaN      Morris Street   False  NaN    False   \n",
      "3  89958916  residential   NaN     Coleman Avenue   False  NaN     True   \n",
      "4  21150792     tertiary   NaN        Ruth Street   False  NaN     True   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  281.132210      NaN  LINESTRING (655810.207 3074514.941, 656046.746...   \n",
      "1   98.376559      NaN  LINESTRING (655842.677 3074420.28, 655941.207 ...   \n",
      "2  101.324854      NaN  LINESTRING (655843.445 3074315.449, 655944.914...   \n",
      "3  100.119788      NaN  LINESTRING (655846.291 3074230.635, 655946.584...   \n",
      "4  100.178653      NaN  LINESTRING (655848.507 3074145.016, 655948.821...   \n",
      "\n",
      "   ...       agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0  ...   6724.699007  0.000664  1586.279553   6.526563  28.843399  44.494374   \n",
      "1  ...   8429.342498  0.000831  2490.019889  11.494276  29.689891  57.409532   \n",
      "2  ...  10204.453983  0.000503  3011.494163  13.612558  29.594486  50.970519   \n",
      "3  ...  10437.723912  0.000445  3195.825095  13.529357  29.671367  43.016041   \n",
      "4  ...   8017.932741  0.000875  3160.983300  14.012185  29.979459  78.388945   \n",
      "\n",
      "  agg_HHpBLD  length_cal  UrbanArea       emp_den  \n",
      "0   1.851652  281.594984          1   1298.151013  \n",
      "1   1.593379   98.548456          1   1298.151013  \n",
      "2   1.347512  101.501837          1   1298.151013  \n",
      "3   1.429428  100.294721          1   1298.151013  \n",
      "4   1.359001  100.353592          1  13526.227945  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "-- Final roads GeoDataFrame with employment density sample:\n",
      "      osmid      highway lanes               name  oneway  ref reversed  \\\n",
      "0  21153170  residential   NaN  Marguerite Street   False  NaN     True   \n",
      "1  21154066  residential   NaN        Mary Street   False  NaN     True   \n",
      "2  21157903  residential   NaN      Morris Street   False  NaN    False   \n",
      "3  89958916  residential   NaN     Coleman Avenue   False  NaN     True   \n",
      "4  21150792     tertiary   NaN        Ruth Street   False  NaN     True   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  281.132210      NaN  LINESTRING (655810.207 3074514.941, 656046.746...   \n",
      "1   98.376559      NaN  LINESTRING (655842.677 3074420.28, 655941.207 ...   \n",
      "2  101.324854      NaN  LINESTRING (655843.445 3074315.449, 655944.914...   \n",
      "3  100.119788      NaN  LINESTRING (655846.291 3074230.635, 655946.584...   \n",
      "4  100.178653      NaN  LINESTRING (655848.507 3074145.016, 655948.821...   \n",
      "\n",
      "   ...       agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0  ...   6724.699007  0.000664  1586.279553   6.526563  28.843399  44.494374   \n",
      "1  ...   8429.342498  0.000831  2490.019889  11.494276  29.689891  57.409532   \n",
      "2  ...  10204.453983  0.000503  3011.494163  13.612558  29.594486  50.970519   \n",
      "3  ...  10437.723912  0.000445  3195.825095  13.529357  29.671367  43.016041   \n",
      "4  ...   8017.932741  0.000875  3160.983300  14.012185  29.979459  78.388945   \n",
      "\n",
      "  agg_HHpBLD  length_cal  UrbanArea       emp_den  \n",
      "0   1.851652  281.594984          1   1298.151013  \n",
      "1   1.593379   98.548456          1   1298.151013  \n",
      "2   1.347512  101.501837          1   1298.151013  \n",
      "3   1.429428  100.294721          1   1298.151013  \n",
      "4   1.359001  100.353592          1  13526.227945  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_road_metrics(blocks_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame, buffer_size_feet=50) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate road-level metrics based on spatial overlap with blocks (weighted aggregation).\n",
    "    Includes Block IDs and Overlap Percentages as columns, flags roads intersecting evacuation routes using buffered geometry,\n",
    "    and calculates new columns: agg_HHpBLD (weighted households per building count) and length_cal (road segment length).\n",
    "    \"\"\"\n",
    "    print(f\"- Calculating road metrics with buffer size (for spatial analysis): {buffer_size_feet} feet...\")\n",
    "    buffer_size_meters = buffer_size_feet * 0.3048  # Convert buffer size to meters\n",
    "\n",
    "    # Blocks subset should maintain the original BLOCK_ID\n",
    "    blocks_subset = blocks_subset.copy()\n",
    "    print(blocks_subset.head())  # Output a sample of blocks_subset\n",
    "\n",
    "    roads_subset = roads_subset.copy().reset_index(drop=True)\n",
    "    roads_subset[\"road_id\"] = roads_subset.index\n",
    "\n",
    "    # Check if required block-level metrics exist\n",
    "    required_columns = [\"area_sm\", \"pop_den\", \"bld_area\", \"bld_ctsm\", \"avg_hght\", \"avg_sback\", \"HHpBLD\"]\n",
    "    missing_columns = [col for col in required_columns if col not in blocks_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: The following required block-level metrics are missing: {missing_columns}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Create buffers for roads\n",
    "    print(\"- Creating road buffers...\")\n",
    "    road_buffers = roads_subset.copy()\n",
    "    road_buffers[\"geometry\"] = roads_subset.geometry.buffer(buffer_size_meters)\n",
    "    road_buffers[\"road_id\"] = roads_subset[\"road_id\"]\n",
    "    road_buffers[\"buffer_area\"] = road_buffers.geometry.area  # Area of the road buffer, for overlap proportion calculation\n",
    "    print(\"-- Road buffers structure after buffering:\")\n",
    "    print(road_buffers.columns)\n",
    "\n",
    "    # Perform spatial join of blocks with road buffers\n",
    "    print(\"- Performing spatial join of blocks with road buffers...\")\n",
    "    intersections = gpd.overlay(blocks_subset, road_buffers, how=\"intersection\")\n",
    "    print(\"-- Intersections structure after spatial join:\")\n",
    "    print(intersections.head())\n",
    "\n",
    "    # Validate road buffer areas and block intersections\n",
    "    print(\"-- Adding road buffer areas for overlap proportion calculation...\")\n",
    "    intersections[\"road_buffer_area\"] = intersections[\"road_id\"].map(road_buffers.set_index(\"road_id\")[\"buffer_area\"])\n",
    "    intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "    intersections[\"overlap_proportion\"] = intersections[\"overlap_area\"] / intersections[\"road_buffer_area\"]\n",
    "    print(\"-- Intersection overlap proportions sample:\")\n",
    "    print(intersections[[\"BLOCK_ID\", \"road_id\", \"overlap_area\", \"road_buffer_area\", \"overlap_proportion\"]].head())\n",
    "\n",
    "    # Create overlap dictionary for roads\n",
    "    print(\"- Creating overlap dictionary for roads...\")\n",
    "    road_block_overlap = (\n",
    "        intersections.groupby(\"road_id\")\n",
    "        .apply(lambda rows: dict(zip(rows[\"BLOCK_ID\"], rows[\"overlap_proportion\"])))\n",
    "        .to_dict()\n",
    "    )\n",
    "    print(\"-- Adding block IDs and overlap percentages columns...\")\n",
    "    # Add block IDs and overlap percentages columns\n",
    "    roads_subset[\"block_ids\"] = [\n",
    "        list(overlap_dict.keys()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "    roads_subset[\"overlap_percs\"] = [\n",
    "        list(overlap_dict.values()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "\n",
    "    # Load Evacuation Routes\n",
    "    print(\"- Loading evacuation routes shapefile...\")\n",
    "    evac_path = os.path.join(one_drive_path, \"Data\", \"TxDOT Evacuation Routes AGO.shp\")\n",
    "    evacuation_routes = gpd.read_file(evac_path)\n",
    "\n",
    "    # Ensure both datasets use the same CRS for spatial operations\n",
    "    if evacuation_routes.crs != road_buffers.crs:\n",
    "        evacuation_routes = evacuation_routes.to_crs(road_buffers.crs)\n",
    "\n",
    "    print(\"-- Flagging roads based on evacuation route overlap...\")\n",
    "\n",
    "    def flag_evacuation_routes(road_buffer_geom):\n",
    "        \"\"\"\n",
    "        Flag roads based on overlap with evacuation routes.\n",
    "        0 = No overlap\n",
    "        1 = Major Evacuation Routes\n",
    "        2 = Potential Contraflow\n",
    "        3 = Potential EvacuLanes\n",
    "        \"\"\"\n",
    "        for _, evac_row in evacuation_routes.iterrows():\n",
    "            if road_buffer_geom.intersects(evac_row.geometry):\n",
    "                route_type = evac_row[\"ROUTE_TYPE\"]\n",
    "                if route_type == \"Major Evacuation Routes\":\n",
    "                    return 1\n",
    "                elif route_type == \"Potential Contraflow\":\n",
    "                    return 2\n",
    "                elif route_type == \"Potential EvacuLanes\":\n",
    "                    return 3\n",
    "        return 0  # No overlap\n",
    "\n",
    "    # Use the buffered geometry to calculate evacuation flags\n",
    "    roads_subset[\"evac_flag\"] = road_buffers.geometry.apply(flag_evacuation_routes)\n",
    "\n",
    "    # Calculate weighted metrics for roads\n",
    "    print(\"- Weighting metrics for roads...\")\n",
    "    def compute_weighted_metrics(road_id, overlap_dict):\n",
    "        \"\"\"\n",
    "        Compute weighted metrics for a road using normalized weights for metrics like `agg_hght` and `agg_sback`.\n",
    "        Includes HHpBLD as a new weighted metric.\n",
    "        \"\"\"\n",
    "        blocks = blocks_subset.set_index(\"BLOCK_ID\").loc[overlap_dict.keys()]\n",
    "        raw_weights = pd.Series({BLOCK_ID: overlap_dict[BLOCK_ID] for BLOCK_ID in blocks.index})\n",
    "        total_weight = raw_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = raw_weights / total_weight\n",
    "        else:\n",
    "            return pd.Series({\n",
    "                \"agg_pop\": 0,\n",
    "                \"agg_area\": 0,\n",
    "                \"agg_ctsm\": 0,\n",
    "                \"agg_bldprc\": 0,\n",
    "                \"agg_hght\": 0,\n",
    "                \"agg_sback\": 0,\n",
    "                \"agg_HHpBLD\": 0,\n",
    "            })\n",
    "        valid_blocks_for_hght_sback = blocks[(blocks[\"avg_hght\"] > 0) & (blocks[\"avg_sback\"] > 0)]\n",
    "        aggregated_metrics = {\n",
    "            \"agg_pop\": (blocks[\"pop_den\"] * normalized_weights).sum(),\n",
    "            \"agg_area\": (blocks[\"bld_area\"] * normalized_weights).sum(),\n",
    "            \"agg_ctsm\": (blocks[\"bld_ctsm\"] * normalized_weights).sum(),\n",
    "            \"agg_bldprc\": (blocks[\"bld_prc\"] * normalized_weights).sum(),\n",
    "            \"agg_hght\": (valid_blocks_for_hght_sback[\"avg_hght\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "            \"agg_sback\": (valid_blocks_for_hght_sback[\"avg_sback\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "            \"agg_HHpBLD\": (blocks[\"HHpBLD\"] * normalized_weights).sum(),\n",
    "        }\n",
    "        return pd.Series(aggregated_metrics)\n",
    "\n",
    "    print(\"- Aggregating weighted metrics across roads...\")\n",
    "    road_metrics_df = pd.DataFrame([\n",
    "        compute_weighted_metrics(road_id, overlap_dict)\n",
    "        for road_id, overlap_dict in road_block_overlap.items()\n",
    "    ], index=road_block_overlap.keys())\n",
    "    print(\"-- Aggregated road metrics structure:\")\n",
    "    print(road_metrics_df.head())\n",
    "\n",
    "    # Merge aggregated metrics into roads dataset\n",
    "    print(\"- Merging metrics into roads dataset...\")\n",
    "    roads_subset = roads_subset.merge(road_metrics_df, left_on=\"road_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    # Calculate road length and add as a new column\n",
    "    print(\"-- Adding calculated road segment lengths as length_cal...\")\n",
    "    roads_subset[\"length_cal\"] = roads_subset.geometry.length  # Calculate length based on geometry\n",
    "\n",
    "    print(\"-- Final roads structure:\")\n",
    "    print(roads_subset.head())\n",
    "    return roads_subset\n",
    "\n",
    "def add_urban_area_flag(roads: gpd.GeoDataFrame, polygons: gpd.GeoDataFrame, city_limits: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Adds a flag ('UrbanArea') to the roads GeoDataFrame indicating whether\n",
    "    a road segment is inside or touches any polygon in the polygons GeoDataFrame.\n",
    "    \"\"\"\n",
    "    print(\"- Adding 'UrbanArea' flag to roads based on intersection or touching polygons...\")\n",
    "\n",
    "    # Ensure CRS is consistent for spatial operations\n",
    "    if roads.crs != polygons.crs:\n",
    "        print(\"-- Reprojecting urban area polygons to match roads CRS...\")\n",
    "        \n",
    "        polygons = polygons.to_crs(roads.crs)\n",
    "\n",
    "    def check_intersects_or_touches(road_geom):\n",
    "        \"\"\"\n",
    "        Check if a road segment intersects or touches any polygon.\n",
    "        Returns 1 if true (overlap/match), else 0 (no overlap/match).\n",
    "        \"\"\"\n",
    "        for poly_geom in polygons.geometry:\n",
    "            if road_geom.intersects(poly_geom) or road_geom.touches(poly_geom):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    # Apply the function to calculate 'UrbanArea' flag\n",
    "    roads[\"UrbanArea\"] = roads.geometry.apply(check_intersects_or_touches)\n",
    "    print(\"-- Added 'UrbanArea' column to roads GeoDataFrame:\")\n",
    "    \n",
    "    return roads\n",
    "\n",
    "def associate_roads_with_employment_density(roads: gpd.GeoDataFrame, polygons: gpd.GeoDataFrame, density_column: str, city_limits: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Associates roads with employment density polygons based on spatial intersection.\n",
    "    Trims employment density polygons to be within the city limits (provided city_limits GeoDataFrame).\n",
    "    Converts jobs/acre to jobs/square mile.\n",
    "    If a road intersects multiple polygons, assigns the value of the polygon with the higher employment density.\n",
    "    \"\"\"\n",
    "    print(\"- Ensuring CRS consistency and clipping polygons...\")\n",
    "\n",
    "    # Check if the CRS of polygons and city limits match\n",
    "    if polygons.crs != city_limits.crs:\n",
    "        print(\"-- CRS mismatch between polygons and city limits.\")\n",
    "        if roads.crs == polygons.crs:\n",
    "            print(\"--- Converting city limits CRS to match polygons CRS...\")\n",
    "            city_limits = city_limits.to_crs(polygons.crs)\n",
    "        else:\n",
    "            print(\"--- Converting both city limits and polygons CRS to match roads CRS...\")\n",
    "            city_limits = city_limits.to_crs(roads.crs)\n",
    "            polygons = polygons.to_crs(roads.crs)\n",
    "\n",
    "    # If CRS matches between polygons and city limits, trim polygons first, then optionally convert\n",
    "    if polygons.crs == city_limits.crs:\n",
    "        print(\"-- CRS match between polygons and city limits. Clipping polygons to city limits...\")\n",
    "        polygons_clipped = gpd.clip(polygons, city_limits)\n",
    "        if roads.crs != polygons.crs:\n",
    "            print(\"--- Converting clipped polygons CRS to match roads CRS...\")\n",
    "            polygons_clipped = polygons_clipped.to_crs(roads.crs)\n",
    "    else:\n",
    "        polygons_clipped = polygons  # Already transformed earlier, no need to clip again\n",
    "\n",
    "    print(\"-- Sample of clipped polygons:\")\n",
    "    print(polygons_clipped.head())\n",
    "\n",
    "    # Convert employment density to jobs per square mile\n",
    "    print(f\"- Converting {density_column} from jobs/acre to jobs/square mile...\")\n",
    "    polygons_clipped[\"emp_den\"] = polygons_clipped[density_column] * 640  # 1 square mile = 640 acres\n",
    "    print(\"-- Updated density column with jobs/square mile (after clipping):\")\n",
    "    print(polygons_clipped[[\"emp_den\"]].head())\n",
    "\n",
    "    # Perform spatial join to associate roads with clipped polygons\n",
    "    print(\"- Performing spatial join to associate roads with employment density polygons...\")\n",
    "    roads_with_density = gpd.sjoin(roads, polygons_clipped, how=\"left\", predicate=\"intersects\")\n",
    "    print(\"-- Results of spatial join sample:\")\n",
    "    print(roads_with_density[[density_column, \"emp_den\"]].head())\n",
    "\n",
    "    # Identify the road-polygon intersections and select the highest density value for each road\n",
    "    print(\"- Aggregating road intersections to take the highest employment density for each road...\")\n",
    "    roads_with_density_agg = (\n",
    "        roads_with_density.groupby(\"road_id\")  # Assuming \"road_id\" is unique for roads\n",
    "        .agg({\"emp_den\": \"max\"})  # Take the maximum density value\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"-- Aggregated density by road (max jobs/sq mile):\")\n",
    "    print(roads_with_density_agg.head())\n",
    "\n",
    "    # Merge the maximum density back into the roads GeoDataFrame\n",
    "    print(\"- Merging maximum density values back into the roads GeoDataFrame...\")\n",
    "    roads = roads.merge(roads_with_density_agg, on=\"road_id\", how=\"left\")\n",
    "    print(\"-- Final roads sample with employment density:\")\n",
    "    print(roads.head())\n",
    "\n",
    "    return roads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Execute road metrics calculation\n",
    "roads_processed = calculate_road_metrics(blocks_processed, roads_subset, buffer_size_feet=50)\n",
    "\n",
    "print(\"- Loading the shapefile with polygons for 'UrbanArea' flagging...\")\n",
    "urbanarea_file = os.path.join(one_drive_path, \"Data\", \"2020_Census_Urban_Areas\", \"2020_Census_Urban_Areas.shp\")\n",
    "urban_area_polygons = gpd.read_file(urbanarea_file)\n",
    "# Path to the Corpus Christi city limits shapefile\n",
    "cc_limits_file = os.path.join(one_drive_path, \"Data\", \"CC-citylimits\", \"Citylimits.shp\")\n",
    "city_limits = gpd.read_file(cc_limits_file)\n",
    "\n",
    "# Read the employment density shapefile and inspect column names\n",
    "emp_density_file = os.path.join(one_drive_path, \"Data\", \"CBG2010_SLD_YY\", \"CBG2010_SLD_YY.shp\")\n",
    "employment_density_polygons = gpd.read_file(emp_density_file)\n",
    "\n",
    "roads_processed = add_urban_area_flag(roads_processed, urban_area_polygons, city_limits)\n",
    "\n",
    "print(\"-- Final roads with 'UrbanArea' flag sample:\")\n",
    "print(roads_processed.head())\n",
    "\n",
    "\n",
    "\n",
    "# Specify the column name for employment density\n",
    "density_column = \"D1C\" \n",
    "\n",
    "# Process the roads to associate employment density (clipped to Corpus Christi city limits)\n",
    "roads_processed = associate_roads_with_employment_density(\n",
    "    roads=roads_processed,\n",
    "    polygons=employment_density_polygons,\n",
    "    density_column=density_column,\n",
    "    city_limits=city_limits\n",
    ")\n",
    "\n",
    "# Final debugging output\n",
    "print(\"-- Final roads GeoDataFrame with employment density sample:\")\n",
    "print(roads_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving subsets to shapefiles...\n",
      "- Saving buildings subset...\n",
      "-- Full column names in buildings_subset:\n",
      "  geometry\n",
      "  addr:state\n",
      "  building\n",
      "  ele\n",
      "  gnis:feature_id\n",
      "  name\n",
      "  source\n",
      "  addr:city\n",
      "  addr:housename\n",
      "  addr:housenumber\n",
      "  addr:postcode\n",
      "  addr:street\n",
      "  amenity\n",
      "  brand\n",
      "  brand:wikidata\n",
      "  cuisine\n",
      "  healthcare\n",
      "  healthcare:counselling\n",
      "  height\n",
      "  phone\n",
      "  website\n",
      "  generator:method\n",
      "  generator:output:electricity\n",
      "  generator:source\n",
      "  generator:type\n",
      "  layer\n",
      "  power\n",
      "  shop\n",
      "  opening_hours\n",
      "  ref\n",
      "  wholesale\n",
      "  addr:unit\n",
      "  official_name\n",
      "  takeaway\n",
      "  note\n",
      "  email\n",
      "  museum\n",
      "  tourism\n",
      "  addr:country\n",
      "  alt_name\n",
      "  contact:email\n",
      "  contact:facebook\n",
      "  contact:twitter\n",
      "  man_made\n",
      "  content\n",
      "  office\n",
      "  start_date\n",
      "  building:levels\n",
      "  leisure\n",
      "  sport\n",
      "  wikidata\n",
      "  check_date\n",
      "  fee\n",
      "  fixme\n",
      "  heritage\n",
      "  heritage:operator\n",
      "  historic\n",
      "  ref:nrhp\n",
      "  ship:type\n",
      "  wikipedia\n",
      "  building:part\n",
      "  disused:shop\n",
      "  type\n",
      "  aeroway\n",
      "  operator\n",
      "  short_name\n",
      "  parking\n",
      "  old_name\n",
      "  branch\n",
      "  air_conditioning\n",
      "  drive_through\n",
      "  opening_hours:drive_through\n",
      "  indoor_seating\n",
      "  outdoor_seating\n",
      "  delivery\n",
      "  payment:NFC_mobile_payments\n",
      "  smoking\n",
      "  access\n",
      "  brand:website\n",
      "  brand:wikipedia\n",
      "  denomination\n",
      "  religion\n",
      "  payment:cash\n",
      "  payment:contactless\n",
      "  payment:credit_cards\n",
      "  payment:debit_cards\n",
      "  payment:nfc\n",
      "  website:menu\n",
      "  wheelchair\n",
      "  operator:wikidata\n",
      "  drive_in\n",
      "  compressed_air\n",
      "  toilets\n",
      "  operator:short\n",
      "  operator:type\n",
      "  operator:website\n",
      "  operator:wikipedia\n",
      "  fuel:diesel\n",
      "  fuel:gasoline\n",
      "  self_service\n",
      "  bar\n",
      "  fax\n",
      "  internet_access\n",
      "  internet_access:fee\n",
      "  reservation\n",
      "  rooms\n",
      "  landuse\n",
      "  ref:walmart\n",
      "  atm\n",
      "  shelter_type\n",
      "  roof:shape\n",
      "  roof:levels\n",
      "  diet:chicken\n",
      "  diet:dairy\n",
      "  diet:meat\n",
      "  image\n",
      "  contact:instagram\n",
      "  description\n",
      "  second_hand\n",
      "  screen\n",
      "  bin\n",
      "  toilets:disposal\n",
      "  unisex\n",
      "  social_facility\n",
      "  social_facility:for\n",
      "  beauty\n",
      "  healthcare:speciality\n",
      "  clothes\n",
      "  dispensing\n",
      "  was:shop\n",
      "  check_date:opening_hours\n",
      "  service:vehicle:inspection\n",
      "  service:vehicle:oil_change\n",
      "  contact:foursquare\n",
      "  drinking_water\n",
      "  opening_hours:covid19\n",
      "  highchair\n",
      "  abandoned\n",
      "  automated\n",
      "  payment:coins\n",
      "  telecom\n",
      "  craft\n",
      "  after_school\n",
      "  isced:level\n",
      "  nursery\n",
      "  preschool\n",
      "  abandoned:building\n",
      "  microbrewery\n",
      "  contact:website\n",
      "  not:brand:wikidata\n",
      "  teaching\n",
      "  swimming_pool\n",
      "  bus\n",
      "  public_transport\n",
      "  contact:phone\n",
      "  service:vehicle:car_repair\n",
      "  building:min_level\n",
      "  max_level\n",
      "  min_level\n",
      "  theatre\n",
      "  dance:style\n",
      "  dance:teaching\n",
      "  disused\n",
      "  architect\n",
      "  building:use\n",
      "  diocese\n",
      "  building_1\n",
      "  urgent_care\n",
      "  emergency\n",
      "  lit\n",
      "  bridge\n",
      "  level\n",
      "  community_centre:for\n",
      "  construction_equipment:rental\n",
      "  tool:rental\n",
      "  condo\n",
      "  service:vehicle:car_parts\n",
      "  service:vehicle:new_car_sales\n",
      "  service:vehicle:used_car_sales\n",
      "  bench\n",
      "  government\n",
      "  payment:american_express\n",
      "  payment:mastercard\n",
      "  payment:visa\n",
      "  animal_shelter\n",
      "  animal_shelter:adoption\n",
      "  animal_shelter:release\n",
      "  pets\n",
      "  check_date:opening_hours:drive_through\n",
      "  fuel:octane_87\n",
      "  fuel:octane_89\n",
      "  fuel:octane_93\n",
      "  female\n",
      "  male\n",
      "  portable\n",
      "  toilets:handwashing\n",
      "  capacity\n",
      "  size\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Dropping extraneous columns from buildings_subset: ['addr:state', 'building', 'ele', 'gnis:feature_id', 'name', 'source', 'addr:city', 'addr:housename', 'addr:housenumber', 'addr:postcode', 'addr:street', 'amenity', 'brand', 'brand:wikidata', 'cuisine', 'healthcare', 'healthcare:counselling', 'phone', 'website', 'generator:method', 'generator:output:electricity', 'generator:source', 'generator:type', 'layer', 'power', 'shop', 'opening_hours', 'ref', 'wholesale', 'addr:unit', 'official_name', 'takeaway', 'note', 'email', 'museum', 'tourism', 'addr:country', 'alt_name', 'contact:email', 'contact:facebook', 'contact:twitter', 'man_made', 'content', 'office', 'start_date', 'building:levels', 'leisure', 'sport', 'wikidata', 'check_date', 'fee', 'fixme', 'heritage', 'heritage:operator', 'historic', 'ref:nrhp', 'ship:type', 'wikipedia', 'building:part', 'disused:shop', 'type', 'aeroway', 'operator', 'short_name', 'parking', 'old_name', 'branch', 'air_conditioning', 'drive_through', 'opening_hours:drive_through', 'indoor_seating', 'outdoor_seating', 'delivery', 'payment:NFC_mobile_payments', 'smoking', 'access', 'brand:website', 'brand:wikipedia', 'denomination', 'religion', 'payment:cash', 'payment:contactless', 'payment:credit_cards', 'payment:debit_cards', 'payment:nfc', 'website:menu', 'wheelchair', 'operator:wikidata', 'drive_in', 'compressed_air', 'toilets', 'operator:short', 'operator:type', 'operator:website', 'operator:wikipedia', 'fuel:diesel', 'fuel:gasoline', 'self_service', 'bar', 'fax', 'internet_access', 'internet_access:fee', 'reservation', 'rooms', 'landuse', 'ref:walmart', 'atm', 'shelter_type', 'roof:shape', 'roof:levels', 'diet:chicken', 'diet:dairy', 'diet:meat', 'image', 'contact:instagram', 'description', 'second_hand', 'screen', 'bin', 'toilets:disposal', 'unisex', 'social_facility', 'social_facility:for', 'beauty', 'healthcare:speciality', 'clothes', 'dispensing', 'was:shop', 'check_date:opening_hours', 'service:vehicle:inspection', 'service:vehicle:oil_change', 'contact:foursquare', 'drinking_water', 'opening_hours:covid19', 'highchair', 'abandoned', 'automated', 'payment:coins', 'telecom', 'craft', 'after_school', 'isced:level', 'nursery', 'preschool', 'abandoned:building', 'microbrewery', 'contact:website', 'not:brand:wikidata', 'teaching', 'swimming_pool', 'bus', 'public_transport', 'contact:phone', 'service:vehicle:car_repair', 'building:min_level', 'max_level', 'min_level', 'theatre', 'dance:style', 'dance:teaching', 'disused', 'architect', 'building:use', 'diocese', 'building_1', 'urgent_care', 'emergency', 'lit', 'bridge', 'level', 'community_centre:for', 'construction_equipment:rental', 'tool:rental', 'condo', 'service:vehicle:car_parts', 'service:vehicle:new_car_sales', 'service:vehicle:used_car_sales', 'bench', 'government', 'payment:american_express', 'payment:mastercard', 'payment:visa', 'animal_shelter', 'animal_shelter:adoption', 'animal_shelter:release', 'pets', 'check_date:opening_hours:drive_through', 'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male', 'portable', 'toilets:handwashing', 'capacity', 'size']\n",
      "-- Full column names in buildings_subset after dropping:\n",
      "  geometry\n",
      "  height\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Validating geometries in buildings_subset...\n",
      "-- All geometries are valid in buildings_subset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\1518612113.py:113: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saving blocks processed...\n",
      "-- Full column names in blocks_processed:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  GEOIDFQ20\n",
      "  NAME20\n",
      "  MTFCC20\n",
      "  UR20\n",
      "  UACE20\n",
      "  FUNCSTAT20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  INTPTLAT20\n",
      "  INTPTLON20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "  HHpBLD\n",
      "-- Dropping extraneous columns from blocks_processed: ['GEOIDFQ20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20', 'INTPTLAT20', 'INTPTLON20', 'HHpBLD']\n",
      "-- Full column names in blocks_processed after dropping:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "- Saving roads processed...\n",
      "-- Full column names in roads_processed:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  ref\n",
      "  reversed\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  bridge\n",
      "  junction\n",
      "  width\n",
      "  access\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "  agg_HHpBLD\n",
      "  length_cal\n",
      "  UrbanArea\n",
      "  emp_den\n",
      "-- Dropping extraneous columns from roads_processed: ['ref', 'reversed', 'bridge', 'junction', 'width', 'access']\n",
      "-- Full column names in roads_processed after dropping:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "  agg_HHpBLD\n",
      "  length_cal\n",
      "  UrbanArea\n",
      "  emp_den\n",
      "-- Trimming values in overlap_percs to 4 decimal places for roads_processed...\n",
      "Shapefiles saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\1518612113.py:136: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrames to shapefiles while ensuring the correct geometry column is activated.\n",
    "    Includes functionality to validate geometries only for buildings and trims `overlap_percs` in roads to 4 decimal places.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving subsets to shapefiles...\")\n",
    "\n",
    "    # Suppress warnings about truncated column names for ESRI Shapefiles\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Normalized/laundered field name.*\", category=RuntimeWarning)\n",
    "\n",
    "    def ensure_unique_column_names(df, name):\n",
    "        \"\"\"\n",
    "        Ensure column names in a GeoDataFrame are unique by appending trailing numbers to duplicates.\n",
    "        \"\"\"\n",
    "        duplicates = df.columns[df.columns.duplicated()].unique()\n",
    "        if len(duplicates) > 0:\n",
    "            print(f\"WARNING: Duplicate column names detected in {name}: {duplicates}\")\n",
    "            df = df.rename(columns=lambda x: x[:10])  # Truncate names to 10 characters for ESRI compatibility\n",
    "            # Resolve duplicates by appending a suffix\n",
    "            seen = set()\n",
    "            new_columns = []\n",
    "            for col in df.columns:\n",
    "                if col in seen:\n",
    "                    count = sum([existing.startswith(col) for existing in seen]) + 1\n",
    "                    new_col = f\"{col[:7]}_{count}\"  # Add numeric suffix to resolve duplicates\n",
    "                    print(f\"    Renaming column '{col}' to '{new_col}' to resolve duplication.\")\n",
    "                    new_columns.append(new_col)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "                seen.add(new_columns[-1])\n",
    "            df.columns = new_columns\n",
    "        return df\n",
    "\n",
    "    def print_columns(df, name):\n",
    "        \"\"\"\n",
    "        Print column names in their entirety for a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        print(f\"-- Full column names in {name}:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}\")\n",
    "\n",
    "    def drop_extraneous_columns(df, columns_to_keep, name):\n",
    "        \"\"\"\n",
    "        Drop columns not included in the `columns_to_keep` list, only if they exist in the dataset.\n",
    "        \"\"\"\n",
    "        columns_to_drop = [col for col in df.columns if col not in columns_to_keep]\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Ensure column exists\n",
    "        if columns_to_drop:\n",
    "            print(f\"-- Dropping extraneous columns from {name}: {columns_to_drop}\")\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "        print_columns(df, f\"{name} after dropping\")  # Print columns after dropping\n",
    "        return df\n",
    "\n",
    "    def validate_and_fix_geometries(df, name):\n",
    "        \"\"\"\n",
    "        Validate geometries in a GeoDataFrame, ensuring they are of the correct type.\n",
    "        Filter out invalid geometries or attempt to convert them to `Polygon`/`MultiPolygon`.\n",
    "        Applied **only** to buildings.\n",
    "        \"\"\"\n",
    "        print(f\"-- Validating geometries in {name}...\")\n",
    "        \n",
    "        # Filter valid geometries\n",
    "        valid_types = [\"Polygon\", \"MultiPolygon\"]\n",
    "        \n",
    "        # Separate invalid geometries\n",
    "        invalid_geometries = df[~df.geometry.type.isin(valid_types)]\n",
    "        if len(invalid_geometries) > 0:\n",
    "            print(f\"ERROR: Invalid geometry type detected in {name}. Expected Polygon or MultiPolygon.\")\n",
    "            print(f\"-- Invalid geometries: {len(invalid_geometries)}\")\n",
    "            \n",
    "            # Attempt to convert invalid geometries to polygons\n",
    "            print(\"-- Attempting to convert invalid geometries to valid Polygon/MultiPolygon...\")\n",
    "            def convert_to_polygon(geom):\n",
    "                # Try to buffer as a fix to convert Point/LineString geometries\n",
    "                if geom.is_valid:\n",
    "                    return geom.buffer(0)  # Buffer to create a polygon around invalid geometry\n",
    "                return None  # Skip if geometry cannot be converted\n",
    "                \n",
    "            # Apply conversion\n",
    "            df.loc[~df.geometry.type.isin(valid_types), \"geometry\"] = df.loc[~df.geometry.type.isin(valid_types), \"geometry\"].apply(convert_to_polygon)\n",
    "            \n",
    "            # Remove any geometries that could not be converted\n",
    "            df = df[df.geometry.type.isin(valid_types)]\n",
    "            print(f\"-- After conversion: {len(df)} valid polygons remaining.\")\n",
    "        else:\n",
    "            print(f\"-- All geometries are valid in {name}.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def trim_overlap_percs(df, column_name, name):\n",
    "        \"\"\"\n",
    "        Trim values in the `overlap_percs` field to 4 decimal places.\n",
    "        \"\"\"\n",
    "        print(f\"-- Trimming values in {column_name} to 4 decimal places for {name}...\")\n",
    "        if column_name in df.columns:\n",
    "            df[column_name] = df[column_name].apply(lambda x: [round(value, 4) for value in x])\n",
    "        return df\n",
    "\n",
    "    # Process buildings subset\n",
    "    print(\"- Saving buildings subset...\")\n",
    "    print_columns(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset = drop_extraneous_columns(buildings_subset, columns_to_keep=[\n",
    "        \"geometry\", \"height\", \"setback_ft\", \"build_idx\", \"build_area_sm\", \"height_m\", \"height_ft\"\n",
    "    ], name=\"buildings_subset\")\n",
    "    buildings_subset = validate_and_fix_geometries(buildings_subset, \"buildings_subset\")  # Apply validation/fixing ONLY here\n",
    "    buildings_subset = ensure_unique_column_names(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n",
    "\n",
    "    # Process blocks subset\n",
    "    print(\"- Saving blocks processed...\")\n",
    "    print_columns(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed = drop_extraneous_columns(blocks_processed, columns_to_keep=[\n",
    "        \"STATEFP20\", \"COUNTYFP20\", \"TRACTCE20\", \"BLOCKCE20\", \"GEOID20\",\n",
    "        \"ALAND20\", \"AWATER20\", \"HOUSING20\", \"POP20\", \"geometry\", \"BLOCK_ID\", \"area_sm\",\n",
    "        \"pop_den\", \"avg_hght\", \"avg_sback\", \"bld_area\", \"bld_cnt\", \"bld_ctsm\", \"bld_prc\"\n",
    "    ], name=\"blocks_processed\")\n",
    "    blocks_processed = ensure_unique_column_names(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed.to_file(f\"{output_dir}/Corpus_Christi_blocks_processed.shp\")\n",
    "\n",
    "    # Process roads subset\n",
    "    print(\"- Saving roads processed...\")\n",
    "    print_columns(roads_processed, \"roads_processed\")\n",
    "    roads_processed = drop_extraneous_columns(roads_processed, columns_to_keep=[\n",
    "        \"osmid\", \"highway\", \"lanes\", \"name\", \"oneway\", \"length\", \"maxspeed\", \"geometry\",\n",
    "        \"road_id\", \"agg_pop\", \"agg_area\", \"agg_ctsm\", \"agg_bldprc\", \"agg_HHpBLD\", \"agg_hght\", \"agg_sback\",\n",
    "        \"block_ids\", \"overlap_percs\", \"evac_flag\", \"length_cal\", \"UrbanArea\", \"emp_den\"\n",
    "    ], name=\"roads_processed\")\n",
    "    roads_processed = trim_overlap_percs(roads_processed, \"overlap_percs\", \"roads_processed\")  # Trim `overlap_percs`\n",
    "    roads_processed = ensure_unique_column_names(roads_processed, \"roads_processed\")\n",
    "    roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n",
    "\n",
    "    print(\"Shapefiles saved successfully.\")\n",
    "\n",
    "# Save the processed data to shapefiles\n",
    "output_dir = \"output/rev7\"\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0d494-81e3-4da7-acab-24623d288483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Running decision tree method...\n",
      "- Reading roads shapefile...\n",
      "- Reading classification rules CSV...\n",
      "- Reading classification priority CSV...\n",
      "- Applying classification rules...\n",
      "- Resolving classifications to single priority-based classification...\n",
      "- Saving updated shapefile to: output/rev7/Corpus_Christi_roads_processed_decisiontree.shp\n",
      "- Decision tree classification completed and file saved.\n",
      "- Running metric-based classification method...\n",
      "- Reading roads shapefile...\n",
      "- Reading metrics CSV...\n",
      "-- Metrics loaded. 25 rows to process.\n",
      "- Reading classification priority CSV...\n",
      "- Initializing classification data structures...\n",
      "- Applying metric-based classifications...\n",
      "- Resolving final classification using priority...\n",
      "- Saving updated shapefile to: output/rev7/Corpus_Christi_roads_processed_metrics.shp\n",
      "- Metric-based classification completed and file saved.\n"
     ]
    }
   ],
   "source": [
    "import os  # Import os for directory creation\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Function to resolve classifications into priority-based order\n",
    "def resolve_to_priority(class_list: list, priority_mapping: dict, default_classification: str = \"NOT FOUND IN LIST\") -> str:\n",
    "    \"\"\"\n",
    "    Resolves multiple classifications to a single one based on priority csv\n",
    "    \"\"\"\n",
    "    if len(class_list) == 0:\n",
    "        return default_classification\n",
    "    valid_classes = [cls for cls in class_list if cls in priority_mapping]\n",
    "    if len(valid_classes) == 0:\n",
    "        return default_classification\n",
    "    return sorted(valid_classes, key=lambda c: priority_mapping[c])[0]\n",
    "\n",
    "# Function for decision-tree-based classification\n",
    "def classify_using_decision_tree(input_shp: str, rules_csv: str, priority_csv: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Classifies roads using decision tree rules and resolves classifications based on priority.\n",
    "    Saves a shapefile with classifications prioritized\n",
    "    \"\"\"\n",
    "    print(\"- Reading roads shapefile...\")\n",
    "    roads_gdf = gpd.read_file(input_shp)\n",
    "    print(\"- Reading classification rules CSV...\")\n",
    "    rules = pd.read_csv(rules_csv)\n",
    "    print(\"- Reading classification priority CSV...\")\n",
    "    priority = pd.read_csv(priority_csv)\n",
    "    priority_mapping = dict(zip(priority[\"Classification\"], priority[\"Priority\"]))\n",
    "    default_classification = \"NOT FOUND IN LIST\"\n",
    "\n",
    "    # Initialize empty classifications for roads\n",
    "    roads_gdf[\"All_Class\"] = [[] for _ in range(len(roads_gdf))]\n",
    "\n",
    "    print(\"- Applying classification rules...\")\n",
    "    for _, rule in rules.iterrows():\n",
    "        # Parse conditions, operators, and values from rule\n",
    "        conditions = []\n",
    "        for i in range(1, 10):  # Process up to 9 conditions\n",
    "            condition = rule.get(f\"Condition_{i}\")\n",
    "            operator = rule.get(f\"Operator_{i}\")\n",
    "            value = rule.get(f\"Value_{i}\")\n",
    "\n",
    "            # Skip if any part of the condition is missing\n",
    "            if pd.isna(condition) or pd.isna(operator) or pd.isna(value):\n",
    "                continue\n",
    "\n",
    "            # Format the value properly (add quotes for string, leave numeric as is)\n",
    "            if isinstance(value, str):\n",
    "                value = f'\"{value}\"'\n",
    "            else:\n",
    "                value = str(value)\n",
    "\n",
    "            # Append the condition as a valid query string\n",
    "            conditions.append(f\"({condition} {operator} {value})\")\n",
    "\n",
    "        # Combine conditions into a query string\n",
    "        if len(conditions) > 0:\n",
    "            query = \" and \".join(conditions)\n",
    "        else:\n",
    "            continue  # Skip rules without valid conditions\n",
    "\n",
    "        # Classification from rule\n",
    "        classification = rule[\"Classification\"]\n",
    "\n",
    "        try:\n",
    "            # Evaluate query and get matching road indices\n",
    "            matching_indices = roads_gdf.query(query).index\n",
    "            for idx in matching_indices:\n",
    "                roads_gdf.at[idx, \"All_Class\"].append(classification)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying rule: {rule}\")\n",
    "            print(f\"Generated query: {query}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            continue  # Skip to the next rule\n",
    "\n",
    "    print(\"- Resolving classifications to single priority-based classification...\")\n",
    "    roads_gdf[\"FINALCLASS\"] = roads_gdf[\"All_Class\"].apply(\n",
    "        lambda class_list: resolve_to_priority(class_list, priority_mapping, default_classification)\n",
    "    )\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the resulting shapefile\n",
    "    output_shp = f\"{output_dir}/Corpus_Christi_roads_processed_decisiontree.shp\"\n",
    "    print(f\"- Saving updated shapefile to: {output_shp}\")\n",
    "    roads_gdf.to_file(output_shp, driver=\"ESRI Shapefile\")\n",
    "    print(\"- Decision tree classification completed and file saved.\")\n",
    "\n",
    "# Function for point-based metric classification\n",
    "def classify_using_metrics(input_shp: str, metrics_csv: str, priority_csv: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Classifies roads based on metrics using ranges, weights, and priorities\n",
    "    Resolves classification ties using the resolve_to_priority function\n",
    "    Saves a shapefile with classifications\n",
    "    \"\"\"\n",
    "    print(\"- Reading roads shapefile...\")\n",
    "    roads_gdf = gpd.read_file(input_shp)\n",
    "    print(\"- Reading metrics CSV...\")\n",
    "    metrics = pd.read_csv(metrics_csv)\n",
    "    metrics = metrics[metrics[\"Metric\"].notna()]\n",
    "    print(f\"-- Metrics loaded. {len(metrics)} rows to process.\")\n",
    "    print(\"- Reading classification priority CSV...\")\n",
    "    priority = pd.read_csv(priority_csv)\n",
    "    priority_mapping = dict(zip(priority[\"Classification\"], priority[\"Priority\"]))\n",
    "    default_classification = \"Unclassified\"\n",
    "\n",
    "    print(\"- Initializing classification data structures...\")\n",
    "    roads_gdf[\"Class_PNT\"] = [{} for _ in range(len(roads_gdf))]\n",
    "\n",
    "    print(\"- Applying metric-based classifications...\")\n",
    "    for _, rule in metrics.iterrows():\n",
    "        metric = rule[\"Metric\"]\n",
    "        min_value = rule[\"Min\"]\n",
    "        max_value = rule[\"Max\"]\n",
    "        weight = rule[\"Weight\"]\n",
    "        classification = rule[\"Classification\"]\n",
    "\n",
    "        # Build query conditions for the metric range\n",
    "        query = f\"{metric} > {min_value}\" if not pd.isna(min_value) else \"\"\n",
    "        if not pd.isna(max_value):\n",
    "            query += f\" and {metric} <= {max_value}\" if query else f\"{metric} <= {max_value}\"\n",
    "\n",
    "        # Apply query to get matching indices\n",
    "        matching_indices = roads_gdf.query(query).index\n",
    "        for idx in matching_indices:\n",
    "            # Update points for classification\n",
    "            roads_gdf.at[idx, \"Class_PNT\"].setdefault(classification, 0)\n",
    "            roads_gdf.at[idx, \"Class_PNT\"][classification] += weight\n",
    "\n",
    "    print(\"- Resolving final classification using priority...\")\n",
    "    def resolve_tie(classes_points):\n",
    "        \"\"\"\n",
    "        Resolves the classification when point values tie\n",
    "        Uses the resolve_to_priority function to determine the final priority-based classification\n",
    "        \"\"\"\n",
    "        if not classes_points:\n",
    "            return default_classification\n",
    "        max_point_value = max(classes_points.values())\n",
    "        tied_classes = [\n",
    "            c for c, points in classes_points.items() if points == max_point_value\n",
    "        ]\n",
    "        if len(tied_classes) == 1:  # No tie\n",
    "            return tied_classes[0]\n",
    "        # Resolve tie using priority mapping\n",
    "        return resolve_to_priority(tied_classes, priority_mapping, default_classification)\n",
    "\n",
    "    # Apply tie-resolution logic to each row\n",
    "    roads_gdf[\"FINALCLASS\"] = roads_gdf[\"Class_PNT\"].apply(resolve_tie)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the resulting shapefile\n",
    "    output_shp = f\"{output_dir}/Corpus_Christi_roads_processed_metrics.shp\"\n",
    "    print(f\"- Saving updated shapefile to: {output_shp}\")\n",
    "    roads_gdf.to_file(output_shp, driver=\"ESRI Shapefile\")\n",
    "    print(\"- Metric-based classification completed and file saved.\")\n",
    "\n",
    "# CHANGE VALUES HERE FOR YOUR INPUTS\n",
    "input_shp_path = \"output/rev7/Corpus_Christi_roads_processed.shp\"\n",
    "rules_csv_path = \"decisiontree_3.csv\"\n",
    "priority_csv_path = \"Classification_Priority.csv\"\n",
    "metrics_csv_path = \"Classification_Point.csv\"\n",
    "output_directory = \"output/rev7\"\n",
    "\n",
    "# Run Decision Tree Classification\n",
    "print(\"- Running decision tree method...\")\n",
    "classify_using_decision_tree(\n",
    "    input_shp=input_shp_path,\n",
    "    rules_csv=rules_csv_path,\n",
    "    priority_csv=priority_csv_path,\n",
    "    output_dir=output_directory\n",
    ")\n",
    "\n",
    "# Run Metric-Based Classification\n",
    "print(\"- Running metric-based classification method...\")\n",
    "classify_using_metrics(\n",
    "    input_shp=input_shp_path,\n",
    "    metrics_csv=metrics_csv_path,\n",
    "    priority_csv=priority_csv_path,\n",
    "    output_dir=output_directory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
