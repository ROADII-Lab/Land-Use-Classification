{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Converting center coordinates (27.783611, -97.414779) to UTM...\n",
      "Corpus Christi center in UTM (EPSG:32614): (656184.519263486, 3074239.5261650323)\n",
      "- Checking cache for blocks...\n",
      "Loading data from cache: cache\\blocks.pkl\n",
      "- Checking cache for buildings...\n",
      "Loading data from cache: cache\\buildings.pkl\n",
      "- Checking cache for roads...\n",
      "Loading data from cache: cache\\roads.pkl\n",
      "- Subsampling datasets...\n",
      "- Subsampling data using bounding box...\n",
      "-- Bounding box geometry: POLYGON ((662280.519263486 3068143.5261650323, 662280.519263486 3080335.5261650323, 650088.519263486 3080335.5261650323, 650088.519263486 3068143.5261650323, 662280.519263486 3068143.5261650323))\n",
      "-- Blocks CRS: EPSG:32614\n",
      "-- Buildings CRS before reprojection: EPSG:32614\n",
      "-- Roads CRS: EPSG:32614\n",
      "-- Buildings CRS after reprojection: EPSG:32614\n",
      "-- Buildings subset shape after intersects: (41003, 198)\n",
      "-- Subsampled 2407 blocks, 41003 buildings, and 11328 roads.\n",
      "- Data loading complete.\n",
      "Calculating building setbacks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\541574086.py:189: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Blocks subset:\n",
      "       STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "211079        48        355    001904      2001  483550019042001   \n",
      "211080        48        355    002002      2008  483550020022008   \n",
      "211081        48        355    001300      4003  483550013004003   \n",
      "211082        48        355    001300      4004  483550013004004   \n",
      "211083        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                       GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20 FUNCSTAT20  \\\n",
      "211079  1000000US483550019042001  Block2001   G5040    U  20287          S   \n",
      "211080  1000000US483550020022008  Block2008   G5040    U  20287          S   \n",
      "211081  1000000US483550013004003  Block4003   G5040    U  20287          S   \n",
      "211082  1000000US483550013004004  Block4004   G5040    U  20287          S   \n",
      "211083  1000000US483550013004012  Block4012   G5040    U  20287          S   \n",
      "\n",
      "        ALAND20  AWATER20   INTPTLAT20    INTPTLON20  HOUSING20  POP20  \\\n",
      "211079    35973         0  +27.7446668  -097.4119843         49    124   \n",
      "211080    35948         0  +27.7448396  -097.4071982         46    111   \n",
      "211081    24467         0  +27.7697013  -097.4050378         32     77   \n",
      "211082    17105         0  +27.7691166  -097.4057168          0      0   \n",
      "211083    25512         0  +27.7677189  -097.4037248         42     88   \n",
      "\n",
      "                                                 geometry  \n",
      "211079  POLYGON ((656383.497 3069802.332, 656388.176 3...  \n",
      "211080  POLYGON ((656781.143 3070143.675, 656831.652 3...  \n",
      "211081  POLYGON ((657076.274 3072786.758, 657080.514 3...  \n",
      "211082  POLYGON ((657029.407 3072699.161, 657058.753 3...  \n",
      "211083  POLYGON ((657185.832 3072577.964, 657270.53 30...  \n",
      "-- Buildings subset:\n",
      "                                          geometry addr:state building  ele  \\\n",
      "element id                                                                    \n",
      "node    368160094   POINT (657977.733 3075774.146)         TX      yes   10   \n",
      "        368164119   POINT (658027.202 3076877.861)         TX    house    1   \n",
      "        368164527   POINT (657937.057 3074696.302)         TX      yes   11   \n",
      "        368164702   POINT (658013.486 3076875.577)         TX    house    1   \n",
      "        8223520640  POINT (658135.205 3075904.051)         TX      yes  NaN   \n",
      "\n",
      "                   gnis:feature_id                        name         source  \\\n",
      "element id                                                                      \n",
      "node    368160094          2031465  Broadway Bluff Improvement  USGS Geonames   \n",
      "        368164119          2031422     Charlotte Sidbury House  USGS Geonames   \n",
      "        368164527          2031675          Richard King House  USGS Geonames   \n",
      "        368164702          2032681                      S Juli  USGS Geonames   \n",
      "        8223520640             NaN        Treatment Associates            NaN   \n",
      "\n",
      "                         addr:city addr:housename addr:housenumber  ...  \\\n",
      "element id                                                          ...   \n",
      "node    368160094              NaN            NaN              NaN  ...   \n",
      "        368164119              NaN            NaN              NaN  ...   \n",
      "        368164527              NaN            NaN              NaN  ...   \n",
      "        368164702              NaN            NaN              NaN  ...   \n",
      "        8223520640  Corpus Christi            NaN              720  ...   \n",
      "\n",
      "                   fuel:octane_87 fuel:octane_89 fuel:octane_93 female male  \\\n",
      "element id                                                                    \n",
      "node    368160094             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164119             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164527             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164702             NaN            NaN            NaN    NaN  NaN   \n",
      "        8223520640            NaN            NaN            NaN    NaN  NaN   \n",
      "\n",
      "                   portable toilets:handwashing capacity size  setback_ft  \n",
      "element id                                                                 \n",
      "node    368160094       NaN                 NaN      NaN  NaN    8.142634  \n",
      "        368164119       NaN                 NaN      NaN  NaN   45.583627  \n",
      "        368164527       NaN                 NaN      NaN  NaN  124.052313  \n",
      "        368164702       NaN                 NaN      NaN  NaN   44.269826  \n",
      "        8223520640      NaN                 NaN      NaN  NaN   60.298453  \n",
      "\n",
      "[5 rows x 199 columns]\n",
      "-- Roads subset:\n",
      "                             osmid      highway lanes                  name  \\\n",
      "u         v         key                                                       \n",
      "227454417 227539516 0     21156845  residential   NaN      Winnebago Street   \n",
      "          227491475 0     21156845  residential   NaN      Winnebago Street   \n",
      "          227536659 0     21160211    secondary     2     North Port Avenue   \n",
      "          227579522 0     21160211    secondary     2     North Port Avenue   \n",
      "227454455 227538593 0    831149706    secondary   NaN  South Alameda Street   \n",
      "\n",
      "                         oneway  ref reversed      length maxspeed  \\\n",
      "u         v         key                                              \n",
      "227454417 227539516 0     False  NaN    False  111.207782      NaN   \n",
      "          227491475 0     False  NaN     True  239.351195      NaN   \n",
      "          227536659 0     False  NaN    False   91.759789   40 mph   \n",
      "          227579522 0     False  NaN     True  181.553317   40 mph   \n",
      "227454455 227538593 0     False  NaN    False   98.207032      NaN   \n",
      "\n",
      "                                                                  geometry  \\\n",
      "u         v         key                                                      \n",
      "227454417 227539516 0    LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "          227491475 0    LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "          227536659 0    LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "          227579522 0    LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "227454455 227538593 0    LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "                        bridge junction width access  \n",
      "u         v         key                               \n",
      "227454417 227539516 0      NaN      NaN   NaN    NaN  \n",
      "          227491475 0      NaN      NaN   NaN    NaN  \n",
      "          227536659 0      NaN      NaN   NaN    NaN  \n",
      "          227579522 0      NaN      NaN   NaN    NaN  \n",
      "227454455 227538593 0      NaN      NaN   NaN    NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "### Utility Functions ###\n",
    "def read_path_from_file(file_path: str) -> str:\n",
    "    \"\"\"Read OneDrive path from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            path = file.readline().strip()  # Read the first line and strip whitespace\n",
    "        return path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_cache(data, filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Save a GeoDataFrame or Python object to a pickle file in the cache directory.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    print(f\"Saving data to cache: {cache_path}\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_from_cache(filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Load a GeoDataFrame or Python object from a pickle file in the cache directory.\"\"\"\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading data from cache: {cache_path}\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def latlon_to_utm(lat: float, lon: float, epsg: int = 32614):\n",
    "    \"\"\"\n",
    "    Convert latitude and longitude (EPSG:4326) to UTM coordinates (e.g., EPSG:32614).\n",
    "    \"\"\"\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg}\", always_xy=True)\n",
    "    x, y = transformer.transform(lon, lat)  # Transform to target coordinates\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def subsample_data(center_point: tuple, scale: float, blocks: gpd.GeoDataFrame, buildings: gpd.GeoDataFrame, roads: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Subsample blocks, buildings, and roads using a bounding box around a central point.\n",
    "    Ensures proper alignment of CRS before subsampling.\n",
    "    \"\"\"\n",
    "    print(\"- Subsampling data using bounding box...\")\n",
    "\n",
    "    # Default bounding box size: ~10,000 feet (~3,048 meters) on each side\n",
    "    default_bbox_size = 3048  # Half the total size on each dimension, in meters\n",
    "    scaled_bbox_size = default_bbox_size * scale\n",
    "\n",
    "    # Calculate bounding box geometry based on scaled size\n",
    "    center_x, center_y = center_point\n",
    "    bbox = box(\n",
    "        center_x - scaled_bbox_size,  # Min X\n",
    "        center_y - scaled_bbox_size,  # Min Y\n",
    "        center_x + scaled_bbox_size,  # Max X\n",
    "        center_y + scaled_bbox_size   # Max Y\n",
    "    )\n",
    "    print(\"-- Bounding box geometry:\", bbox)\n",
    "\n",
    "    # Debugging: Check CRS alignment for all datasets\n",
    "    print(\"-- Blocks CRS:\", blocks.crs)\n",
    "    print(\"-- Buildings CRS before reprojection:\", buildings.crs)\n",
    "    print(\"-- Roads CRS:\", roads.crs)\n",
    "\n",
    "    # Reproject buildings to match the CRS of the bounding box (EPSG:32614)\n",
    "    if buildings.crs.to_string().lower() != \"epsg:32614\":\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "    print(\"-- Buildings CRS after reprojection:\", buildings.crs)\n",
    "\n",
    "    # Filter each GeoDataFrame by the bounding box intersection\n",
    "    blocks_subset = blocks[blocks.geometry.intersects(bbox)]\n",
    "    buildings_subset = buildings[buildings.geometry.intersects(bbox)]\n",
    "    roads_subset = roads[roads.geometry.intersects(bbox)]\n",
    "\n",
    "    # Debugging: Check if any buildings are retained in the subset\n",
    "    print(\"-- Buildings subset shape after intersects:\", buildings_subset.shape)\n",
    "    if len(buildings_subset) == 0:\n",
    "        print(\"-- Warning: No buildings found within the bounding box.\")\n",
    "        print(\"-- Original buildings dataset sample (after reprojection):\")\n",
    "        print(buildings.geometry.head())  # Inspect original geometries\n",
    "\n",
    "    print(f\"-- Subsampled {len(blocks_subset)} blocks, {len(buildings_subset)} buildings, and {len(roads_subset)} roads.\")\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "def load_data(block_path, osm_boundary_place, cache_dir=\"cache\", subsample_scale=1.0):\n",
    "    \"\"\"\n",
    "    Load blocks, buildings, and roads data from cache (if available), process, and optionally subsample using a scaled bounding box.\n",
    "    Uses only the 'height' column for building heights and outputs the percentage of valid values.\n",
    "    \"\"\"\n",
    "    # Corpus Christi center point from Google Maps\n",
    "    corpus_christi_lat = 27.783611  # Latitude\n",
    "    corpus_christi_lon = -97.414779  # Longitude\n",
    "\n",
    "    print(f\"Converting center coordinates ({corpus_christi_lat}, {corpus_christi_lon}) to UTM...\")\n",
    "    corpus_christi_center_utm = latlon_to_utm(corpus_christi_lat, corpus_christi_lon)  # Convert to UTM (EPSG:32614)\n",
    "    CC_Padre_center_utm = latlon_to_utm(27.614956, -97.269562)\n",
    "    print(f\"Corpus Christi center in UTM (EPSG:32614): {corpus_christi_center_utm}\")\n",
    "\n",
    "    # Check cache for blocks\n",
    "    print(\"- Checking cache for blocks...\")\n",
    "    blocks = load_from_cache(\"blocks.pkl\", cache_dir)\n",
    "    if blocks is None:\n",
    "        print(\"-- Loading block data...\")\n",
    "        blocks = gpd.read_file(block_path).to_crs(epsg=32614)\n",
    "        blocks.loc[:, \"POP20\"] = pd.to_numeric(blocks[\"POP20\"], errors=\"coerce\")\n",
    "        save_to_cache(blocks, \"blocks.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for buildings\n",
    "    print(\"- Checking cache for buildings...\")\n",
    "    buildings = load_from_cache(\"buildings.pkl\", cache_dir)\n",
    "    if buildings is None:\n",
    "        print(\"-- Fetching building data...\")\n",
    "        buildings = ox.features_from_place(\n",
    "            osm_boundary_place,\n",
    "            tags={\"building\": True, \"building:height\": True, \"building:levels\": True}\n",
    "        )\n",
    "\n",
    "        print(\"-- Raw buildings dataset columns:\")\n",
    "        print(buildings.columns)  # Print available columns for validation\n",
    "        print(\"-- Preview of raw building dataset:\")\n",
    "        print(buildings.head())  # Inspect raw data for possible errors\n",
    "\n",
    "        if \"height\" in buildings.columns:\n",
    "            print(\"-- 'height' column found. Attempting numerical conversion...\")\n",
    "            buildings.loc[:, \"height\"] = pd.to_numeric(buildings[\"height\"], errors=\"coerce\")\n",
    "\n",
    "            # Debugging step to Calculate percentage of valid values (>0 and numeric)\n",
    "            total_height_values = len(buildings)\n",
    "            valid_height_values = buildings[\"height\"].dropna().gt(0).sum()\n",
    "            percentage_valid = (valid_height_values / total_height_values) * 100 if total_height_values > 0 else 0\n",
    "            print(f\"-- Valid height values: {valid_height_values} / Total: {total_height_values} ({percentage_valid:.2f}%)\")\n",
    "\n",
    "            print(\"-- Converted 'height' values to numeric. Example values:\")\n",
    "            print(buildings[\"height\"].head())\n",
    "        else:\n",
    "            print(\"-- ERROR: 'height' column is missing in the dataset! Setting height to None.\")\n",
    "            buildings.loc[:, \"height\"] = None\n",
    "\n",
    "        # Reproject buildings to CRS: EPSG:32614\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "        # Save the buildings dataset to cache\n",
    "        save_to_cache(buildings, \"buildings.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for roads\n",
    "    print(\"- Checking cache for roads...\")\n",
    "    roads = load_from_cache(\"roads.pkl\", cache_dir)\n",
    "    if roads is None:\n",
    "        print(\"-- Fetching road data...\")\n",
    "        roads = ox.graph_to_gdfs(ox.graph_from_place(osm_boundary_place, network_type=\"drive\"), nodes=False)\n",
    "        roads = roads.to_crs(epsg=32614)\n",
    "        save_to_cache(roads, \"roads.pkl\", cache_dir)\n",
    "\n",
    "    # Subsample datasets using the bounding box around Corpus Christi center in UTM\n",
    "    print(\"- Subsampling datasets...\")\n",
    "    # CC Padre Island as center of box\n",
    "    #blocks_subset, buildings_subset, roads_subset = subsample_data(CC_Padre_center_utm, subsample_scale, blocks, buildings, roads)\n",
    "    # CC Corpus Christi Downtown as the center of box\n",
    "    blocks_subset, buildings_subset, roads_subset = subsample_data(corpus_christi_center_utm, subsample_scale, blocks, buildings, roads) #COMMENT OUT IF RUNNING ON ALL OF CC\n",
    "    print(\"- Data loading complete.\")\n",
    "\n",
    "    '''\n",
    "    # IF you want to calculate all of Corpus Christi, uncomment the lines below and comment out the lines above for subsampling\n",
    "    blocks_subset = blocks\n",
    "    building_subset = buildings\n",
    "    road_subset = roads\n",
    "    '''\n",
    "    \n",
    "    # Calculate building setbacks (distance to nearest road)\n",
    "    print(\"Calculating building setbacks...\")\n",
    "    roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
    "    buildings_subset[\"setback_ft\"] = buildings_subset.geometry.apply(\n",
    "        lambda building_geom: building_geom.distance(roads_union) * 3.28084  # Convert meters to feet\n",
    "    )\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "# Define paths to block shapefile and Corpus Christi boundary\n",
    "one_drive_path = read_path_from_file(\"OneDrive.txt\")\n",
    "block_path = os.path.join(one_drive_path, \"Data\", \"tl_2023_48_tabblock20\", \"tl_2023_48_tabblock20.shp\")\n",
    "osm_boundary_place = \"Corpus Christi, Texas, USA\"\n",
    "\n",
    "# Load data with subsampling\n",
    "print(\"Starting data loading...\")\n",
    "blocks_subset, buildings_subset, roads_subset = load_data(block_path, osm_boundary_place, subsample_scale=0.25) # CHANGE SCALE OF BOUNDING BOX HERE\n",
    "\n",
    "# Inspect subsampled data\n",
    "print(\"-- Blocks subset:\")\n",
    "print(blocks_subset.head())\n",
    "print(\"-- Buildings subset:\")\n",
    "print(buildings_subset.head())\n",
    "print(\"-- Roads subset:\")\n",
    "print(roads_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc4d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in buildings dataset:\n",
      "Index(['geometry', 'addr:state', 'building', 'ele', 'gnis:feature_id', 'name',\n",
      "       'source', 'addr:city', 'addr:housename', 'addr:housenumber',\n",
      "       ...\n",
      "       'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male',\n",
      "       'portable', 'toilets:handwashing', 'capacity', 'size', 'setback_ft'],\n",
      "      dtype='object', length=199)\n",
      "Calculating block metrics...\n",
      "Calculating building area coverage (sq-mile)...\n",
      "Processing building heights...\n",
      "Associating buildings with blocks via spatial join...\n",
      "Aggregating building metrics per block...\n",
      "Calculating population density and other metrics for blocks...\n",
      "Calculating additional block metrics: Building count per square mile, building area percentage, and households per building count...\n",
      "Calculating Households per Building Count (HHpBLD)...\n",
      "Merging building metrics into block dataset...\n",
      "Filling missing values for block metrics...\n",
      "Block metrics calculated successfully.\n",
      "Block metrics successfully calculated:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    002002      2008  483550020022008   \n",
      "2        48        355    001300      4003  483550013004003   \n",
      "3        48        355    001300      4004  483550013004004   \n",
      "4        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... BLOCK_ID  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...   211079   \n",
      "1  1000000US483550020022008  Block2008   G5040    U  20287  ...   211080   \n",
      "2  1000000US483550013004003  Block4003   G5040    U  20287  ...   211081   \n",
      "3  1000000US483550013004004  Block4004   G5040    U  20287  ...   211082   \n",
      "4  1000000US483550013004012  Block4012   G5040    U  20287  ...   211083   \n",
      "\n",
      "    area_sm      pop_den   avg_hght  avg_sback  bld_area  bld_cnt  \\\n",
      "0  0.013887  8929.477778  33.216162  54.357708  0.002815     49.0   \n",
      "1  0.013877  7998.964124  32.435641  44.567143  0.002327     47.0   \n",
      "2  0.009445  8152.607692  32.740521  55.002728  0.001837     29.0   \n",
      "3  0.006603     0.000000  32.414699  33.839178  0.002782      1.0   \n",
      "4  0.009849  8935.348238  32.753367  61.108800  0.002250     31.0   \n",
      "\n",
      "      bld_ctsm    bld_prc    HHpBLD  \n",
      "0  3528.583961  20.269451  1.000000  \n",
      "1  3386.948773  16.770413  0.978723  \n",
      "2  3070.462637  19.445682  1.103448  \n",
      "3   151.448236  42.140090  0.000000  \n",
      "4  3147.679493  22.849059  1.354839  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_block_metrics(blocks_subset: gpd.GeoDataFrame, buildings_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate block-level metrics, including a new HHpBLD metric based on households per building count.\n",
    "    \"\"\"\n",
    "    print(\"Available columns in buildings dataset:\")\n",
    "    print(buildings_subset.columns)\n",
    "    # Check if required columns exist in the buildings dataset\n",
    "    required_columns = [\"geometry\", \"height\"]\n",
    "    missing_columns = [col for col in required_columns if col not in buildings_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing required columns in buildings dataset: {missing_columns}\")\n",
    "        print(\"Preview of buildings dataset:\")\n",
    "        print(buildings_subset.head())\n",
    "        raise KeyError(f\"Required columns {missing_columns} are not found in buildings dataset.\")\n",
    "\n",
    "    print(\"Calculating block metrics...\")\n",
    "    blocks_subset[\"BLOCK_ID\"] = blocks_subset.index  # Assign unique ID to each block\n",
    "    buildings_subset[\"build_idx\"] = buildings_subset.index  # Assign unique ID to each building\n",
    "    \n",
    "    # Calculate building area (square meters -> square miles)\n",
    "    print(\"Calculating building area coverage (sq-mile)...\")\n",
    "    buildings_subset[\"build_area_sm\"] = buildings_subset.geometry.area * 0.00000038610215855  # Convert area to sq-mile\n",
    "    \n",
    "    # Handle building height (convert meters to feet and filter valid heights)\n",
    "    print(\"Processing building heights...\")\n",
    "    buildings_subset[\"height_m\"] = pd.to_numeric(buildings_subset[\"height\"], errors=\"coerce\")\n",
    "    buildings_subset[\"height_ft\"] = buildings_subset[\"height_m\"] * 3.28084  # Convert height to feet\n",
    "    buildings_subset = buildings_subset[buildings_subset[\"height_ft\"] > 0]  # Filter buildings with positive heights\n",
    "    \n",
    "    # Spatial join to associate buildings with blocks\n",
    "    print(\"Associating buildings with blocks via spatial join...\")\n",
    "    buildings_with_blocks = gpd.sjoin(buildings_subset, blocks_subset, how=\"inner\", predicate=\"intersects\")\n",
    "    \n",
    "    # Aggregate metrics per block\n",
    "    print(\"Aggregating building metrics per block...\")\n",
    "    building_metrics = buildings_with_blocks.groupby(\"BLOCK_ID\").agg(\n",
    "        avg_hght=(\"height_ft\", \"mean\"),  # Average building height\n",
    "        avg_sback=(\"setback_ft\", \"mean\"),  # Average setback distance\n",
    "        bld_area=(\"build_area_sm\", \"sum\"),  # Sum building areas\n",
    "        bld_cnt=(\"build_idx\", \"count\"),  # Count number of buildings\n",
    "    )\n",
    "    \n",
    "    # Calculate population density for blocks\n",
    "    print(\"Calculating population density and other metrics for blocks...\")\n",
    "    blocks_subset[\"area_sm\"] = blocks_subset.geometry.area * 0.00000038610215855  # Convert block area to sq-mile\n",
    "    blocks_subset[\"pop_den\"] = blocks_subset[\"POP20\"] / blocks_subset[\"area_sm\"]  # Population density per sq-mile\n",
    "    \n",
    "    # additional metrics\n",
    "    print(\"Calculating additional block metrics: Building count per square mile, building area percentage, and households per building count...\")\n",
    "    building_metrics[\"bld_ctsm\"] = building_metrics[\"bld_cnt\"] / blocks_subset[\"area_sm\"]  # Building count per sq-mile\n",
    "    building_metrics[\"bld_prc\"] = (building_metrics[\"bld_area\"] / blocks_subset[\"area_sm\"]) * 100  # Building area as percentage of block area\n",
    "    \n",
    "    # Add Households per Building Count (HHpBLD)\n",
    "    print(\"Calculating Households per Building Count (HHpBLD)...\")\n",
    "    building_metrics[\"HHpBLD\"] = blocks_subset[\"HOUSING20\"] / building_metrics[\"bld_cnt\"]\n",
    "    building_metrics[\"HHpBLD\"] = building_metrics[\"HHpBLD\"].fillna(0)  # Fill missing values with 0 (e.g., no buildings)\n",
    "\n",
    "    # Merge metrics into block dataset\n",
    "    print(\"Merging building metrics into block dataset...\")\n",
    "    blocks_subset = blocks_subset.merge(building_metrics, on=\"BLOCK_ID\", how=\"left\")\n",
    "    \n",
    "    # Fill missing values explicitly for columns\n",
    "    print(\"Filling missing values for block metrics...\")\n",
    "    blocks_subset[\"avg_hght\"] = blocks_subset[\"avg_hght\"].fillna(0)  \n",
    "    blocks_subset[\"avg_sback\"] = blocks_subset[\"avg_sback\"].fillna(0)  \n",
    "    blocks_subset[\"bld_area\"] = blocks_subset[\"bld_area\"].fillna(0)  \n",
    "    blocks_subset[\"bld_ctsm\"] = blocks_subset[\"bld_ctsm\"].fillna(0)  \n",
    "    blocks_subset[\"bld_prc\"] = blocks_subset[\"bld_prc\"].fillna(0)  \n",
    "    blocks_subset[\"bld_cnt\"] = blocks_subset[\"bld_cnt\"].fillna(0)  \n",
    "    blocks_subset[\"HHpBLD\"] = blocks_subset[\"HHpBLD\"].fillna(0)  \n",
    "\n",
    "    print(\"Block metrics calculated successfully.\")\n",
    "    return blocks_subset\n",
    "\n",
    "# Calculate block-level metrics\n",
    "blocks_processed = calculate_block_metrics(blocks_subset, buildings_subset, roads_subset)\n",
    "print(\"Block metrics successfully calculated:\")\n",
    "print(blocks_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48683f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculating road metrics with buffer size (for spatial analysis): 50 feet...\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    002002      2008  483550020022008   \n",
      "2        48        355    001300      4003  483550013004003   \n",
      "3        48        355    001300      4004  483550013004004   \n",
      "4        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... BLOCK_ID  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...   211079   \n",
      "1  1000000US483550020022008  Block2008   G5040    U  20287  ...   211080   \n",
      "2  1000000US483550013004003  Block4003   G5040    U  20287  ...   211081   \n",
      "3  1000000US483550013004004  Block4004   G5040    U  20287  ...   211082   \n",
      "4  1000000US483550013004012  Block4012   G5040    U  20287  ...   211083   \n",
      "\n",
      "    area_sm      pop_den   avg_hght  avg_sback  bld_area  bld_cnt  \\\n",
      "0  0.013887  8929.477778  33.216162  54.357708  0.002815     49.0   \n",
      "1  0.013877  7998.964124  32.435641  44.567143  0.002327     47.0   \n",
      "2  0.009445  8152.607692  32.740521  55.002728  0.001837     29.0   \n",
      "3  0.006603     0.000000  32.414699  33.839178  0.002782      1.0   \n",
      "4  0.009849  8935.348238  32.753367  61.108800  0.002250     31.0   \n",
      "\n",
      "      bld_ctsm    bld_prc    HHpBLD  \n",
      "0  3528.583961  20.269451  1.000000  \n",
      "1  3386.948773  16.770413  0.978723  \n",
      "2  3070.462637  19.445682  1.103448  \n",
      "3   151.448236  42.140090  0.000000  \n",
      "4  3147.679493  22.849059  1.354839  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "- Creating road buffers...\n",
      "-- Road buffers structure after buffering:\n",
      "Index(['osmid', 'highway', 'lanes', 'name', 'oneway', 'ref', 'reversed',\n",
      "       'length', 'maxspeed', 'geometry', 'bridge', 'junction', 'width',\n",
      "       'access', 'road_id', 'buffer_area'],\n",
      "      dtype='object')\n",
      "- Performing spatial join of blocks with road buffers...\n",
      "-- Intersections structure after spatial join:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    001904      1018  483550019041018   \n",
      "2        48        355    001904      2000  483550019042000   \n",
      "3        48        355    001904      2005  483550019042005   \n",
      "4        48        355    001904      2001  483550019042001   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... reversed  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...    False   \n",
      "1  1000000US483550019041018  Block1018   G5040    U  20287  ...    False   \n",
      "2  1000000US483550019042000  Block2000   G5040    U  20287  ...    False   \n",
      "3  1000000US483550019042005  Block2005   G5040    U  20287  ...    False   \n",
      "4  1000000US483550019042001  Block2001   G5040    U  20287  ...    False   \n",
      "\n",
      "      length  maxspeed bridge junction  width  access  road_id  buffer_area  \\\n",
      "0  94.165957       NaN    NaN      NaN    NaN     NaN     5300  3592.182272   \n",
      "1  94.165957       NaN    NaN      NaN    NaN     NaN     5300  3592.182272   \n",
      "2  94.165957       NaN    NaN      NaN    NaN     NaN     5300  3592.182272   \n",
      "3  94.165957       NaN    NaN      NaN    NaN     NaN     5300  3592.182272   \n",
      "4  51.078478       NaN    NaN      NaN    NaN     NaN     6399  2283.689421   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((656388.176 3069806.492, 656388.475 3...  \n",
      "1  POLYGON ((656331.076 3069876.455, 656322.38 30...  \n",
      "2  POLYGON ((656338.743 3069888.691, 656339.119 3...  \n",
      "3  POLYGON ((656322.38 3069885.651, 656331.076 30...  \n",
      "4  POLYGON ((656577.893 3070140.495, 656585.777 3...  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "-- Adding road buffer areas for overlap proportion calculation...\n",
      "-- Intersection overlap proportions sample:\n",
      "   BLOCK_ID  road_id  overlap_area  road_buffer_area  overlap_proportion\n",
      "0    211079     5300    193.005975       3592.182272            0.053729\n",
      "1    475983     5300    142.839906       3592.182272            0.039764\n",
      "2    591650     5300   1446.078383       3592.182272            0.402563\n",
      "3    633619     5300   1810.258008       3592.182272            0.503944\n",
      "4    211079     6399   1069.411001       2283.689421            0.468282\n",
      "- Creating overlap dictionary for roads...\n",
      "-- Adding block IDs and overlap percentages columns...\n",
      "- Loading evacuation routes shapefile...\n",
      "-- Flagging roads based on evacuation route overlap...\n",
      "- Weighting metrics for roads...\n",
      "- Aggregating weighted metrics across roads...\n",
      "-- Aggregated road metrics structure:\n",
      "       agg_pop  agg_area     agg_ctsm  agg_bldprc   agg_hght  agg_sback  \\\n",
      "0   179.386868  0.000747   615.723759   10.501223  41.096999  41.270940   \n",
      "1   212.881493  0.000385   337.863022    6.376996  26.131773  33.134895   \n",
      "2   693.493616  0.000575   817.996603   14.102548  40.959569  43.455655   \n",
      "3    58.904260  0.001291   611.881951   15.083942  45.226433  53.047322   \n",
      "4  5543.597413  0.002104  2756.273755   23.302067  34.273597  66.820678   \n",
      "\n",
      "   agg_HHpBLD  \n",
      "0    0.275447  \n",
      "1    0.097216  \n",
      "2    0.353608  \n",
      "3    0.108892  \n",
      "4    0.639933  \n",
      "- Merging metrics into roads dataset...\n",
      "-- Adding calculated road segment lengths as length_cal...\n",
      "-- Final roads structure:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ...                                      overlap_percs evac_flag  \\\n",
      "0  ...  [0.410884402784681, 0.04926583611313816, 0.047...         0   \n",
      "1  ...  [0.021641946929578066, 0.16740453513625067, 0....         0   \n",
      "2  ...  [0.03351930391955583, 0.04936305717900521, 0.0...         0   \n",
      "3  ...  [0.4247128338513536, 0.48637656963460946, 0.03...         0   \n",
      "4  ...  [0.42981149729602536, 0.053945707680300964, 0....         0   \n",
      "\n",
      "       agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0   179.386868  0.000747   615.723759  10.501223  41.096999  41.270940   \n",
      "1   212.881493  0.000385   337.863022   6.376996  26.131773  33.134895   \n",
      "2   693.493616  0.000575   817.996603  14.102548  40.959569  43.455655   \n",
      "3    58.904260  0.001291   611.881951  15.083942  45.226433  53.047322   \n",
      "4  5543.597413  0.002104  2756.273755  23.302067  34.273597  66.820678   \n",
      "\n",
      "   agg_HHpBLD  length_cal  \n",
      "0    0.275447  111.245279  \n",
      "1    0.097216  239.451047  \n",
      "2    0.353608   91.561893  \n",
      "3    0.108892  181.155878  \n",
      "4    0.639933   98.008247  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "- Loading the shapefile with polygons for 'UrbanArea' flagging...\n",
      "- Adding 'UrbanArea' flag to roads based on intersection or touching polygons...\n",
      "-- Reprojecting urban area polygons to match roads CRS...\n",
      "-- Added 'UrbanArea' column to roads GeoDataFrame:\n",
      "-- Final roads with 'UrbanArea' flag sample:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ... evac_flag      agg_pop  agg_area     agg_ctsm  agg_bldprc   agg_hght  \\\n",
      "0  ...         0   179.386868  0.000747   615.723759   10.501223  41.096999   \n",
      "1  ...         0   212.881493  0.000385   337.863022    6.376996  26.131773   \n",
      "2  ...         0   693.493616  0.000575   817.996603   14.102548  40.959569   \n",
      "3  ...         0    58.904260  0.001291   611.881951   15.083942  45.226433   \n",
      "4  ...         0  5543.597413  0.002104  2756.273755   23.302067  34.273597   \n",
      "\n",
      "   agg_sback  agg_HHpBLD  length_cal  UrbanArea  \n",
      "0  41.270940    0.275447  111.245279          1  \n",
      "1  33.134895    0.097216  239.451047          1  \n",
      "2  43.455655    0.353608   91.561893          1  \n",
      "3  53.047322    0.108892  181.155878          1  \n",
      "4  66.820678    0.639933   98.008247          1  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "- Ensuring CRS consistency and clipping polygons...\n",
      "-- CRS mismatch between polygons and city limits.\n",
      "--- Converting both city limits and polygons CRS to match roads CRS...\n",
      "-- CRS match between polygons and city limits. Clipping polygons to city limits...\n",
      "-- Sample of clipped polygons:\n",
      "           D1C    Shape__Are     Shape__Len  \\\n",
      "9186  0.001412  1.740383e+09  702468.025945   \n",
      "8826  0.035271  1.493616e+08   60699.525117   \n",
      "9187  0.000000  3.860989e+04   80104.012519   \n",
      "8890  0.163972  1.769925e+06    9368.110551   \n",
      "8908  0.068378  1.677014e+06    5987.189241   \n",
      "\n",
      "                                    GlobalID  \\\n",
      "9186  {0a7bd5fb-24b3-4230-bf36-c08cd27e2735}   \n",
      "8826  {6f535ab8-7e82-4204-b21c-14621919ede8}   \n",
      "9187  {d458cafe-3ddf-46b0-a80e-11e7d8fa4b15}   \n",
      "8890  {7f90fd59-28bc-470f-afd8-534cbb41bf3f}   \n",
      "8908  {e366b08b-6529-4a18-9e11-3ff58403e32f}   \n",
      "\n",
      "                                               geometry  \n",
      "9186  MULTIPOLYGON (((669560.837 3048792.708, 669596...  \n",
      "8826  POLYGON ((649059.12 3068886.087, 649181.187 30...  \n",
      "9187  POLYGON ((675438.745 3051553.313, 675426.238 3...  \n",
      "8890  POLYGON ((668192.606 3055870.206, 668179.696 3...  \n",
      "8908  POLYGON ((667863.538 3057007.098, 667945.252 3...  \n",
      "- Converting D1C from jobs/acre to jobs/square mile...\n",
      "-- Updated density column with jobs/square mile (after clipping):\n",
      "         emp_den\n",
      "9186    0.903962\n",
      "8826   22.573742\n",
      "9187    0.000000\n",
      "8890  104.942059\n",
      "8908   43.761863\n",
      "- Performing spatial join to associate roads with employment density polygons...\n",
      "-- Results of spatial join sample:\n",
      "        D1C      emp_den\n",
      "0  4.611677  2951.473078\n",
      "1  4.611677  2951.473078\n",
      "2  4.611677  2951.473078\n",
      "3  4.611677  2951.473078\n",
      "4  3.902442  2497.562933\n",
      "- Aggregating road intersections to take the highest employment density for each road...\n",
      "-- Aggregated density by road (max jobs/sq mile):\n",
      "   road_id      emp_den\n",
      "0        0  2951.473078\n",
      "1        1  2951.473078\n",
      "2        2  2951.473078\n",
      "3        3  2951.473078\n",
      "4        4  2497.562933\n",
      "- Merging maximum density values back into the roads GeoDataFrame...\n",
      "-- Final roads sample with employment density:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ...      agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0  ...   179.386868  0.000747   615.723759  10.501223  41.096999  41.270940   \n",
      "1  ...   212.881493  0.000385   337.863022   6.376996  26.131773  33.134895   \n",
      "2  ...   693.493616  0.000575   817.996603  14.102548  40.959569  43.455655   \n",
      "3  ...    58.904260  0.001291   611.881951  15.083942  45.226433  53.047322   \n",
      "4  ...  5543.597413  0.002104  2756.273755  23.302067  34.273597  66.820678   \n",
      "\n",
      "  agg_HHpBLD  length_cal  UrbanArea      emp_den  \n",
      "0   0.275447  111.245279          1  2951.473078  \n",
      "1   0.097216  239.451047          1  2951.473078  \n",
      "2   0.353608   91.561893          1  2951.473078  \n",
      "3   0.108892  181.155878          1  2951.473078  \n",
      "4   0.639933   98.008247          1  2497.562933  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "-- Final roads GeoDataFrame with employment density sample:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ...      agg_pop  agg_area     agg_ctsm agg_bldprc   agg_hght  agg_sback  \\\n",
      "0  ...   179.386868  0.000747   615.723759  10.501223  41.096999  41.270940   \n",
      "1  ...   212.881493  0.000385   337.863022   6.376996  26.131773  33.134895   \n",
      "2  ...   693.493616  0.000575   817.996603  14.102548  40.959569  43.455655   \n",
      "3  ...    58.904260  0.001291   611.881951  15.083942  45.226433  53.047322   \n",
      "4  ...  5543.597413  0.002104  2756.273755  23.302067  34.273597  66.820678   \n",
      "\n",
      "  agg_HHpBLD  length_cal  UrbanArea      emp_den  \n",
      "0   0.275447  111.245279          1  2951.473078  \n",
      "1   0.097216  239.451047          1  2951.473078  \n",
      "2   0.353608   91.561893          1  2951.473078  \n",
      "3   0.108892  181.155878          1  2951.473078  \n",
      "4   0.639933   98.008247          1  2497.562933  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_road_metrics(blocks_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame, buffer_size_feet=50) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate road-level metrics based on spatial overlap with blocks (weighted aggregation).\n",
    "    Includes Block IDs and Overlap Percentages as columns, flags roads intersecting evacuation routes using buffered geometry,\n",
    "    and calculates new columns: agg_HHpBLD (weighted households per building count) and length_cal (road segment length).\n",
    "    \"\"\"\n",
    "    print(f\"- Calculating road metrics with buffer size (for spatial analysis): {buffer_size_feet} feet...\")\n",
    "    buffer_size_meters = buffer_size_feet * 0.3048  # Convert buffer size to meters\n",
    "\n",
    "    # Blocks subset should maintain the original BLOCK_ID\n",
    "    blocks_subset = blocks_subset.copy()\n",
    "    print(blocks_subset.head())  # Output a sample of blocks_subset\n",
    "\n",
    "    roads_subset = roads_subset.copy().reset_index(drop=True)\n",
    "    roads_subset[\"road_id\"] = roads_subset.index\n",
    "\n",
    "    # Check if required block-level metrics exist\n",
    "    required_columns = [\"area_sm\", \"pop_den\", \"bld_area\", \"bld_ctsm\", \"avg_hght\", \"avg_sback\", \"HHpBLD\"]\n",
    "    missing_columns = [col for col in required_columns if col not in blocks_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: The following required block-level metrics are missing: {missing_columns}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Create buffers for roads\n",
    "    print(\"- Creating road buffers...\")\n",
    "    road_buffers = roads_subset.copy()\n",
    "    road_buffers[\"geometry\"] = roads_subset.geometry.buffer(buffer_size_meters)\n",
    "    road_buffers[\"road_id\"] = roads_subset[\"road_id\"]\n",
    "    road_buffers[\"buffer_area\"] = road_buffers.geometry.area  # Area of the road buffer, for overlap proportion calculation\n",
    "    print(\"-- Road buffers structure after buffering:\")\n",
    "    print(road_buffers.columns)\n",
    "\n",
    "    # Perform spatial join of blocks with road buffers\n",
    "    print(\"- Performing spatial join of blocks with road buffers...\")\n",
    "    intersections = gpd.overlay(blocks_subset, road_buffers, how=\"intersection\")\n",
    "    print(\"-- Intersections structure after spatial join:\")\n",
    "    print(intersections.head())\n",
    "\n",
    "    # Validate road buffer areas and block intersections\n",
    "    print(\"-- Adding road buffer areas for overlap proportion calculation...\")\n",
    "    intersections[\"road_buffer_area\"] = intersections[\"road_id\"].map(road_buffers.set_index(\"road_id\")[\"buffer_area\"])\n",
    "    intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "    intersections[\"overlap_proportion\"] = intersections[\"overlap_area\"] / intersections[\"road_buffer_area\"]\n",
    "    print(\"-- Intersection overlap proportions sample:\")\n",
    "    print(intersections[[\"BLOCK_ID\", \"road_id\", \"overlap_area\", \"road_buffer_area\", \"overlap_proportion\"]].head())\n",
    "\n",
    "    # Create overlap dictionary for roads\n",
    "    print(\"- Creating overlap dictionary for roads...\")\n",
    "    road_block_overlap = (\n",
    "        intersections.groupby(\"road_id\")\n",
    "        .apply(lambda rows: dict(zip(rows[\"BLOCK_ID\"], rows[\"overlap_proportion\"])))\n",
    "        .to_dict()\n",
    "    )\n",
    "    print(\"-- Adding block IDs and overlap percentages columns...\")\n",
    "    # Add block IDs and overlap percentages columns\n",
    "    roads_subset[\"block_ids\"] = [\n",
    "        list(overlap_dict.keys()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "    roads_subset[\"overlap_percs\"] = [\n",
    "        list(overlap_dict.values()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "\n",
    "    # Load Evacuation Routes\n",
    "    print(\"- Loading evacuation routes shapefile...\")\n",
    "    evac_path = os.path.join(one_drive_path, \"Data\", \"TxDOT Evacuation Routes AGO.shp\")\n",
    "    evacuation_routes = gpd.read_file(evac_path)\n",
    "\n",
    "    # Ensure both datasets use the same CRS for spatial operations\n",
    "    if evacuation_routes.crs != road_buffers.crs:\n",
    "        evacuation_routes = evacuation_routes.to_crs(road_buffers.crs)\n",
    "\n",
    "    print(\"-- Flagging roads based on evacuation route overlap...\")\n",
    "\n",
    "    def flag_evacuation_routes(road_buffer_geom):\n",
    "        \"\"\"\n",
    "        Flag roads based on overlap with evacuation routes.\n",
    "        0 = No overlap\n",
    "        1 = Major Evacuation Routes\n",
    "        2 = Potential Contraflow\n",
    "        3 = Potential EvacuLanes\n",
    "        \"\"\"\n",
    "        for _, evac_row in evacuation_routes.iterrows():\n",
    "            if road_buffer_geom.intersects(evac_row.geometry):\n",
    "                route_type = evac_row[\"ROUTE_TYPE\"]\n",
    "                if route_type == \"Major Evacuation Routes\":\n",
    "                    return 1\n",
    "                elif route_type == \"Potential Contraflow\":\n",
    "                    return 2\n",
    "                elif route_type == \"Potential EvacuLanes\":\n",
    "                    return 3\n",
    "        return 0  # No overlap\n",
    "\n",
    "    # Use the buffered geometry to calculate evacuation flags\n",
    "    roads_subset[\"evac_flag\"] = road_buffers.geometry.apply(flag_evacuation_routes)\n",
    "\n",
    "    # Calculate weighted metrics for roads\n",
    "    print(\"- Weighting metrics for roads...\")\n",
    "    def compute_weighted_metrics(road_id, overlap_dict):\n",
    "        \"\"\"\n",
    "        Compute weighted metrics for a road using normalized weights for metrics like `agg_hght` and `agg_sback`.\n",
    "        Includes HHpBLD as a new weighted metric.\n",
    "        \"\"\"\n",
    "        blocks = blocks_subset.set_index(\"BLOCK_ID\").loc[overlap_dict.keys()]\n",
    "        raw_weights = pd.Series({BLOCK_ID: overlap_dict[BLOCK_ID] for BLOCK_ID in blocks.index})\n",
    "        total_weight = raw_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = raw_weights / total_weight\n",
    "        else:\n",
    "            return pd.Series({\n",
    "                \"agg_pop\": 0,\n",
    "                \"agg_area\": 0,\n",
    "                \"agg_ctsm\": 0,\n",
    "                \"agg_bldprc\": 0,\n",
    "                \"agg_hght\": 0,\n",
    "                \"agg_sback\": 0,\n",
    "                \"agg_HHpBLD\": 0,\n",
    "            })\n",
    "        valid_blocks_for_hght_sback = blocks[(blocks[\"avg_hght\"] > 0) & (blocks[\"avg_sback\"] > 0)]\n",
    "        aggregated_metrics = {\n",
    "            \"agg_pop\": (blocks[\"pop_den\"] * normalized_weights).sum(),\n",
    "            \"agg_area\": (blocks[\"bld_area\"] * normalized_weights).sum(),\n",
    "            \"agg_ctsm\": (blocks[\"bld_ctsm\"] * normalized_weights).sum(),\n",
    "            \"agg_bldprc\": (blocks[\"bld_prc\"] * normalized_weights).sum(),\n",
    "            \"agg_hght\": (valid_blocks_for_hght_sback[\"avg_hght\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "            \"agg_sback\": (valid_blocks_for_hght_sback[\"avg_sback\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "            \"agg_HHpBLD\": (blocks[\"HHpBLD\"] * normalized_weights).sum(),\n",
    "        }\n",
    "        return pd.Series(aggregated_metrics)\n",
    "\n",
    "    print(\"- Aggregating weighted metrics across roads...\")\n",
    "    road_metrics_df = pd.DataFrame([\n",
    "        compute_weighted_metrics(road_id, overlap_dict)\n",
    "        for road_id, overlap_dict in road_block_overlap.items()\n",
    "    ], index=road_block_overlap.keys())\n",
    "    print(\"-- Aggregated road metrics structure:\")\n",
    "    print(road_metrics_df.head())\n",
    "\n",
    "    # Merge aggregated metrics into roads dataset\n",
    "    print(\"- Merging metrics into roads dataset...\")\n",
    "    roads_subset = roads_subset.merge(road_metrics_df, left_on=\"road_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    # Calculate road length and add as a new column\n",
    "    print(\"-- Adding calculated road segment lengths as length_cal...\")\n",
    "    roads_subset[\"length_cal\"] = roads_subset.geometry.length  # Calculate length based on geometry\n",
    "\n",
    "    print(\"-- Final roads structure:\")\n",
    "    print(roads_subset.head())\n",
    "    return roads_subset\n",
    "\n",
    "def add_urban_area_flag(roads: gpd.GeoDataFrame, polygons: gpd.GeoDataFrame, city_limits: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Adds a flag ('UrbanArea') to the roads GeoDataFrame indicating whether\n",
    "    a road segment is inside or touches any polygon in the polygons GeoDataFrame.\n",
    "    \"\"\"\n",
    "    print(\"- Adding 'UrbanArea' flag to roads based on intersection or touching polygons...\")\n",
    "\n",
    "    # Ensure CRS is consistent for spatial operations\n",
    "    if roads.crs != polygons.crs:\n",
    "        print(\"-- Reprojecting urban area polygons to match roads CRS...\")\n",
    "        \n",
    "        polygons = polygons.to_crs(roads.crs)\n",
    "\n",
    "    def check_intersects_or_touches(road_geom):\n",
    "        \"\"\"\n",
    "        Check if a road segment intersects or touches any polygon.\n",
    "        Returns 1 if true (overlap/match), else 0 (no overlap/match).\n",
    "        \"\"\"\n",
    "        for poly_geom in polygons.geometry:\n",
    "            if road_geom.intersects(poly_geom) or road_geom.touches(poly_geom):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    # Apply the function to calculate 'UrbanArea' flag\n",
    "    roads[\"UrbanArea\"] = roads.geometry.apply(check_intersects_or_touches)\n",
    "    print(\"-- Added 'UrbanArea' column to roads GeoDataFrame:\")\n",
    "    \n",
    "    return roads\n",
    "\n",
    "def associate_roads_with_employment_density(roads: gpd.GeoDataFrame, polygons: gpd.GeoDataFrame, density_column: str, city_limits: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Associates roads with employment density polygons based on spatial intersection.\n",
    "    Trims employment density polygons to be within the city limits (provided city_limits GeoDataFrame).\n",
    "    Converts jobs/acre to jobs/square mile.\n",
    "    If a road intersects multiple polygons, assigns the value of the polygon with the higher employment density.\n",
    "    \"\"\"\n",
    "    print(\"- Ensuring CRS consistency and clipping polygons...\")\n",
    "\n",
    "    # Check if the CRS of polygons and city limits match\n",
    "    if polygons.crs != city_limits.crs:\n",
    "        print(\"-- CRS mismatch between polygons and city limits.\")\n",
    "        if roads.crs == polygons.crs:\n",
    "            print(\"--- Converting city limits CRS to match polygons CRS...\")\n",
    "            city_limits = city_limits.to_crs(polygons.crs)\n",
    "        else:\n",
    "            print(\"--- Converting both city limits and polygons CRS to match roads CRS...\")\n",
    "            city_limits = city_limits.to_crs(roads.crs)\n",
    "            polygons = polygons.to_crs(roads.crs)\n",
    "\n",
    "    # If CRS matches between polygons and city limits, trim polygons first, then optionally convert\n",
    "    if polygons.crs == city_limits.crs:\n",
    "        print(\"-- CRS match between polygons and city limits. Clipping polygons to city limits...\")\n",
    "        polygons_clipped = gpd.clip(polygons, city_limits)\n",
    "        if roads.crs != polygons.crs:\n",
    "            print(\"--- Converting clipped polygons CRS to match roads CRS...\")\n",
    "            polygons_clipped = polygons_clipped.to_crs(roads.crs)\n",
    "    else:\n",
    "        polygons_clipped = polygons  # Already transformed earlier, no need to clip again\n",
    "\n",
    "    print(\"-- Sample of clipped polygons:\")\n",
    "    print(polygons_clipped.head())\n",
    "\n",
    "    # Convert employment density to jobs per square mile\n",
    "    print(f\"- Converting {density_column} from jobs/acre to jobs/square mile...\")\n",
    "    polygons_clipped[\"emp_den\"] = polygons_clipped[density_column] * 640  # 1 square mile = 640 acres\n",
    "    print(\"-- Updated density column with jobs/square mile (after clipping):\")\n",
    "    print(polygons_clipped[[\"emp_den\"]].head())\n",
    "\n",
    "    # Perform spatial join to associate roads with clipped polygons\n",
    "    print(\"- Performing spatial join to associate roads with employment density polygons...\")\n",
    "    roads_with_density = gpd.sjoin(roads, polygons_clipped, how=\"left\", predicate=\"intersects\")\n",
    "    print(\"-- Results of spatial join sample:\")\n",
    "    print(roads_with_density[[density_column, \"emp_den\"]].head())\n",
    "\n",
    "    # Identify the road-polygon intersections and select the highest density value for each road\n",
    "    print(\"- Aggregating road intersections to take the highest employment density for each road...\")\n",
    "    roads_with_density_agg = (\n",
    "        roads_with_density.groupby(\"road_id\")  # Assuming \"road_id\" is unique for roads\n",
    "        .agg({\"emp_den\": \"max\"})  # Take the maximum density value\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"-- Aggregated density by road (max jobs/sq mile):\")\n",
    "    print(roads_with_density_agg.head())\n",
    "\n",
    "    # Merge the maximum density back into the roads GeoDataFrame\n",
    "    print(\"- Merging maximum density values back into the roads GeoDataFrame...\")\n",
    "    roads = roads.merge(roads_with_density_agg, on=\"road_id\", how=\"left\")\n",
    "    print(\"-- Final roads sample with employment density:\")\n",
    "    print(roads.head())\n",
    "\n",
    "    return roads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Execute road metrics calculation\n",
    "roads_processed = calculate_road_metrics(blocks_processed, roads_subset, buffer_size_feet=50)\n",
    "\n",
    "print(\"- Loading the shapefile with polygons for 'UrbanArea' flagging...\")\n",
    "urbanarea_file = os.path.join(one_drive_path, \"Data\", \"2020_Census_Urban_Areas\", \"2020_Census_Urban_Areas.shp\")\n",
    "urban_area_polygons = gpd.read_file(urbanarea_file)\n",
    "# Path to the Corpus Christi city limits shapefile\n",
    "cc_limits_file = os.path.join(one_drive_path, \"Data\", \"CC-citylimits\", \"Citylimits.shp\")\n",
    "city_limits = gpd.read_file(cc_limits_file)\n",
    "\n",
    "# Read the employment density shapefile and inspect column names\n",
    "emp_density_file = os.path.join(one_drive_path, \"Data\", \"CBG2010_SLD_YY\", \"CBG2010_SLD_YY.shp\")\n",
    "employment_density_polygons = gpd.read_file(emp_density_file)\n",
    "\n",
    "roads_processed = add_urban_area_flag(roads_processed, urban_area_polygons, city_limits)\n",
    "\n",
    "print(\"-- Final roads with 'UrbanArea' flag sample:\")\n",
    "print(roads_processed.head())\n",
    "\n",
    "\n",
    "\n",
    "# Specify the column name for employment density\n",
    "density_column = \"D1C\" \n",
    "\n",
    "# Process the roads to associate employment density (clipped to Corpus Christi city limits)\n",
    "roads_processed = associate_roads_with_employment_density(\n",
    "    roads=roads_processed,\n",
    "    polygons=employment_density_polygons,\n",
    "    density_column=density_column,\n",
    "    city_limits=city_limits\n",
    ")\n",
    "\n",
    "# Final debugging output\n",
    "print(\"-- Final roads GeoDataFrame with employment density sample:\")\n",
    "print(roads_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving subsets to shapefiles...\n",
      "- Saving buildings subset...\n",
      "-- Full column names in buildings_subset:\n",
      "  geometry\n",
      "  addr:state\n",
      "  building\n",
      "  ele\n",
      "  gnis:feature_id\n",
      "  name\n",
      "  source\n",
      "  addr:city\n",
      "  addr:housename\n",
      "  addr:housenumber\n",
      "  addr:postcode\n",
      "  addr:street\n",
      "  amenity\n",
      "  brand\n",
      "  brand:wikidata\n",
      "  cuisine\n",
      "  healthcare\n",
      "  healthcare:counselling\n",
      "  height\n",
      "  phone\n",
      "  website\n",
      "  generator:method\n",
      "  generator:output:electricity\n",
      "  generator:source\n",
      "  generator:type\n",
      "  layer\n",
      "  power\n",
      "  shop\n",
      "  opening_hours\n",
      "  ref\n",
      "  wholesale\n",
      "  addr:unit\n",
      "  official_name\n",
      "  takeaway\n",
      "  note\n",
      "  email\n",
      "  museum\n",
      "  tourism\n",
      "  addr:country\n",
      "  alt_name\n",
      "  contact:email\n",
      "  contact:facebook\n",
      "  contact:twitter\n",
      "  man_made\n",
      "  content\n",
      "  office\n",
      "  start_date\n",
      "  building:levels\n",
      "  leisure\n",
      "  sport\n",
      "  wikidata\n",
      "  check_date\n",
      "  fee\n",
      "  fixme\n",
      "  heritage\n",
      "  heritage:operator\n",
      "  historic\n",
      "  ref:nrhp\n",
      "  ship:type\n",
      "  wikipedia\n",
      "  building:part\n",
      "  disused:shop\n",
      "  type\n",
      "  aeroway\n",
      "  operator\n",
      "  short_name\n",
      "  parking\n",
      "  old_name\n",
      "  branch\n",
      "  air_conditioning\n",
      "  drive_through\n",
      "  opening_hours:drive_through\n",
      "  indoor_seating\n",
      "  outdoor_seating\n",
      "  delivery\n",
      "  payment:NFC_mobile_payments\n",
      "  smoking\n",
      "  access\n",
      "  brand:website\n",
      "  brand:wikipedia\n",
      "  denomination\n",
      "  religion\n",
      "  payment:cash\n",
      "  payment:contactless\n",
      "  payment:credit_cards\n",
      "  payment:debit_cards\n",
      "  payment:nfc\n",
      "  website:menu\n",
      "  wheelchair\n",
      "  operator:wikidata\n",
      "  drive_in\n",
      "  compressed_air\n",
      "  toilets\n",
      "  operator:short\n",
      "  operator:type\n",
      "  operator:website\n",
      "  operator:wikipedia\n",
      "  fuel:diesel\n",
      "  fuel:gasoline\n",
      "  self_service\n",
      "  bar\n",
      "  fax\n",
      "  internet_access\n",
      "  internet_access:fee\n",
      "  reservation\n",
      "  rooms\n",
      "  landuse\n",
      "  ref:walmart\n",
      "  atm\n",
      "  shelter_type\n",
      "  roof:shape\n",
      "  roof:levels\n",
      "  diet:chicken\n",
      "  diet:dairy\n",
      "  diet:meat\n",
      "  image\n",
      "  contact:instagram\n",
      "  description\n",
      "  second_hand\n",
      "  screen\n",
      "  bin\n",
      "  toilets:disposal\n",
      "  unisex\n",
      "  social_facility\n",
      "  social_facility:for\n",
      "  beauty\n",
      "  healthcare:speciality\n",
      "  clothes\n",
      "  dispensing\n",
      "  was:shop\n",
      "  check_date:opening_hours\n",
      "  service:vehicle:inspection\n",
      "  service:vehicle:oil_change\n",
      "  contact:foursquare\n",
      "  drinking_water\n",
      "  opening_hours:covid19\n",
      "  highchair\n",
      "  abandoned\n",
      "  automated\n",
      "  payment:coins\n",
      "  telecom\n",
      "  craft\n",
      "  after_school\n",
      "  isced:level\n",
      "  nursery\n",
      "  preschool\n",
      "  abandoned:building\n",
      "  microbrewery\n",
      "  contact:website\n",
      "  not:brand:wikidata\n",
      "  teaching\n",
      "  swimming_pool\n",
      "  bus\n",
      "  public_transport\n",
      "  contact:phone\n",
      "  service:vehicle:car_repair\n",
      "  building:min_level\n",
      "  max_level\n",
      "  min_level\n",
      "  theatre\n",
      "  dance:style\n",
      "  dance:teaching\n",
      "  disused\n",
      "  architect\n",
      "  building:use\n",
      "  diocese\n",
      "  building_1\n",
      "  urgent_care\n",
      "  emergency\n",
      "  lit\n",
      "  bridge\n",
      "  level\n",
      "  community_centre:for\n",
      "  construction_equipment:rental\n",
      "  tool:rental\n",
      "  condo\n",
      "  service:vehicle:car_parts\n",
      "  service:vehicle:new_car_sales\n",
      "  service:vehicle:used_car_sales\n",
      "  bench\n",
      "  government\n",
      "  payment:american_express\n",
      "  payment:mastercard\n",
      "  payment:visa\n",
      "  animal_shelter\n",
      "  animal_shelter:adoption\n",
      "  animal_shelter:release\n",
      "  pets\n",
      "  check_date:opening_hours:drive_through\n",
      "  fuel:octane_87\n",
      "  fuel:octane_89\n",
      "  fuel:octane_93\n",
      "  female\n",
      "  male\n",
      "  portable\n",
      "  toilets:handwashing\n",
      "  capacity\n",
      "  size\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Dropping extraneous columns from buildings_subset: ['addr:state', 'building', 'ele', 'gnis:feature_id', 'name', 'source', 'addr:city', 'addr:housename', 'addr:housenumber', 'addr:postcode', 'addr:street', 'amenity', 'brand', 'brand:wikidata', 'cuisine', 'healthcare', 'healthcare:counselling', 'phone', 'website', 'generator:method', 'generator:output:electricity', 'generator:source', 'generator:type', 'layer', 'power', 'shop', 'opening_hours', 'ref', 'wholesale', 'addr:unit', 'official_name', 'takeaway', 'note', 'email', 'museum', 'tourism', 'addr:country', 'alt_name', 'contact:email', 'contact:facebook', 'contact:twitter', 'man_made', 'content', 'office', 'start_date', 'building:levels', 'leisure', 'sport', 'wikidata', 'check_date', 'fee', 'fixme', 'heritage', 'heritage:operator', 'historic', 'ref:nrhp', 'ship:type', 'wikipedia', 'building:part', 'disused:shop', 'type', 'aeroway', 'operator', 'short_name', 'parking', 'old_name', 'branch', 'air_conditioning', 'drive_through', 'opening_hours:drive_through', 'indoor_seating', 'outdoor_seating', 'delivery', 'payment:NFC_mobile_payments', 'smoking', 'access', 'brand:website', 'brand:wikipedia', 'denomination', 'religion', 'payment:cash', 'payment:contactless', 'payment:credit_cards', 'payment:debit_cards', 'payment:nfc', 'website:menu', 'wheelchair', 'operator:wikidata', 'drive_in', 'compressed_air', 'toilets', 'operator:short', 'operator:type', 'operator:website', 'operator:wikipedia', 'fuel:diesel', 'fuel:gasoline', 'self_service', 'bar', 'fax', 'internet_access', 'internet_access:fee', 'reservation', 'rooms', 'landuse', 'ref:walmart', 'atm', 'shelter_type', 'roof:shape', 'roof:levels', 'diet:chicken', 'diet:dairy', 'diet:meat', 'image', 'contact:instagram', 'description', 'second_hand', 'screen', 'bin', 'toilets:disposal', 'unisex', 'social_facility', 'social_facility:for', 'beauty', 'healthcare:speciality', 'clothes', 'dispensing', 'was:shop', 'check_date:opening_hours', 'service:vehicle:inspection', 'service:vehicle:oil_change', 'contact:foursquare', 'drinking_water', 'opening_hours:covid19', 'highchair', 'abandoned', 'automated', 'payment:coins', 'telecom', 'craft', 'after_school', 'isced:level', 'nursery', 'preschool', 'abandoned:building', 'microbrewery', 'contact:website', 'not:brand:wikidata', 'teaching', 'swimming_pool', 'bus', 'public_transport', 'contact:phone', 'service:vehicle:car_repair', 'building:min_level', 'max_level', 'min_level', 'theatre', 'dance:style', 'dance:teaching', 'disused', 'architect', 'building:use', 'diocese', 'building_1', 'urgent_care', 'emergency', 'lit', 'bridge', 'level', 'community_centre:for', 'construction_equipment:rental', 'tool:rental', 'condo', 'service:vehicle:car_parts', 'service:vehicle:new_car_sales', 'service:vehicle:used_car_sales', 'bench', 'government', 'payment:american_express', 'payment:mastercard', 'payment:visa', 'animal_shelter', 'animal_shelter:adoption', 'animal_shelter:release', 'pets', 'check_date:opening_hours:drive_through', 'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male', 'portable', 'toilets:handwashing', 'capacity', 'size']\n",
      "-- Full column names in buildings_subset after dropping:\n",
      "  geometry\n",
      "  height\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Validating geometries in buildings_subset...\n",
      "ERROR: Invalid geometry type detected in buildings_subset. Expected Polygon or MultiPolygon.\n",
      "-- Invalid geometries: 7\n",
      "-- Attempting to convert invalid geometries to valid Polygon/MultiPolygon...\n",
      "-- After conversion: 41003 valid polygons remaining.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\1518612113.py:113: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saving blocks processed...\n",
      "-- Full column names in blocks_processed:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  GEOIDFQ20\n",
      "  NAME20\n",
      "  MTFCC20\n",
      "  UR20\n",
      "  UACE20\n",
      "  FUNCSTAT20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  INTPTLAT20\n",
      "  INTPTLON20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "  HHpBLD\n",
      "-- Dropping extraneous columns from blocks_processed: ['GEOIDFQ20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20', 'INTPTLAT20', 'INTPTLON20', 'HHpBLD']\n",
      "-- Full column names in blocks_processed after dropping:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "- Saving roads processed...\n",
      "-- Full column names in roads_processed:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  ref\n",
      "  reversed\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  bridge\n",
      "  junction\n",
      "  width\n",
      "  access\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "  agg_HHpBLD\n",
      "  length_cal\n",
      "  UrbanArea\n",
      "  emp_den\n",
      "-- Dropping extraneous columns from roads_processed: ['ref', 'reversed', 'bridge', 'junction', 'width', 'access']\n",
      "-- Full column names in roads_processed after dropping:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "  agg_HHpBLD\n",
      "  length_cal\n",
      "  UrbanArea\n",
      "  emp_den\n",
      "-- Trimming values in overlap_percs to 4 decimal places for roads_processed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_14040\\1518612113.py:136: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefiles saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[367222, 367223, 465912, 500640, 500657, 520508, 520635, 520737, 541568, 541569, 541570, 541671, 575979, 591242, 591543, 591545, 591547, 591580, 591608, 591611, 604709, 604835, 604843, 606728, 606753, 606934, 607041, 607042, 607043, 607050, 607052, 608896, 608897, 620980, 620995, 621003, 642476]' of field block_ids has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrames to shapefiles while ensuring the correct geometry column is activated.\n",
    "    Includes functionality to validate geometries only for buildings and trims `overlap_percs` in roads to 4 decimal places.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving subsets to shapefiles...\")\n",
    "\n",
    "    # Suppress warnings about truncated column names for ESRI Shapefiles\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Normalized/laundered field name.*\", category=RuntimeWarning)\n",
    "\n",
    "    def ensure_unique_column_names(df, name):\n",
    "        \"\"\"\n",
    "        Ensure column names in a GeoDataFrame are unique by appending trailing numbers to duplicates.\n",
    "        \"\"\"\n",
    "        duplicates = df.columns[df.columns.duplicated()].unique()\n",
    "        if len(duplicates) > 0:\n",
    "            print(f\"WARNING: Duplicate column names detected in {name}: {duplicates}\")\n",
    "            df = df.rename(columns=lambda x: x[:10])  # Truncate names to 10 characters for ESRI compatibility\n",
    "            # Resolve duplicates by appending a suffix\n",
    "            seen = set()\n",
    "            new_columns = []\n",
    "            for col in df.columns:\n",
    "                if col in seen:\n",
    "                    count = sum([existing.startswith(col) for existing in seen]) + 1\n",
    "                    new_col = f\"{col[:7]}_{count}\"  # Add numeric suffix to resolve duplicates\n",
    "                    print(f\"    Renaming column '{col}' to '{new_col}' to resolve duplication.\")\n",
    "                    new_columns.append(new_col)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "                seen.add(new_columns[-1])\n",
    "            df.columns = new_columns\n",
    "        return df\n",
    "\n",
    "    def print_columns(df, name):\n",
    "        \"\"\"\n",
    "        Print column names in their entirety for a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        print(f\"-- Full column names in {name}:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}\")\n",
    "\n",
    "    def drop_extraneous_columns(df, columns_to_keep, name):\n",
    "        \"\"\"\n",
    "        Drop columns not included in the `columns_to_keep` list, only if they exist in the dataset.\n",
    "        \"\"\"\n",
    "        columns_to_drop = [col for col in df.columns if col not in columns_to_keep]\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Ensure column exists\n",
    "        if columns_to_drop:\n",
    "            print(f\"-- Dropping extraneous columns from {name}: {columns_to_drop}\")\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "        print_columns(df, f\"{name} after dropping\")  # Print columns after dropping\n",
    "        return df\n",
    "\n",
    "    def validate_and_fix_geometries(df, name):\n",
    "        \"\"\"\n",
    "        Validate geometries in a GeoDataFrame, ensuring they are of the correct type.\n",
    "        Filter out invalid geometries or attempt to convert them to `Polygon`/`MultiPolygon`.\n",
    "        Applied **only** to buildings.\n",
    "        \"\"\"\n",
    "        print(f\"-- Validating geometries in {name}...\")\n",
    "        \n",
    "        # Filter valid geometries\n",
    "        valid_types = [\"Polygon\", \"MultiPolygon\"]\n",
    "        \n",
    "        # Separate invalid geometries\n",
    "        invalid_geometries = df[~df.geometry.type.isin(valid_types)]\n",
    "        if len(invalid_geometries) > 0:\n",
    "            print(f\"ERROR: Invalid geometry type detected in {name}. Expected Polygon or MultiPolygon.\")\n",
    "            print(f\"-- Invalid geometries: {len(invalid_geometries)}\")\n",
    "            \n",
    "            # Attempt to convert invalid geometries to polygons\n",
    "            print(\"-- Attempting to convert invalid geometries to valid Polygon/MultiPolygon...\")\n",
    "            def convert_to_polygon(geom):\n",
    "                # Try to buffer as a fix to convert Point/LineString geometries\n",
    "                if geom.is_valid:\n",
    "                    return geom.buffer(0)  # Buffer to create a polygon around invalid geometry\n",
    "                return None  # Skip if geometry cannot be converted\n",
    "                \n",
    "            # Apply conversion\n",
    "            df.loc[~df.geometry.type.isin(valid_types), \"geometry\"] = df.loc[~df.geometry.type.isin(valid_types), \"geometry\"].apply(convert_to_polygon)\n",
    "            \n",
    "            # Remove any geometries that could not be converted\n",
    "            df = df[df.geometry.type.isin(valid_types)]\n",
    "            print(f\"-- After conversion: {len(df)} valid polygons remaining.\")\n",
    "        else:\n",
    "            print(f\"-- All geometries are valid in {name}.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def trim_overlap_percs(df, column_name, name):\n",
    "        \"\"\"\n",
    "        Trim values in the `overlap_percs` field to 4 decimal places.\n",
    "        \"\"\"\n",
    "        print(f\"-- Trimming values in {column_name} to 4 decimal places for {name}...\")\n",
    "        if column_name in df.columns:\n",
    "            df[column_name] = df[column_name].apply(lambda x: [round(value, 4) for value in x])\n",
    "        return df\n",
    "\n",
    "    # Process buildings subset\n",
    "    print(\"- Saving buildings subset...\")\n",
    "    print_columns(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset = drop_extraneous_columns(buildings_subset, columns_to_keep=[\n",
    "        \"geometry\", \"height\", \"setback_ft\", \"build_idx\", \"build_area_sm\", \"height_m\", \"height_ft\"\n",
    "    ], name=\"buildings_subset\")\n",
    "    buildings_subset = validate_and_fix_geometries(buildings_subset, \"buildings_subset\")  # Apply validation/fixing ONLY here\n",
    "    buildings_subset = ensure_unique_column_names(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n",
    "\n",
    "    # Process blocks subset\n",
    "    print(\"- Saving blocks processed...\")\n",
    "    print_columns(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed = drop_extraneous_columns(blocks_processed, columns_to_keep=[\n",
    "        \"STATEFP20\", \"COUNTYFP20\", \"TRACTCE20\", \"BLOCKCE20\", \"GEOID20\",\n",
    "        \"ALAND20\", \"AWATER20\", \"HOUSING20\", \"POP20\", \"geometry\", \"BLOCK_ID\", \"area_sm\",\n",
    "        \"pop_den\", \"avg_hght\", \"avg_sback\", \"bld_area\", \"bld_cnt\", \"bld_ctsm\", \"bld_prc\"\n",
    "    ], name=\"blocks_processed\")\n",
    "    blocks_processed = ensure_unique_column_names(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed.to_file(f\"{output_dir}/Corpus_Christi_blocks_processed.shp\")\n",
    "\n",
    "    # Process roads subset\n",
    "    print(\"- Saving roads processed...\")\n",
    "    print_columns(roads_processed, \"roads_processed\")\n",
    "    roads_processed = drop_extraneous_columns(roads_processed, columns_to_keep=[\n",
    "        \"osmid\", \"highway\", \"lanes\", \"name\", \"oneway\", \"length\", \"maxspeed\", \"geometry\",\n",
    "        \"road_id\", \"agg_pop\", \"agg_area\", \"agg_ctsm\", \"agg_bldprc\", \"agg_HHpBLD\", \"agg_hght\", \"agg_sback\",\n",
    "        \"block_ids\", \"overlap_percs\", \"evac_flag\", \"length_cal\", \"UrbanArea\", \"emp_den\"\n",
    "    ], name=\"roads_processed\")\n",
    "    roads_processed = trim_overlap_percs(roads_processed, \"overlap_percs\", \"roads_processed\")  # Trim `overlap_percs`\n",
    "    roads_processed = ensure_unique_column_names(roads_processed, \"roads_processed\")\n",
    "    roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n",
    "\n",
    "    print(\"Shapefiles saved successfully.\")\n",
    "\n",
    "# Save the processed data to shapefiles\n",
    "output_dir = \"output/rev7\"\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe0d494-81e3-4da7-acab-24623d288483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Running decision tree method...\n",
      "- Reading roads shapefile...\n",
      "- Reading classification rules CSV...\n",
      "- Reading classification priority CSV...\n",
      "- Applying classification rules...\n",
      "- Resolving classifications to single priority-based classification...\n",
      "- Saving updated shapefile to: output/rev7/Corpus_Christi_roads_processed_decisiontree.shp\n",
      "- Decision tree classification completed and file saved.\n",
      "- Running metric-based classification method...\n",
      "- Reading roads shapefile...\n",
      "- Reading metrics CSV...\n",
      "-- Metrics loaded. 25 rows to process.\n",
      "- Reading classification priority CSV...\n",
      "- Initializing classification data structures...\n",
      "- Applying metric-based classifications...\n",
      "- Resolving final classification using priority...\n",
      "- Saving updated shapefile to: output/rev7/Corpus_Christi_roads_processed_counts.shp\n",
      "- Metric-based classification completed and file saved.\n"
     ]
    }
   ],
   "source": [
    "import os  # Import os for directory creation\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Function to resolve classifications into priority-based order\n",
    "def resolve_to_priority(class_list: list, priority_mapping: dict, default_classification: str = \"NOT FOUND IN LIST\") -> str:\n",
    "    \"\"\"\n",
    "    Resolves multiple classifications to a single one based on priority csv\n",
    "    \"\"\"\n",
    "    if len(class_list) == 0:\n",
    "        return default_classification\n",
    "    valid_classes = [cls for cls in class_list if cls in priority_mapping]\n",
    "    if len(valid_classes) == 0:\n",
    "        return default_classification\n",
    "    return sorted(valid_classes, key=lambda c: priority_mapping[c])[0]\n",
    "\n",
    "# Function for decision-tree-based classification\n",
    "def classify_using_decision_tree(input_shp: str, rules_csv: str, priority_csv: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Classifies roads using decision tree rules and resolves classifications based on priority.\n",
    "    Saves a shapefile with classifications prioritized\n",
    "    \"\"\"\n",
    "    print(\"- Reading roads shapefile...\")\n",
    "    roads_gdf = gpd.read_file(input_shp)\n",
    "    print(\"- Reading classification rules CSV...\")\n",
    "    rules = pd.read_csv(rules_csv)\n",
    "    print(\"- Reading classification priority CSV...\")\n",
    "    priority = pd.read_csv(priority_csv)\n",
    "    priority_mapping = dict(zip(priority[\"Classification\"], priority[\"Priority\"]))\n",
    "    default_classification = \"NOT FOUND IN LIST\"\n",
    "\n",
    "    # Initialize empty classifications for roads\n",
    "    roads_gdf[\"All_Class\"] = [[] for _ in range(len(roads_gdf))]\n",
    "\n",
    "    print(\"- Applying classification rules...\")\n",
    "    for _, rule in rules.iterrows():\n",
    "        # Parse conditions, operators, and values from rule\n",
    "        conditions = []\n",
    "        for i in range(1, 10):  # Process up to 9 conditions\n",
    "            condition = rule.get(f\"Condition_{i}\")\n",
    "            operator = rule.get(f\"Operator_{i}\")\n",
    "            value = rule.get(f\"Value_{i}\")\n",
    "\n",
    "            # Skip if any part of the condition is missing\n",
    "            if pd.isna(condition) or pd.isna(operator) or pd.isna(value):\n",
    "                continue\n",
    "\n",
    "            # Format the value properly (add quotes for string, leave numeric as is)\n",
    "            if isinstance(value, str):\n",
    "                value = f'\"{value}\"'\n",
    "            else:\n",
    "                value = str(value)\n",
    "\n",
    "            # Append the condition as a valid query string\n",
    "            conditions.append(f\"({condition} {operator} {value})\")\n",
    "\n",
    "        # Combine conditions into a query string\n",
    "        if len(conditions) > 0:\n",
    "            query = \" and \".join(conditions)\n",
    "        else:\n",
    "            continue  # Skip rules without valid conditions\n",
    "\n",
    "        # Classification from rule\n",
    "        classification = rule[\"Classification\"]\n",
    "\n",
    "        try:\n",
    "            # Evaluate query and get matching road indices\n",
    "            matching_indices = roads_gdf.query(query).index\n",
    "            for idx in matching_indices:\n",
    "                roads_gdf.at[idx, \"All_Class\"].append(classification)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying rule: {rule}\")\n",
    "            print(f\"Generated query: {query}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            continue  # Skip to the next rule\n",
    "\n",
    "    print(\"- Resolving classifications to single priority-based classification...\")\n",
    "    roads_gdf[\"FINALCLASS\"] = roads_gdf[\"All_Class\"].apply(\n",
    "        lambda class_list: resolve_to_priority(class_list, priority_mapping, default_classification)\n",
    "    )\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the resulting shapefile\n",
    "    output_shp = f\"{output_dir}/Corpus_Christi_roads_processed_decisiontree.shp\"\n",
    "    print(f\"- Saving updated shapefile to: {output_shp}\")\n",
    "    roads_gdf.to_file(output_shp, driver=\"ESRI Shapefile\")\n",
    "    print(\"- Decision tree classification completed and file saved.\")\n",
    "\n",
    "# Function for point-based metric classification\n",
    "def classify_using_counts(input_shp: str, metrics_csv: str, priority_csv: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Classifies roads based on metrics using ranges, weights, and priorities\n",
    "    Resolves classification ties using the resolve_to_priority function\n",
    "    Saves a shapefile with classifications\n",
    "    \"\"\"\n",
    "    print(\"- Reading roads shapefile...\")\n",
    "    roads_gdf = gpd.read_file(input_shp)\n",
    "    print(\"- Reading metrics CSV...\")\n",
    "    metrics = pd.read_csv(metrics_csv)\n",
    "    metrics = metrics[metrics[\"Metric\"].notna()]\n",
    "    print(f\"-- Metrics loaded. {len(metrics)} rows to process.\")\n",
    "    print(\"- Reading classification priority CSV...\")\n",
    "    priority = pd.read_csv(priority_csv)\n",
    "    priority_mapping = dict(zip(priority[\"Classification\"], priority[\"Priority\"]))\n",
    "    default_classification = \"Unclassified\"\n",
    "\n",
    "    print(\"- Initializing classification data structures...\")\n",
    "    roads_gdf[\"Class_PNT\"] = [{} for _ in range(len(roads_gdf))]\n",
    "\n",
    "    print(\"- Applying metric-based classifications...\")\n",
    "    for _, rule in metrics.iterrows():\n",
    "        metric = rule[\"Metric\"]\n",
    "        min_value = rule[\"Min\"]\n",
    "        max_value = rule[\"Max\"]\n",
    "        weight = rule[\"Weight\"]\n",
    "        classification = rule[\"Classification\"]\n",
    "\n",
    "        # Build query conditions for the metric range\n",
    "        query = f\"{metric} > {min_value}\" if not pd.isna(min_value) else \"\"\n",
    "        if not pd.isna(max_value):\n",
    "            query += f\" and {metric} <= {max_value}\" if query else f\"{metric} <= {max_value}\"\n",
    "\n",
    "        # Apply query to get matching indices\n",
    "        matching_indices = roads_gdf.query(query).index\n",
    "        for idx in matching_indices:\n",
    "            # Update points for classification\n",
    "            roads_gdf.at[idx, \"Class_PNT\"].setdefault(classification, 0)\n",
    "            roads_gdf.at[idx, \"Class_PNT\"][classification] += weight\n",
    "\n",
    "    print(\"- Resolving final classification using priority...\")\n",
    "    def resolve_tie(classes_points):\n",
    "        \"\"\"\n",
    "        Resolves the classification when point values tie\n",
    "        Uses the resolve_to_priority function to determine the final priority-based classification\n",
    "        \"\"\"\n",
    "        if not classes_points:\n",
    "            return default_classification\n",
    "        max_point_value = max(classes_points.values())\n",
    "        tied_classes = [\n",
    "            c for c, points in classes_points.items() if points == max_point_value\n",
    "        ]\n",
    "        if len(tied_classes) == 1:  # No tie\n",
    "            return tied_classes[0]\n",
    "        # Resolve tie using priority mapping\n",
    "        return resolve_to_priority(tied_classes, priority_mapping, default_classification)\n",
    "\n",
    "    # Apply tie-resolution logic to each row\n",
    "    roads_gdf[\"FINALCLASS\"] = roads_gdf[\"Class_PNT\"].apply(resolve_tie)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the resulting shapefile\n",
    "    output_shp = f\"{output_dir}/Corpus_Christi_roads_processed_counts.shp\"\n",
    "    print(f\"- Saving updated shapefile to: {output_shp}\")\n",
    "    roads_gdf.to_file(output_shp, driver=\"ESRI Shapefile\")\n",
    "    print(\"- Metric-based classification completed and file saved.\")\n",
    "\n",
    "# CHANGE VALUES HERE FOR YOUR INPUTS\n",
    "input_shp_path = \"output/rev7/Corpus_Christi_roads_processed.shp\"\n",
    "rules_csv_path = \"decisiontree_3.csv\"\n",
    "priority_csv_path = \"Classification_Priority.csv\"\n",
    "counts_csv_path = \"Classification_Point.csv\"\n",
    "output_directory = \"output/rev7\"\n",
    "\n",
    "# Run Decision Tree Classification\n",
    "print(\"- Running decision tree method...\")\n",
    "classify_using_decision_tree(\n",
    "    input_shp=input_shp_path,\n",
    "    rules_csv=rules_csv_path,\n",
    "    priority_csv=priority_csv_path,\n",
    "    output_dir=output_directory\n",
    ")\n",
    "\n",
    "# Run Metric-Based Classification\n",
    "print(\"- Running metric-based classification method...\")\n",
    "classify_using_counts(\n",
    "    input_shp=input_shp_path,\n",
    "    metrics_csv=counts_csv_path,\n",
    "    priority_csv=priority_csv_path,\n",
    "    output_dir=output_directory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
