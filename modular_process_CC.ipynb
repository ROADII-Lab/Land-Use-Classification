{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1b3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Converting center coordinates (27.783611, -97.414779) to UTM...\n",
      "Corpus Christi center in UTM (EPSG:32614): (656184.519263486, 3074239.5261650323)\n",
      "- Checking cache for blocks...\n",
      "Loading data from cache: cache\\blocks.pkl\n",
      "- Checking cache for buildings...\n",
      "Loading data from cache: cache\\buildings.pkl\n",
      "- Checking cache for roads...\n",
      "Loading data from cache: cache\\roads.pkl\n",
      "- Subsampling datasets...\n",
      "- Subsampling data using bounding box...\n",
      "-- Bounding box geometry: POLYGON ((656946.519263486 3073477.5261650323, 656946.519263486 3075001.5261650323, 655422.519263486 3075001.5261650323, 655422.519263486 3073477.5261650323, 656946.519263486 3073477.5261650323))\n",
      "-- Blocks CRS: EPSG:32614\n",
      "-- Buildings CRS before reprojection: EPSG:32614\n",
      "-- Roads CRS: EPSG:32614\n",
      "-- Buildings CRS after reprojection: EPSG:32614\n",
      "-- Buildings subset shape after intersects: (1622, 198)\n",
      "-- Subsampled 160 blocks, 1622 buildings, and 634 roads.\n",
      "- Data loading complete.\n",
      "-- Blocks subset:\n",
      "       STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "211117        48        355    001100      1028  483550011001028   \n",
      "211215        48        355    001202      1030  483550012021030   \n",
      "316218        48        355    001000      4013  483550010004013   \n",
      "316219        48        355    001000      4017  483550010004017   \n",
      "316220        48        355    001300      1001  483550013001001   \n",
      "\n",
      "                       GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20 FUNCSTAT20  \\\n",
      "211117  1000000US483550011001028  Block1028   G5040    U  20287          S   \n",
      "211215  1000000US483550012021030  Block1030   G5040    U  20287          S   \n",
      "316218  1000000US483550010004013  Block4013   G5040    U  20287          S   \n",
      "316219  1000000US483550010004017  Block4017   G5040    U  20287          S   \n",
      "316220  1000000US483550013001001  Block1001   G5040    U  20287          S   \n",
      "\n",
      "        ALAND20  AWATER20   INTPTLAT20    INTPTLON20  HOUSING20  POP20  \\\n",
      "211117    39102         0  +27.7876717  -097.4162704          0      0   \n",
      "211215    13835         0  +27.7818257  -097.4066443         16     34   \n",
      "316218     9962         0  +27.7850464  -097.4079537         12     12   \n",
      "316219    13996         0  +27.7816977  -097.4087415         16     43   \n",
      "316220    12522         0  +27.7776745  -097.4124932          0      0   \n",
      "\n",
      "                                                 geometry  \n",
      "211117  POLYGON ((655808.425 3074684.031, 655808.908 3...  \n",
      "211215  POLYGON ((656931.526 3074114.103, 656952.401 3...  \n",
      "316218  POLYGON ((656801.724 3074449.076, 656820.212 3...  \n",
      "316219  POLYGON ((656725.762 3074097.032, 656746.534 3...  \n",
      "316220  POLYGON ((656342.866 3073631.427, 656360.103 3...  \n",
      "-- Buildings subset:\n",
      "                                                             geometry  \\\n",
      "element  id                                                             \n",
      "relation 7631335    POLYGON ((655971.44 3073866.272, 655976.652 30...   \n",
      "way      360098785  POLYGON ((656275.96 3073709.551, 656277.505 30...   \n",
      "         360098790  POLYGON ((656166.068 3073703.956, 656182.247 3...   \n",
      "         360098791  POLYGON ((656258.097 3073723.571, 656260.633 3...   \n",
      "         520297581  POLYGON ((656957.178 3074855.464, 656966.725 3...   \n",
      "\n",
      "                   addr:state  building  ele gnis:feature_id name source  \\\n",
      "element  id                                                                \n",
      "relation 7631335          NaN  hospital  NaN             NaN  NaN    NaN   \n",
      "way      360098785        NaN       yes  NaN             NaN  NaN   Bing   \n",
      "         360098790        NaN       yes  NaN             NaN  NaN   Bing   \n",
      "         360098791        NaN       yes  NaN             NaN  NaN   Bing   \n",
      "         520297581        NaN       yes  NaN             NaN  NaN    NaN   \n",
      "\n",
      "                   addr:city addr:housename addr:housenumber  ...  \\\n",
      "element  id                                                   ...   \n",
      "relation 7631335         NaN            NaN              NaN  ...   \n",
      "way      360098785       NaN            NaN              NaN  ...   \n",
      "         360098790       NaN            NaN              NaN  ...   \n",
      "         360098791       NaN            NaN              NaN  ...   \n",
      "         520297581       NaN            NaN              NaN  ...   \n",
      "\n",
      "                   check_date:opening_hours:drive_through fuel:octane_87  \\\n",
      "element  id                                                                \n",
      "relation 7631335                                      NaN            NaN   \n",
      "way      360098785                                    NaN            NaN   \n",
      "         360098790                                    NaN            NaN   \n",
      "         360098791                                    NaN            NaN   \n",
      "         520297581                                    NaN            NaN   \n",
      "\n",
      "                   fuel:octane_89 fuel:octane_93 female male portable  \\\n",
      "element  id                                                             \n",
      "relation 7631335              NaN            NaN    NaN  NaN      NaN   \n",
      "way      360098785            NaN            NaN    NaN  NaN      NaN   \n",
      "         360098790            NaN            NaN    NaN  NaN      NaN   \n",
      "         360098791            NaN            NaN    NaN  NaN      NaN   \n",
      "         520297581            NaN            NaN    NaN  NaN      NaN   \n",
      "\n",
      "                   toilets:handwashing capacity size  \n",
      "element  id                                           \n",
      "relation 7631335                   NaN      NaN  NaN  \n",
      "way      360098785                 NaN      NaN  NaN  \n",
      "         360098790                 NaN      NaN  NaN  \n",
      "         360098791                 NaN      NaN  NaN  \n",
      "         520297581                 NaN      NaN  NaN  \n",
      "\n",
      "[5 rows x 198 columns]\n",
      "-- Roads subset:\n",
      "                             osmid      highway lanes             name  \\\n",
      "u         v         key                                                  \n",
      "227454545 227454551 0     21145794     tertiary   NaN  Highland Avenue   \n",
      "          227502421 0    371318831  residential   NaN      Ruth Street   \n",
      "          227502418 0    371318832     tertiary   NaN      Ruth Street   \n",
      "227454551 227454554 0     21145794     tertiary   NaN  Highland Avenue   \n",
      "          227536777 0     21160049  residential   NaN  Hibiscus Street   \n",
      "\n",
      "                         oneway  ref reversed      length maxspeed  \\\n",
      "u         v         key                                              \n",
      "227454545 227454551 0     False  NaN    False  117.941931      NaN   \n",
      "          227502421 0     False  NaN    False   26.240267      NaN   \n",
      "          227502418 0     False  NaN     True   42.143124      NaN   \n",
      "227454551 227454554 0     False  NaN    False  152.238928      NaN   \n",
      "          227536777 0     False  NaN    False  166.763729      NaN   \n",
      "\n",
      "                                                                  geometry  \\\n",
      "u         v         key                                                      \n",
      "227454545 227454551 0    LINESTRING (655587.283 3074123.505, 655546.617...   \n",
      "          227502421 0    LINESTRING (655587.283 3074123.505, 655575.176...   \n",
      "          227502418 0    LINESTRING (655587.283 3074123.505, 655604.94 ...   \n",
      "227454551 227454554 0    LINESTRING (655495.562 3074049.411, 655374.926...   \n",
      "          227536777 0    LINESTRING (655495.562 3074049.411, 655501.252...   \n",
      "\n",
      "                        bridge junction width access  \n",
      "u         v         key                               \n",
      "227454545 227454551 0      NaN      NaN   NaN    NaN  \n",
      "          227502421 0      NaN      NaN   NaN    NaN  \n",
      "          227502418 0      NaN      NaN   NaN    NaN  \n",
      "227454551 227454554 0      NaN      NaN   NaN    NaN  \n",
      "          227536777 0      NaN      NaN   NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "### Utility Functions ###\n",
    "def read_path_from_file(file_path: str) -> str:\n",
    "    \"\"\"Read OneDrive path from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            path = file.readline().strip()  # Read the first line and strip whitespace\n",
    "        return path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_cache(data, filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Save a GeoDataFrame or Python object to a pickle file in the cache directory.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    print(f\"Saving data to cache: {cache_path}\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_from_cache(filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Load a GeoDataFrame or Python object from a pickle file in the cache directory.\"\"\"\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading data from cache: {cache_path}\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def latlon_to_utm(lat: float, lon: float, epsg: int = 32614):\n",
    "    \"\"\"\n",
    "    Convert latitude and longitude (EPSG:4326) to UTM coordinates (e.g., EPSG:32614).\n",
    "    \"\"\"\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg}\", always_xy=True)\n",
    "    x, y = transformer.transform(lon, lat)  # Transform to target coordinates\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def subsample_data(center_point: tuple, scale: float, blocks: gpd.GeoDataFrame, buildings: gpd.GeoDataFrame, roads: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Subsample blocks, buildings, and roads using a bounding box around a central point.\n",
    "    Ensures proper alignment of CRS before subsampling.\n",
    "    \"\"\"\n",
    "    print(\"- Subsampling data using bounding box...\")\n",
    "\n",
    "    # Default bounding box size: ~10,000 feet (~3,048 meters) on each side\n",
    "    default_bbox_size = 3048  # Half the total size on each dimension, in meters\n",
    "    scaled_bbox_size = default_bbox_size * scale\n",
    "\n",
    "    # Calculate bounding box geometry based on scaled size\n",
    "    center_x, center_y = center_point\n",
    "    bbox = box(\n",
    "        center_x - scaled_bbox_size,  # Min X\n",
    "        center_y - scaled_bbox_size,  # Min Y\n",
    "        center_x + scaled_bbox_size,  # Max X\n",
    "        center_y + scaled_bbox_size   # Max Y\n",
    "    )\n",
    "    print(\"-- Bounding box geometry:\", bbox)\n",
    "\n",
    "    # Debugging: Check CRS alignment for all datasets\n",
    "    print(\"-- Blocks CRS:\", blocks.crs)\n",
    "    print(\"-- Buildings CRS before reprojection:\", buildings.crs)\n",
    "    print(\"-- Roads CRS:\", roads.crs)\n",
    "\n",
    "    # Reproject buildings to match the CRS of the bounding box (EPSG:32614)\n",
    "    if buildings.crs.to_string().lower() != \"epsg:32614\":\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "    print(\"-- Buildings CRS after reprojection:\", buildings.crs)\n",
    "\n",
    "    # Filter each GeoDataFrame by the bounding box intersection\n",
    "    blocks_subset = blocks[blocks.geometry.intersects(bbox)]\n",
    "    buildings_subset = buildings[buildings.geometry.intersects(bbox)]\n",
    "    roads_subset = roads[roads.geometry.intersects(bbox)]\n",
    "\n",
    "    # Debugging: Check if any buildings are retained in the subset\n",
    "    print(\"-- Buildings subset shape after intersects:\", buildings_subset.shape)\n",
    "    if len(buildings_subset) == 0:\n",
    "        print(\"-- Warning: No buildings found within the bounding box.\")\n",
    "        print(\"-- Original buildings dataset sample (after reprojection):\")\n",
    "        print(buildings.geometry.head())  # Inspect original geometries\n",
    "\n",
    "    print(f\"-- Subsampled {len(blocks_subset)} blocks, {len(buildings_subset)} buildings, and {len(roads_subset)} roads.\")\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "def load_data(block_path, osm_boundary_place, cache_dir=\"cache\", subsample_scale=1.0):\n",
    "    \"\"\"\n",
    "    Load blocks, buildings, and roads data from cache (if available), process, and optionally subsample using a scaled bounding box.\n",
    "    Uses only the 'height' column for building heights and outputs the percentage of valid values.\n",
    "    \"\"\"\n",
    "    # Corpus Christi center point from Google Maps\n",
    "    corpus_christi_lat = 27.783611  # Latitude\n",
    "    corpus_christi_lon = -97.414779  # Longitude\n",
    "\n",
    "    print(f\"Converting center coordinates ({corpus_christi_lat}, {corpus_christi_lon}) to UTM...\")\n",
    "    corpus_christi_center_utm = latlon_to_utm(corpus_christi_lat, corpus_christi_lon)  # Convert to UTM (EPSG:32614)\n",
    "    print(f\"Corpus Christi center in UTM (EPSG:32614): {corpus_christi_center_utm}\")\n",
    "\n",
    "    # Check cache for blocks\n",
    "    print(\"- Checking cache for blocks...\")\n",
    "    blocks = load_from_cache(\"blocks.pkl\", cache_dir)\n",
    "    if blocks is None:\n",
    "        print(\"-- Loading block data...\")\n",
    "        blocks = gpd.read_file(block_path).to_crs(epsg=32614)\n",
    "        blocks.loc[:, \"POP20\"] = pd.to_numeric(blocks[\"POP20\"], errors=\"coerce\")  # Explicitly use `.loc`\n",
    "        save_to_cache(blocks, \"blocks.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for buildings\n",
    "    print(\"- Checking cache for buildings...\")\n",
    "    buildings = load_from_cache(\"buildings.pkl\", cache_dir)\n",
    "    if buildings is None:\n",
    "        print(\"-- Fetching building data...\")\n",
    "        buildings = ox.features_from_place(\n",
    "            osm_boundary_place,\n",
    "            tags={\"building\": True, \"building:height\": True, \"building:levels\": True}\n",
    "        )\n",
    "\n",
    "        print(\"-- Raw buildings dataset columns:\")\n",
    "        print(buildings.columns)  # Print available columns for validation\n",
    "        print(\"-- Preview of raw building dataset:\")\n",
    "        print(buildings.head())  # Inspect raw data for possible errors\n",
    "\n",
    "        # Focus only on the 'height' column for building heights\n",
    "        print(\"-- Focusing on the 'height' column for building heights...\")\n",
    "        if \"height\" in buildings.columns:\n",
    "            print(\"-- 'height' column found. Attempting numerical conversion...\")\n",
    "            buildings.loc[:, \"height\"] = pd.to_numeric(buildings[\"height\"], errors=\"coerce\")\n",
    "\n",
    "            # Debugging: Calculate percentage of valid values (>0 and numeric)\n",
    "            total_height_values = len(buildings)\n",
    "            valid_height_values = buildings[\"height\"].dropna().gt(0).sum()\n",
    "            percentage_valid = (valid_height_values / total_height_values) * 100 if total_height_values > 0 else 0\n",
    "            print(f\"-- Valid height values: {valid_height_values} / Total: {total_height_values} ({percentage_valid:.2f}%)\")\n",
    "\n",
    "            print(\"-- Converted 'height' values to numeric. Example values:\")\n",
    "            print(buildings[\"height\"].head())\n",
    "        else:\n",
    "            print(\"-- ERROR: 'height' column is missing in the dataset! Setting height to None.\")\n",
    "            buildings.loc[:, \"height\"] = None\n",
    "\n",
    "        # Reproject buildings to CRS: EPSG:32614\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "        # Save the buildings dataset to cache\n",
    "        save_to_cache(buildings, \"buildings.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for roads\n",
    "    print(\"- Checking cache for roads...\")\n",
    "    roads = load_from_cache(\"roads.pkl\", cache_dir)\n",
    "    if roads is None:\n",
    "        print(\"-- Fetching road data...\")\n",
    "        roads = ox.graph_to_gdfs(ox.graph_from_place(osm_boundary_place, network_type=\"drive\"), nodes=False)\n",
    "        roads = roads.to_crs(epsg=32614)\n",
    "        save_to_cache(roads, \"roads.pkl\", cache_dir)\n",
    "\n",
    "    # Subsample datasets using the bounding box around Corpus Christi center in UTM\n",
    "    print(\"- Subsampling datasets...\")\n",
    "    blocks_subset, buildings_subset, roads_subset = subsample_data(corpus_christi_center_utm, subsample_scale, blocks, buildings, roads)\n",
    "    print(\"- Data loading complete.\")\n",
    "\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "# Define paths to block shapefile and Corpus Christi boundary\n",
    "one_drive_path = read_path_from_file(\"OneDrive.txt\")\n",
    "block_path = os.path.join(one_drive_path, \"Data\", \"tl_2023_48_tabblock20\", \"tl_2023_48_tabblock20.shp\")\n",
    "osm_boundary_place = \"Corpus Christi, Texas, USA\"\n",
    "\n",
    "# Load data with subsampling\n",
    "print(\"Starting data loading...\")\n",
    "blocks_subset, buildings_subset, roads_subset = load_data(block_path, osm_boundary_place, subsample_scale=0.25)\n",
    "\n",
    "# Inspect subsampled data\n",
    "print(\"-- Blocks subset:\")\n",
    "print(blocks_subset.head())\n",
    "print(\"-- Buildings subset:\")\n",
    "print(buildings_subset.head())\n",
    "print(\"-- Roads subset:\")\n",
    "print(roads_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc4d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in buildings dataset:\n",
      "Index(['geometry', 'addr:state', 'building', 'ele', 'gnis:feature_id', 'name',\n",
      "       'source', 'addr:city', 'addr:housename', 'addr:housenumber',\n",
      "       ...\n",
      "       'check_date:opening_hours:drive_through', 'fuel:octane_87',\n",
      "       'fuel:octane_89', 'fuel:octane_93', 'female', 'male', 'portable',\n",
      "       'toilets:handwashing', 'capacity', 'size'],\n",
      "      dtype='object', length=198)\n",
      "Calculating block metrics...\n",
      "Calculating building area coverage (sq-mile)...\n",
      "Processing building heights...\n",
      "Calculating building setbacks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_16756\\1787435943.py:33: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating buildings with blocks via spatial join...\n",
      "Aggregating building metrics per block...\n",
      "Calculating population density and other metrics for blocks...\n",
      "Calculating additional block metrics: Building count per square mile and building area percentage...\n",
      "Merging building metrics into block dataset...\n",
      "Filling missing values for block metrics...\n",
      "Block metrics calculated successfully.\n",
      "Block metrics successfully calculated:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001100      1028  483550011001028   \n",
      "1        48        355    001202      1030  483550012021030   \n",
      "2        48        355    001000      4013  483550010004013   \n",
      "3        48        355    001000      4017  483550010004017   \n",
      "4        48        355    001300      1001  483550013001001   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ...  \\\n",
      "0  1000000US483550011001028  Block1028   G5040    U  20287  ...   \n",
      "1  1000000US483550012021030  Block1030   G5040    U  20287  ...   \n",
      "2  1000000US483550010004013  Block4013   G5040    U  20287  ...   \n",
      "3  1000000US483550010004017  Block4017   G5040    U  20287  ...   \n",
      "4  1000000US483550013001001  Block1001   G5040    U  20287  ...   \n",
      "\n",
      "                                            geometry  BLOCK_ID   area_sm  \\\n",
      "0  POLYGON ((655808.425 3074684.031, 655808.908 3...    211117  0.015095   \n",
      "1  POLYGON ((656931.526 3074114.103, 656952.401 3...    211215  0.005341   \n",
      "2  POLYGON ((656801.724 3074449.076, 656820.212 3...    316218  0.003846   \n",
      "3  POLYGON ((656725.762 3074097.032, 656746.534 3...    316219  0.005403   \n",
      "4  POLYGON ((656342.866 3073631.427, 656360.103 3...    316220  0.004834   \n",
      "\n",
      "       pop_den   avg_hght  avg_sback  bld_area bld_cnt     bld_ctsm    bld_prc  \n",
      "0     0.000000  43.879412  53.606100  0.003305     9.0   596.240897  21.892470  \n",
      "1  6366.290170  31.233597  41.231358  0.000048     1.0   187.243829   0.900601  \n",
      "2  3120.504416  29.353423  49.380499  0.000369    13.0  3380.546450   9.589928  \n",
      "3  7958.542980  32.637328  48.844595  0.000922    14.0  2591.153528  17.069260  \n",
      "4     0.000000  37.598426  60.856869  0.000721     2.0   413.736603  14.907763  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "def calculate_block_metrics(blocks_subset: gpd.GeoDataFrame, buildings_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate block-level metrics\n",
    "    \"\"\"\n",
    "    print(\"Available columns in buildings dataset:\")\n",
    "    print(buildings_subset.columns)\n",
    "\n",
    "    # Check if required columns exist in the buildings dataset\n",
    "    required_columns = [\"geometry\", \"height\"]\n",
    "    missing_columns = [col for col in required_columns if col not in buildings_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing required columns in buildings dataset: {missing_columns}\")\n",
    "        print(\"Preview of buildings dataset:\")\n",
    "        print(buildings_subset.head())\n",
    "        raise KeyError(f\"Required columns {missing_columns} are not found in buildings dataset.\")\n",
    "\n",
    "    print(\"Calculating block metrics...\")\n",
    "    blocks_subset[\"BLOCK_ID\"] = blocks_subset.index  # Assign unique ID to each block\n",
    "    buildings_subset[\"build_idx\"] = buildings_subset.index  # Assign unique ID to each building\n",
    "\n",
    "    # Calculate building area (square meters -> square miles)\n",
    "    print(\"Calculating building area coverage (sq-mile)...\")\n",
    "    buildings_subset[\"build_area_sm\"] = buildings_subset.geometry.area * 0.00000038610215855  # Convert area to sq-mile\n",
    "\n",
    "    # Handle building height (convert meters to feet and filter valid heights)\n",
    "    print(\"Processing building heights...\")\n",
    "    buildings_subset[\"height_m\"] = pd.to_numeric(buildings_subset[\"height\"], errors=\"coerce\")\n",
    "    buildings_subset[\"height_ft\"] = buildings_subset[\"height_m\"] * 3.28084  # Convert height to feet\n",
    "    buildings_subset = buildings_subset[buildings_subset[\"height_ft\"] > 0]  # Filter buildings with positive heights\n",
    "\n",
    "    # Calculate building setbacks (distance to nearest road)\n",
    "    print(\"Calculating building setbacks...\")\n",
    "    roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
    "    buildings_subset[\"setback_ft\"] = buildings_subset.geometry.apply(\n",
    "        lambda building_geom: building_geom.distance(roads_union) * 3.28084  # Convert meters to feet\n",
    "    )\n",
    "\n",
    "    # Spatial join to associate buildings with blocks\n",
    "    print(\"Associating buildings with blocks via spatial join...\")\n",
    "    buildings_with_blocks = gpd.sjoin(buildings_subset, blocks_subset, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    # Aggregate metrics per block\n",
    "    print(\"Aggregating building metrics per block...\")\n",
    "    building_metrics = buildings_with_blocks.groupby(\"BLOCK_ID\").agg(\n",
    "        avg_hght=(\"height_ft\", \"mean\"),  # Renamed: Average building height\n",
    "        avg_sback=(\"setback_ft\", \"mean\"),  # Renamed: Average setback distance\n",
    "        bld_area=(\"build_area_sm\", \"sum\"),  # Renamed: Sum building areas\n",
    "        bld_cnt=(\"build_idx\", \"count\"),  # Renamed: Count number of buildings\n",
    "    )\n",
    "\n",
    "    # Calculate population density for blocks\n",
    "    print(\"Calculating population density and other metrics for blocks...\")\n",
    "    blocks_subset[\"area_sm\"] = blocks_subset.geometry.area * 0.00000038610215855  # Convert block area to sq-mile\n",
    "    blocks_subset[\"pop_den\"] = blocks_subset[\"POP20\"] / blocks_subset[\"area_sm\"]  # Renamed: Population density per sq-mile\n",
    "\n",
    "    # Derive additional metrics\n",
    "    print(\"Calculating additional block metrics: Building count per square mile and building area percentage...\")\n",
    "    building_metrics[\"bld_ctsm\"] = building_metrics[\"bld_cnt\"] / blocks_subset[\"area_sm\"]  # Renamed: Building count per sq-mile\n",
    "    building_metrics[\"bld_prc\"] = (building_metrics[\"bld_area\"] / blocks_subset[\"area_sm\"]) * 100  # Renamed: Building area as percentage of block area\n",
    "\n",
    "    # Merge metrics into block dataset\n",
    "    print(\"Merging building metrics into block dataset...\")\n",
    "    blocks_subset = blocks_subset.merge(building_metrics, on=\"BLOCK_ID\", how=\"left\")\n",
    "\n",
    "    # Fill missing values explicitly for columns\n",
    "    print(\"Filling missing values for block metrics...\")\n",
    "    blocks_subset[\"avg_hght\"] = blocks_subset[\"avg_hght\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"avg_sback\"] = blocks_subset[\"avg_sback\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_area\"] = blocks_subset[\"bld_area\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_ctsm\"] = blocks_subset[\"bld_ctsm\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_prc\"] = blocks_subset[\"bld_prc\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_cnt\"] = blocks_subset[\"bld_cnt\"].fillna(0)  # Replace NaN with 0\n",
    "\n",
    "    print(\"Block metrics calculated successfully.\")\n",
    "    return blocks_subset\n",
    "\n",
    "# Calculate block-level metrics\n",
    "blocks_processed = calculate_block_metrics(blocks_subset, buildings_subset, roads_subset)\n",
    "print(\"Block metrics successfully calculated:\")\n",
    "print(blocks_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48683f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculating road metrics with buffer size (for spatial analysis): 50 feet...\n",
      "-- Inspecting blocks structure before processing:\n",
      "Index(['STATEFP20', 'COUNTYFP20', 'TRACTCE20', 'BLOCKCE20', 'GEOID20',\n",
      "       'GEOIDFQ20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20',\n",
      "       'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20', 'HOUSING20', 'POP20',\n",
      "       'geometry', 'BLOCK_ID', 'area_sm', 'pop_den', 'avg_hght', 'avg_sback',\n",
      "       'bld_area', 'bld_cnt', 'bld_ctsm', 'bld_prc'],\n",
      "      dtype='object')\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001100      1028  483550011001028   \n",
      "1        48        355    001202      1030  483550012021030   \n",
      "2        48        355    001000      4013  483550010004013   \n",
      "3        48        355    001000      4017  483550010004017   \n",
      "4        48        355    001300      1001  483550013001001   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ...  \\\n",
      "0  1000000US483550011001028  Block1028   G5040    U  20287  ...   \n",
      "1  1000000US483550012021030  Block1030   G5040    U  20287  ...   \n",
      "2  1000000US483550010004013  Block4013   G5040    U  20287  ...   \n",
      "3  1000000US483550010004017  Block4017   G5040    U  20287  ...   \n",
      "4  1000000US483550013001001  Block1001   G5040    U  20287  ...   \n",
      "\n",
      "                                            geometry  BLOCK_ID   area_sm  \\\n",
      "0  POLYGON ((655808.425 3074684.031, 655808.908 3...    211117  0.015095   \n",
      "1  POLYGON ((656931.526 3074114.103, 656952.401 3...    211215  0.005341   \n",
      "2  POLYGON ((656801.724 3074449.076, 656820.212 3...    316218  0.003846   \n",
      "3  POLYGON ((656725.762 3074097.032, 656746.534 3...    316219  0.005403   \n",
      "4  POLYGON ((656342.866 3073631.427, 656360.103 3...    316220  0.004834   \n",
      "\n",
      "       pop_den   avg_hght  avg_sback  bld_area bld_cnt     bld_ctsm    bld_prc  \n",
      "0     0.000000  43.879412  53.606100  0.003305     9.0   596.240897  21.892470  \n",
      "1  6366.290170  31.233597  41.231358  0.000048     1.0   187.243829   0.900601  \n",
      "2  3120.504416  29.353423  49.380499  0.000369    13.0  3380.546450   9.589928  \n",
      "3  7958.542980  32.637328  48.844595  0.000922    14.0  2591.153528  17.069260  \n",
      "4     0.000000  37.598426  60.856869  0.000721     2.0   413.736603  14.907763  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "- Creating road buffers...\n",
      "-- Road buffers structure after buffering:\n",
      "Index(['osmid', 'highway', 'lanes', 'name', 'oneway', 'ref', 'reversed',\n",
      "       'length', 'maxspeed', 'geometry', 'bridge', 'junction', 'width',\n",
      "       'access', 'road_id'],\n",
      "      dtype='object')\n",
      "- Performing spatial join of blocks with road buffers...\n",
      "-- Intersections structure after spatial join:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001100      1028  483550011001028   \n",
      "1        48        355    001000      1000  483550010001000   \n",
      "2        48        355    001000      1001  483550010001001   \n",
      "3        48        355    001000      2005  483550010002005   \n",
      "4        48        355    001000      1006  483550010001006   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ...       ref  \\\n",
      "0  1000000US483550011001028  Block1028   G5040    U  20287  ...  Spur 544   \n",
      "1  1000000US483550010001000  Block1000   G5040    U  20287  ...  Spur 544   \n",
      "2  1000000US483550010001001  Block1001   G5040    U  20287  ...  Spur 544   \n",
      "3  1000000US483550010002005  Block2005   G5040    U  20287  ...  Spur 544   \n",
      "4  1000000US483550010001006  Block1006   G5040    U  20287  ...  Spur 544   \n",
      "\n",
      "   reversed      length maxspeed bridge  junction  width  access  road_id  \\\n",
      "0     False  129.157447      NaN    NaN       NaN    NaN     NaN       47   \n",
      "1     False  129.157447      NaN    NaN       NaN    NaN     NaN       47   \n",
      "2     False  129.157447      NaN    NaN       NaN    NaN     NaN       47   \n",
      "3     False  129.157447      NaN    NaN       NaN    NaN     NaN       47   \n",
      "4     False  129.157447      NaN    NaN       NaN    NaN     NaN       47   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((656008.326 3074620.007, 655983.013 3...  \n",
      "1  POLYGON ((655983.013 3074618.794, 656008.326 3...  \n",
      "2  POLYGON ((655954.376 3074615.544, 655963.724 3...  \n",
      "3  POLYGON ((656089.126 3074598.11, 656088.656 30...  \n",
      "4  POLYGON ((656101.937 3074597.949, 656101.959 3...  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "-- Validating `block_area` and recalculating if needed...\n",
      "-- Intersection overlap proportions sample:\n",
      "   BLOCK_ID  road_id  overlap_area    block_area  overlap_proportion\n",
      "0    211117       47    308.614591  39094.757023            0.007894\n",
      "1    476004       47   1902.813662   3531.701324            0.538781\n",
      "2    608947       47   2168.958798  24467.106705            0.088648\n",
      "3    608955       47    142.799433  10028.428790            0.014239\n",
      "4    608956       47    148.581371   6304.172787            0.023569\n",
      "- Creating overlap dictionary for roads...\n",
      "-- Adding block IDs and overlap percentages columns...\n",
      "- Loading evacuation routes shapefile...\n",
      "-- Flagging roads based on evacuation route overlap...\n",
      "- Weighting metrics for roads...\n",
      "- Aggregating weighted metrics across roads...\n",
      "-- Aggregated road metrics structure:\n",
      "        agg_pop  agg_area     agg_ctsm   agg_hght  agg_sback\n",
      "0   3978.153909  0.001071  1601.603484  33.567179  59.081885\n",
      "1   2154.152041  0.001038  1747.033626  34.196582  48.007559\n",
      "2   1132.866601  0.001252  1843.635726  35.036536  45.472436\n",
      "3  10323.980918  0.000629  2025.610849  30.121713  67.250783\n",
      "4   9486.452880  0.000962  2207.122905  31.785245  65.187387\n",
      "- Merging metrics into roads dataset...\n",
      "-- Final roads structure:\n",
      "       osmid      highway lanes             name  oneway  ref reversed  \\\n",
      "0   21145794     tertiary   NaN  Highland Avenue   False  NaN    False   \n",
      "1  371318831  residential   NaN      Ruth Street   False  NaN    False   \n",
      "2  371318832     tertiary   NaN      Ruth Street   False  NaN     True   \n",
      "3   21145794     tertiary   NaN  Highland Avenue   False  NaN    False   \n",
      "4   21160049  residential   NaN  Hibiscus Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  117.941931      NaN  LINESTRING (655587.283 3074123.505, 655546.617...   \n",
      "1   26.240267      NaN  LINESTRING (655587.283 3074123.505, 655575.176...   \n",
      "2   42.143124      NaN  LINESTRING (655587.283 3074123.505, 655604.94 ...   \n",
      "3  152.238928      NaN  LINESTRING (655495.562 3074049.411, 655374.926...   \n",
      "4  166.763729      NaN  LINESTRING (655495.562 3074049.411, 655501.252...   \n",
      "\n",
      "   ... access road_id                                 block_ids  \\\n",
      "0  ...    NaN       0          [633377, 633378, 633380, 633383]   \n",
      "1  ...    NaN       1          [367208, 633377, 633378, 633383]   \n",
      "2  ...    NaN       2  [359479, 606931, 633377, 633378, 633383]   \n",
      "3  ...    NaN       3                  [633378, 633380, 633383]   \n",
      "4  ...    NaN       4  [476034, 633378, 633380, 633383, 633384]   \n",
      "\n",
      "                                       overlap_percs  evac_flag       agg_pop  \\\n",
      "0  [0.015555468895943045, 0.06787825537214592, 0....          0   3978.153909   \n",
      "1  [0.00672684170098161, 0.03678591524781138, 0.0...          0   2154.152041   \n",
      "2  [0.005091677094137467, 0.011208897415474322, 0...          0   1132.866601   \n",
      "3  [0.09382057750007784, 0.14584958623093353, 0.0...          0  10323.980918   \n",
      "4  [0.0133656694076713, 0.012473326609076067, 0.1...          0   9486.452880   \n",
      "\n",
      "   agg_area     agg_ctsm   agg_hght  agg_sback  \n",
      "0  0.001071  1601.603484  33.567179  59.081885  \n",
      "1  0.001038  1747.033626  34.196582  48.007559  \n",
      "2  0.001252  1843.635726  35.036536  45.472436  \n",
      "3  0.000629  2025.610849  30.121713  67.250783  \n",
      "4  0.000962  2207.122905  31.785245  65.187387  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "-- Final processed roads sample:\n",
      "       osmid      highway lanes             name  oneway  ref reversed  \\\n",
      "0   21145794     tertiary   NaN  Highland Avenue   False  NaN    False   \n",
      "1  371318831  residential   NaN      Ruth Street   False  NaN    False   \n",
      "2  371318832     tertiary   NaN      Ruth Street   False  NaN     True   \n",
      "3   21145794     tertiary   NaN  Highland Avenue   False  NaN    False   \n",
      "4   21160049  residential   NaN  Hibiscus Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  117.941931      NaN  LINESTRING (655587.283 3074123.505, 655546.617...   \n",
      "1   26.240267      NaN  LINESTRING (655587.283 3074123.505, 655575.176...   \n",
      "2   42.143124      NaN  LINESTRING (655587.283 3074123.505, 655604.94 ...   \n",
      "3  152.238928      NaN  LINESTRING (655495.562 3074049.411, 655374.926...   \n",
      "4  166.763729      NaN  LINESTRING (655495.562 3074049.411, 655501.252...   \n",
      "\n",
      "   ... access road_id                                 block_ids  \\\n",
      "0  ...    NaN       0          [633377, 633378, 633380, 633383]   \n",
      "1  ...    NaN       1          [367208, 633377, 633378, 633383]   \n",
      "2  ...    NaN       2  [359479, 606931, 633377, 633378, 633383]   \n",
      "3  ...    NaN       3                  [633378, 633380, 633383]   \n",
      "4  ...    NaN       4  [476034, 633378, 633380, 633383, 633384]   \n",
      "\n",
      "                                       overlap_percs  evac_flag       agg_pop  \\\n",
      "0  [0.015555468895943045, 0.06787825537214592, 0....          0   3978.153909   \n",
      "1  [0.00672684170098161, 0.03678591524781138, 0.0...          0   2154.152041   \n",
      "2  [0.005091677094137467, 0.011208897415474322, 0...          0   1132.866601   \n",
      "3  [0.09382057750007784, 0.14584958623093353, 0.0...          0  10323.980918   \n",
      "4  [0.0133656694076713, 0.012473326609076067, 0.1...          0   9486.452880   \n",
      "\n",
      "   agg_area     agg_ctsm   agg_hght  agg_sback  \n",
      "0  0.001071  1601.603484  33.567179  59.081885  \n",
      "1  0.001038  1747.033626  34.196582  48.007559  \n",
      "2  0.001252  1843.635726  35.036536  45.472436  \n",
      "3  0.000629  2025.610849  30.121713  67.250783  \n",
      "4  0.000962  2207.122905  31.785245  65.187387  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_road_metrics(blocks_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame, buffer_size_feet=50) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate road-level metrics based on spatial overlap with blocks (weighted aggregation).\n",
    "    Includes Block IDs and Overlap Percentages as columns, and flags roads intersecting evacuation routes using buffered geometry.\n",
    "    \"\"\"\n",
    "    print(f\"- Calculating road metrics with buffer size (for spatial analysis): {buffer_size_feet} feet...\")\n",
    "    \n",
    "    buffer_size_meters = buffer_size_feet * 0.3048  # Convert buffer size to meters\n",
    "\n",
    "    # Blocks subset should maintain the original BLOCK_ID\n",
    "    blocks_subset = blocks_subset.copy()\n",
    "    print(\"-- Inspecting blocks structure before processing:\")\n",
    "    print(blocks_subset.columns)\n",
    "    print(blocks_subset.head())  # Output a sample of blocks_subset\n",
    "\n",
    "    roads_subset = roads_subset.copy().reset_index(drop=True)\n",
    "    roads_subset[\"road_id\"] = roads_subset.index\n",
    "\n",
    "    # Check if required block-level metrics exist\n",
    "    required_columns = [\"area_sm\", \"pop_den\", \"bld_area\", \"bld_ctsm\", \"avg_hght\", \"avg_sback\"]\n",
    "    missing_columns = [col for col in required_columns if col not in blocks_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: The following required block-level metrics are missing: {missing_columns}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Create buffers for roads\n",
    "    print(\"- Creating road buffers...\")\n",
    "    road_buffers = roads_subset.copy()\n",
    "    road_buffers[\"geometry\"] = roads_subset.geometry.buffer(buffer_size_meters)\n",
    "    road_buffers[\"road_id\"] = roads_subset[\"road_id\"]\n",
    "    print(\"-- Road buffers structure after buffering:\")\n",
    "    print(road_buffers.columns)\n",
    "\n",
    "    # Perform spatial join of blocks with road buffers\n",
    "    print(\"- Performing spatial join of blocks with road buffers...\")\n",
    "    intersections = gpd.overlay(blocks_subset, road_buffers, how=\"intersection\")\n",
    "    print(\"-- Intersections structure after spatial join:\")\n",
    "    print(intersections.head())\n",
    "\n",
    "    # Validate block areas and overlap proportions\n",
    "    print(\"-- Validating `block_area` and recalculating if needed...\")\n",
    "    blocks_subset[\"block_area\"] = blocks_subset.geometry.area\n",
    "    intersections[\"block_area\"] = intersections[\"BLOCK_ID\"].map(blocks_subset.set_index(\"BLOCK_ID\")[\"block_area\"])\n",
    "    intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "    intersections[\"overlap_proportion\"] = intersections[\"overlap_area\"] / intersections[\"block_area\"]\n",
    "    print(\"-- Intersection overlap proportions sample:\")\n",
    "    print(intersections[[\"BLOCK_ID\", \"road_id\", \"overlap_area\", \"block_area\", \"overlap_proportion\"]].head())\n",
    "\n",
    "    # Create overlap dictionary for roads\n",
    "    print(\"- Creating overlap dictionary for roads...\")\n",
    "    road_block_overlap = (\n",
    "        intersections.groupby(\"road_id\")\n",
    "        .apply(lambda rows: dict(zip(rows[\"BLOCK_ID\"], rows[\"overlap_proportion\"])))\n",
    "        .to_dict()\n",
    "    )\n",
    "    print(\"-- Adding block IDs and overlap percentages columns...\")\n",
    "    # Add block IDs and overlap percentages columns\n",
    "    roads_subset[\"block_ids\"] = [\n",
    "        list(overlap_dict.keys()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "    roads_subset[\"overlap_percs\"] = [\n",
    "        list(overlap_dict.values()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "\n",
    "    # Load Evacuation Routes\n",
    "    print(\"- Loading evacuation routes shapefile...\")\n",
    "    evac_path = os.path.join(one_drive_path, \"Data\", \"TxDOT Evacuation Routes AGO.shp\")\n",
    "    evacuation_routes = gpd.read_file(evac_path)\n",
    "\n",
    "    # Ensure both datasets use the same CRS for spatial operations\n",
    "    if evacuation_routes.crs != road_buffers.crs:\n",
    "        evacuation_routes = evacuation_routes.to_crs(road_buffers.crs)\n",
    "\n",
    "    print(\"-- Flagging roads based on evacuation route overlap...\")\n",
    "    def flag_evacuation_routes(road_buffer_geom):\n",
    "        \"\"\"\n",
    "        Flag roads based on overlap with evacuation routes.\n",
    "        0 = No overlap\n",
    "        1 = Major Evacuation Routes\n",
    "        2 = Potential Contraflow\n",
    "        3 = Potential EvacuLanes\n",
    "        \"\"\"\n",
    "        for _, evac_row in evacuation_routes.iterrows():\n",
    "            if road_buffer_geom.intersects(evac_row.geometry):\n",
    "                route_type = evac_row[\"ROUTE_TYPE\"]\n",
    "                if route_type == \"Major Evacuation Routes\":\n",
    "                    return 1\n",
    "                elif route_type == \"Potential Contraflow\":\n",
    "                    return 2\n",
    "                elif route_type == \"Potential EvacuLanes\":\n",
    "                    return 3\n",
    "        return 0  # No overlap\n",
    "\n",
    "    # Use the buffered geometry to calculate evacuation flags\n",
    "    roads_subset[\"evac_flag\"] = road_buffers.geometry.apply(flag_evacuation_routes)\n",
    "\n",
    "    # Calculate weighted metrics for roads\n",
    "    print(\"- Weighting metrics for roads...\")\n",
    "    def compute_weighted_metrics(road_id, overlap_dict):\n",
    "        \"\"\"\n",
    "        Compute weighted metrics for a road using normalized weights for `agg_hght` and `agg_sback`.\n",
    "        \"\"\"\n",
    "        # Subset relevant blocks using overlap proportions\n",
    "        blocks = blocks_subset.set_index(\"BLOCK_ID\").loc[overlap_dict.keys()]\n",
    "        raw_weights = pd.Series({BLOCK_ID: overlap_dict[BLOCK_ID] for BLOCK_ID in blocks.index})  # Use overlap proportions\n",
    "\n",
    "        # Normalize weights for accurate weighted averages\n",
    "        total_weight = raw_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = raw_weights / total_weight  # Ensure weights add up to 1\n",
    "        else:\n",
    "            # If total weight is zero return zero for all aggregated metrics\n",
    "            return pd.Series({\n",
    "                \"agg_pop\": 0,\n",
    "                \"agg_area\": 0,\n",
    "                \"agg_ctsm\": 0,\n",
    "                \"agg_hght\": 0,\n",
    "                \"agg_sback\": 0,\n",
    "            })\n",
    "\n",
    "        # Separate filtering for aggregated height and setback\n",
    "        valid_blocks_for_hght_sback = blocks[(blocks[\"avg_hght\"] > 0) & (blocks[\"avg_sback\"] > 0)]\n",
    "\n",
    "        # Compute weighted metrics\n",
    "        aggregated_metrics = {\n",
    "            \"agg_pop\": (blocks[\"pop_den\"] * normalized_weights).sum(),\n",
    "            \"agg_area\": (blocks[\"bld_area\"] * normalized_weights).sum(),\n",
    "            \"agg_ctsm\": (blocks[\"bld_ctsm\"] * normalized_weights).sum(),\n",
    "            \"agg_hght\": (valid_blocks_for_hght_sback[\"avg_hght\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,  # Handle empty blocks edge case\n",
    "            \"agg_sback\": (valid_blocks_for_hght_sback[\"avg_sback\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0, \n",
    "        }\n",
    "        return pd.Series(aggregated_metrics)\n",
    "\n",
    "    print(\"- Aggregating weighted metrics across roads...\")\n",
    "    road_metrics_df = pd.DataFrame([\n",
    "        compute_weighted_metrics(road_id, overlap_dict)\n",
    "        for road_id, overlap_dict in road_block_overlap.items()\n",
    "    ], index=road_block_overlap.keys())\n",
    "    print(\"-- Aggregated road metrics structure:\")\n",
    "    print(road_metrics_df.head())\n",
    "\n",
    "    # Merge aggregated metrics into roads dataset\n",
    "    print(\"- Merging metrics into roads dataset...\")\n",
    "    roads_subset = roads_subset.merge(road_metrics_df, left_on=\"road_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    print(\"-- Final roads structure:\")\n",
    "    print(roads_subset.head())\n",
    "    return roads_subset\n",
    "\n",
    "# Execute road metrics calculation\n",
    "roads_processed = calculate_road_metrics(blocks_processed, roads_subset, buffer_size_feet=50)\n",
    "print(\"-- Final processed roads sample:\")\n",
    "print(roads_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving subsets to shapefiles...\n",
      "- Saving buildings subset...\n",
      "-- Full column names in buildings_subset:\n",
      "  geometry\n",
      "  addr:state\n",
      "  building\n",
      "  ele\n",
      "  gnis:feature_id\n",
      "  name\n",
      "  source\n",
      "  addr:city\n",
      "  addr:housename\n",
      "  addr:housenumber\n",
      "  addr:postcode\n",
      "  addr:street\n",
      "  amenity\n",
      "  brand\n",
      "  brand:wikidata\n",
      "  cuisine\n",
      "  healthcare\n",
      "  healthcare:counselling\n",
      "  height\n",
      "  phone\n",
      "  website\n",
      "  generator:method\n",
      "  generator:output:electricity\n",
      "  generator:source\n",
      "  generator:type\n",
      "  layer\n",
      "  power\n",
      "  shop\n",
      "  opening_hours\n",
      "  ref\n",
      "  wholesale\n",
      "  addr:unit\n",
      "  official_name\n",
      "  takeaway\n",
      "  note\n",
      "  email\n",
      "  museum\n",
      "  tourism\n",
      "  addr:country\n",
      "  alt_name\n",
      "  contact:email\n",
      "  contact:facebook\n",
      "  contact:twitter\n",
      "  man_made\n",
      "  content\n",
      "  office\n",
      "  start_date\n",
      "  building:levels\n",
      "  leisure\n",
      "  sport\n",
      "  wikidata\n",
      "  check_date\n",
      "  fee\n",
      "  fixme\n",
      "  heritage\n",
      "  heritage:operator\n",
      "  historic\n",
      "  ref:nrhp\n",
      "  ship:type\n",
      "  wikipedia\n",
      "  building:part\n",
      "  disused:shop\n",
      "  type\n",
      "  aeroway\n",
      "  operator\n",
      "  short_name\n",
      "  parking\n",
      "  old_name\n",
      "  branch\n",
      "  air_conditioning\n",
      "  drive_through\n",
      "  opening_hours:drive_through\n",
      "  indoor_seating\n",
      "  outdoor_seating\n",
      "  delivery\n",
      "  payment:NFC_mobile_payments\n",
      "  smoking\n",
      "  access\n",
      "  brand:website\n",
      "  brand:wikipedia\n",
      "  denomination\n",
      "  religion\n",
      "  payment:cash\n",
      "  payment:contactless\n",
      "  payment:credit_cards\n",
      "  payment:debit_cards\n",
      "  payment:nfc\n",
      "  website:menu\n",
      "  wheelchair\n",
      "  operator:wikidata\n",
      "  drive_in\n",
      "  compressed_air\n",
      "  toilets\n",
      "  operator:short\n",
      "  operator:type\n",
      "  operator:website\n",
      "  operator:wikipedia\n",
      "  fuel:diesel\n",
      "  fuel:gasoline\n",
      "  self_service\n",
      "  bar\n",
      "  fax\n",
      "  internet_access\n",
      "  internet_access:fee\n",
      "  reservation\n",
      "  rooms\n",
      "  landuse\n",
      "  ref:walmart\n",
      "  atm\n",
      "  shelter_type\n",
      "  roof:shape\n",
      "  roof:levels\n",
      "  diet:chicken\n",
      "  diet:dairy\n",
      "  diet:meat\n",
      "  image\n",
      "  contact:instagram\n",
      "  description\n",
      "  second_hand\n",
      "  screen\n",
      "  bin\n",
      "  toilets:disposal\n",
      "  unisex\n",
      "  social_facility\n",
      "  social_facility:for\n",
      "  beauty\n",
      "  healthcare:speciality\n",
      "  clothes\n",
      "  dispensing\n",
      "  was:shop\n",
      "  check_date:opening_hours\n",
      "  service:vehicle:inspection\n",
      "  service:vehicle:oil_change\n",
      "  contact:foursquare\n",
      "  drinking_water\n",
      "  opening_hours:covid19\n",
      "  highchair\n",
      "  abandoned\n",
      "  automated\n",
      "  payment:coins\n",
      "  telecom\n",
      "  craft\n",
      "  after_school\n",
      "  isced:level\n",
      "  nursery\n",
      "  preschool\n",
      "  abandoned:building\n",
      "  microbrewery\n",
      "  contact:website\n",
      "  not:brand:wikidata\n",
      "  teaching\n",
      "  swimming_pool\n",
      "  bus\n",
      "  public_transport\n",
      "  contact:phone\n",
      "  service:vehicle:car_repair\n",
      "  building:min_level\n",
      "  max_level\n",
      "  min_level\n",
      "  theatre\n",
      "  dance:style\n",
      "  dance:teaching\n",
      "  disused\n",
      "  architect\n",
      "  building:use\n",
      "  diocese\n",
      "  building_1\n",
      "  urgent_care\n",
      "  emergency\n",
      "  lit\n",
      "  bridge\n",
      "  level\n",
      "  community_centre:for\n",
      "  construction_equipment:rental\n",
      "  tool:rental\n",
      "  condo\n",
      "  service:vehicle:car_parts\n",
      "  service:vehicle:new_car_sales\n",
      "  service:vehicle:used_car_sales\n",
      "  bench\n",
      "  government\n",
      "  payment:american_express\n",
      "  payment:mastercard\n",
      "  payment:visa\n",
      "  animal_shelter\n",
      "  animal_shelter:adoption\n",
      "  animal_shelter:release\n",
      "  pets\n",
      "  check_date:opening_hours:drive_through\n",
      "  fuel:octane_87\n",
      "  fuel:octane_89\n",
      "  fuel:octane_93\n",
      "  female\n",
      "  male\n",
      "  portable\n",
      "  toilets:handwashing\n",
      "  capacity\n",
      "  size\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Dropping extraneous columns from buildings_subset: ['addr:state', 'building', 'ele', 'gnis:feature_id', 'name', 'source', 'addr:city', 'addr:housename', 'addr:housenumber', 'addr:postcode', 'addr:street', 'amenity', 'brand', 'brand:wikidata', 'cuisine', 'healthcare', 'healthcare:counselling', 'phone', 'website', 'generator:method', 'generator:output:electricity', 'generator:source', 'generator:type', 'layer', 'power', 'shop', 'opening_hours', 'ref', 'wholesale', 'addr:unit', 'official_name', 'takeaway', 'note', 'email', 'museum', 'tourism', 'addr:country', 'alt_name', 'contact:email', 'contact:facebook', 'contact:twitter', 'man_made', 'content', 'office', 'start_date', 'building:levels', 'leisure', 'sport', 'wikidata', 'check_date', 'fee', 'fixme', 'heritage', 'heritage:operator', 'historic', 'ref:nrhp', 'ship:type', 'wikipedia', 'building:part', 'disused:shop', 'type', 'aeroway', 'operator', 'short_name', 'parking', 'old_name', 'branch', 'air_conditioning', 'drive_through', 'opening_hours:drive_through', 'indoor_seating', 'outdoor_seating', 'delivery', 'payment:NFC_mobile_payments', 'smoking', 'access', 'brand:website', 'brand:wikipedia', 'denomination', 'religion', 'payment:cash', 'payment:contactless', 'payment:credit_cards', 'payment:debit_cards', 'payment:nfc', 'website:menu', 'wheelchair', 'operator:wikidata', 'drive_in', 'compressed_air', 'toilets', 'operator:short', 'operator:type', 'operator:website', 'operator:wikipedia', 'fuel:diesel', 'fuel:gasoline', 'self_service', 'bar', 'fax', 'internet_access', 'internet_access:fee', 'reservation', 'rooms', 'landuse', 'ref:walmart', 'atm', 'shelter_type', 'roof:shape', 'roof:levels', 'diet:chicken', 'diet:dairy', 'diet:meat', 'image', 'contact:instagram', 'description', 'second_hand', 'screen', 'bin', 'toilets:disposal', 'unisex', 'social_facility', 'social_facility:for', 'beauty', 'healthcare:speciality', 'clothes', 'dispensing', 'was:shop', 'check_date:opening_hours', 'service:vehicle:inspection', 'service:vehicle:oil_change', 'contact:foursquare', 'drinking_water', 'opening_hours:covid19', 'highchair', 'abandoned', 'automated', 'payment:coins', 'telecom', 'craft', 'after_school', 'isced:level', 'nursery', 'preschool', 'abandoned:building', 'microbrewery', 'contact:website', 'not:brand:wikidata', 'teaching', 'swimming_pool', 'bus', 'public_transport', 'contact:phone', 'service:vehicle:car_repair', 'building:min_level', 'max_level', 'min_level', 'theatre', 'dance:style', 'dance:teaching', 'disused', 'architect', 'building:use', 'diocese', 'building_1', 'urgent_care', 'emergency', 'lit', 'bridge', 'level', 'community_centre:for', 'construction_equipment:rental', 'tool:rental', 'condo', 'service:vehicle:car_parts', 'service:vehicle:new_car_sales', 'service:vehicle:used_car_sales', 'bench', 'government', 'payment:american_express', 'payment:mastercard', 'payment:visa', 'animal_shelter', 'animal_shelter:adoption', 'animal_shelter:release', 'pets', 'check_date:opening_hours:drive_through', 'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male', 'portable', 'toilets:handwashing', 'capacity', 'size']\n",
      "-- Full column names in buildings_subset after dropping:\n",
      "  geometry\n",
      "  height\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_16756\\788882335.py:66: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saving blocks processed...\n",
      "-- Full column names in blocks_processed:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  GEOIDFQ20\n",
      "  NAME20\n",
      "  MTFCC20\n",
      "  UR20\n",
      "  UACE20\n",
      "  FUNCSTAT20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  INTPTLAT20\n",
      "  INTPTLON20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "-- Dropping extraneous columns from blocks_processed: ['MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20', 'INTPTLAT20', 'INTPTLON20']\n",
      "-- Full column names in blocks_processed after dropping:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  GEOIDFQ20\n",
      "  NAME20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "- Saving roads processed...\n",
      "-- Full column names in roads_processed:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  ref\n",
      "  reversed\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  bridge\n",
      "  junction\n",
      "  width\n",
      "  access\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "-- Dropping extraneous columns from roads_processed: ['ref', 'reversed', 'bridge', 'junction', 'width', 'access']\n",
      "-- Full column names in roads_processed after dropping:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_hght\n",
      "  agg_sback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_16756\\788882335.py:88: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefiles saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "def save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrames to shapefiles while ensuring the correct geometry column is activated.\n",
    "    Includes functionality to drop extraneous columns only if they exist and to print columns after dropping.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving subsets to shapefiles...\")\n",
    "\n",
    "    # Suppress warnings about truncated column names for ESRI Shapefiles\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Normalized/laundered field name.*\", category=RuntimeWarning)\n",
    "\n",
    "    def ensure_unique_column_names(df, name):\n",
    "        \"\"\"\n",
    "        Ensure column names in a GeoDataFrame are unique by appending trailing numbers to duplicates.\n",
    "        \"\"\"\n",
    "        duplicates = df.columns[df.columns.duplicated()].unique()\n",
    "        if len(duplicates) > 0:\n",
    "            print(f\"WARNING: Duplicate column names detected in {name}: {duplicates}\")\n",
    "            df = df.rename(columns=lambda x: x[:10])  # Truncate names to 10 characters for ESRI compatibility\n",
    "            # Resolve duplicates by appending a suffix\n",
    "            seen = set()\n",
    "            new_columns = []\n",
    "            for col in df.columns:\n",
    "                if col in seen:\n",
    "                    count = sum([existing.startswith(col) for existing in seen]) + 1\n",
    "                    new_col = f\"{col[:7]}_{count}\"  # Add numeric suffix to fix duplicates\n",
    "                    print(f\"    Renaming column '{col}' to '{new_col}' to resolve duplication.\")\n",
    "                    new_columns.append(new_col)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "                seen.add(new_columns[-1])\n",
    "            df.columns = new_columns\n",
    "        return df\n",
    "\n",
    "    def print_columns(df, name):\n",
    "        \"\"\"\n",
    "        Print column names in their entirety for a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        print(f\"-- Full column names in {name}:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}\")\n",
    "\n",
    "    def drop_extraneous_columns(df, columns_to_keep, name):\n",
    "        \"\"\"\n",
    "        Drop columns not included in the `columns_to_keep` list, only if they exist in the dataset.\n",
    "        \"\"\"\n",
    "        columns_to_drop = [col for col in df.columns if col not in columns_to_keep]\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Ensure column exists\n",
    "        if columns_to_drop:\n",
    "            print(f\"-- Dropping extraneous columns from {name}: {columns_to_drop}\")\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "        print_columns(df, f\"{name} after dropping\")  # Print columns after dropping\n",
    "        return df\n",
    "\n",
    "    # Save buildings subset\n",
    "    print(\"- Saving buildings subset...\")\n",
    "    print_columns(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset = drop_extraneous_columns(buildings_subset, columns_to_keep=[\n",
    "        \"geometry\", \"height\", \"build_idx\", \"build_area_sm\", \"height_m\", \"height_ft\"\n",
    "    ], name=\"buildings_subset\")\n",
    "    buildings_subset = ensure_unique_column_names(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n",
    "\n",
    "    # Save blocks subset\n",
    "    print(\"- Saving blocks processed...\")\n",
    "    print_columns(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed = drop_extraneous_columns(blocks_processed, columns_to_keep=[\n",
    "        \"STATEFP20\", \"COUNTYFP20\", \"TRACTCE20\", \"BLOCKCE20\", \"GEOID20\", \"GEOIDFQ20\", \"NAME20\",\n",
    "        \"ALAND20\", \"AWATER20\", \"HOUSING20\", \"POP20\", \"geometry\", \"BLOCK_ID\", \"area_sm\",\n",
    "        \"pop_den\", \"avg_hght\", \"avg_sback\", \"bld_area\", \"bld_cnt\", \"bld_ctsm\", \"bld_prc\"\n",
    "    ], name=\"blocks_processed\")\n",
    "    blocks_processed = ensure_unique_column_names(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed.to_file(f\"{output_dir}/Corpus_Christi_blocks_processed.shp\")\n",
    "\n",
    "    # Save roads subset\n",
    "    print(\"- Saving roads processed...\")\n",
    "    print_columns(roads_processed, \"roads_processed\")\n",
    "    roads_processed = drop_extraneous_columns(roads_processed, columns_to_keep=[\n",
    "        \"osmid\", \"highway\", \"lanes\", \"name\", \"oneway\", \"length\", \"maxspeed\", \"geometry\",\n",
    "        \"road_id\", \"agg_pop\", \"agg_area\", \"agg_ctsm\", \"agg_hght\", \"agg_sback\",\n",
    "        \"block_ids\", \"overlap_percs\", \"evac_flag\"  # New columns added explicitly to the list\n",
    "    ], name=\"roads_processed\")\n",
    "    roads_processed = ensure_unique_column_names(roads_processed, \"roads_processed\")\n",
    "    roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n",
    "\n",
    "    print(\"Shapefiles saved successfully.\")\n",
    "\n",
    "# Save the processed data to shapefiles\n",
    "save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0d494-81e3-4da7-acab-24623d288483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
