{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1b3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Converting center coordinates (27.783611, -97.414779) to UTM...\n",
      "Corpus Christi center in UTM (EPSG:32614): (656184.519263486, 3074239.5261650323)\n",
      "- Checking cache for blocks...\n",
      "Loading data from cache: cache\\blocks.pkl\n",
      "- Checking cache for buildings...\n",
      "Loading data from cache: cache\\buildings.pkl\n",
      "- Checking cache for roads...\n",
      "Loading data from cache: cache\\roads.pkl\n",
      "- Subsampling datasets...\n",
      "- Subsampling data using bounding box...\n",
      "-- Bounding box geometry: POLYGON ((660756.519263486 3069667.5261650323, 660756.519263486 3078811.5261650323, 651612.519263486 3078811.5261650323, 651612.519263486 3069667.5261650323, 660756.519263486 3069667.5261650323))\n",
      "-- Blocks CRS: EPSG:32614\n",
      "-- Buildings CRS before reprojection: EPSG:32614\n",
      "-- Roads CRS: EPSG:32614\n",
      "-- Buildings CRS after reprojection: EPSG:32614\n",
      "-- Buildings subset shape after intersects: (31386, 198)\n",
      "-- Subsampled 1969 blocks, 31386 buildings, and 9035 roads.\n",
      "- Data loading complete.\n",
      "Calculating building setbacks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_30644\\1336055196.py:180: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Blocks subset:\n",
      "       STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "211079        48        355    001904      2001  483550019042001   \n",
      "211080        48        355    002002      2008  483550020022008   \n",
      "211081        48        355    001300      4003  483550013004003   \n",
      "211082        48        355    001300      4004  483550013004004   \n",
      "211083        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                       GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20 FUNCSTAT20  \\\n",
      "211079  1000000US483550019042001  Block2001   G5040    U  20287          S   \n",
      "211080  1000000US483550020022008  Block2008   G5040    U  20287          S   \n",
      "211081  1000000US483550013004003  Block4003   G5040    U  20287          S   \n",
      "211082  1000000US483550013004004  Block4004   G5040    U  20287          S   \n",
      "211083  1000000US483550013004012  Block4012   G5040    U  20287          S   \n",
      "\n",
      "        ALAND20  AWATER20   INTPTLAT20    INTPTLON20  HOUSING20  POP20  \\\n",
      "211079    35973         0  +27.7446668  -097.4119843         49    124   \n",
      "211080    35948         0  +27.7448396  -097.4071982         46    111   \n",
      "211081    24467         0  +27.7697013  -097.4050378         32     77   \n",
      "211082    17105         0  +27.7691166  -097.4057168          0      0   \n",
      "211083    25512         0  +27.7677189  -097.4037248         42     88   \n",
      "\n",
      "                                                 geometry  \n",
      "211079  POLYGON ((656383.497 3069802.332, 656388.176 3...  \n",
      "211080  POLYGON ((656781.143 3070143.675, 656831.652 3...  \n",
      "211081  POLYGON ((657076.274 3072786.758, 657080.514 3...  \n",
      "211082  POLYGON ((657029.407 3072699.161, 657058.753 3...  \n",
      "211083  POLYGON ((657185.832 3072577.964, 657270.53 30...  \n",
      "-- Buildings subset:\n",
      "                                          geometry addr:state building  ele  \\\n",
      "element id                                                                    \n",
      "node    368160094   POINT (657977.733 3075774.146)         TX      yes   10   \n",
      "        368164119   POINT (658027.202 3076877.861)         TX    house    1   \n",
      "        368164527   POINT (657937.057 3074696.302)         TX      yes   11   \n",
      "        368164702   POINT (658013.486 3076875.577)         TX    house    1   \n",
      "        8223520640  POINT (658135.205 3075904.051)         TX      yes  NaN   \n",
      "\n",
      "                   gnis:feature_id                        name         source  \\\n",
      "element id                                                                      \n",
      "node    368160094          2031465  Broadway Bluff Improvement  USGS Geonames   \n",
      "        368164119          2031422     Charlotte Sidbury House  USGS Geonames   \n",
      "        368164527          2031675          Richard King House  USGS Geonames   \n",
      "        368164702          2032681                      S Juli  USGS Geonames   \n",
      "        8223520640             NaN        Treatment Associates            NaN   \n",
      "\n",
      "                         addr:city addr:housename addr:housenumber  ...  \\\n",
      "element id                                                          ...   \n",
      "node    368160094              NaN            NaN              NaN  ...   \n",
      "        368164119              NaN            NaN              NaN  ...   \n",
      "        368164527              NaN            NaN              NaN  ...   \n",
      "        368164702              NaN            NaN              NaN  ...   \n",
      "        8223520640  Corpus Christi            NaN              720  ...   \n",
      "\n",
      "                   fuel:octane_87 fuel:octane_89 fuel:octane_93 female male  \\\n",
      "element id                                                                    \n",
      "node    368160094             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164119             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164527             NaN            NaN            NaN    NaN  NaN   \n",
      "        368164702             NaN            NaN            NaN    NaN  NaN   \n",
      "        8223520640            NaN            NaN            NaN    NaN  NaN   \n",
      "\n",
      "                   portable toilets:handwashing capacity size  setback_ft  \n",
      "element id                                                                 \n",
      "node    368160094       NaN                 NaN      NaN  NaN    8.142634  \n",
      "        368164119       NaN                 NaN      NaN  NaN   45.583627  \n",
      "        368164527       NaN                 NaN      NaN  NaN  124.052313  \n",
      "        368164702       NaN                 NaN      NaN  NaN   44.269826  \n",
      "        8223520640      NaN                 NaN      NaN  NaN   60.298453  \n",
      "\n",
      "[5 rows x 199 columns]\n",
      "-- Roads subset:\n",
      "                             osmid      highway lanes                  name  \\\n",
      "u         v         key                                                       \n",
      "227454417 227539516 0     21156845  residential   NaN      Winnebago Street   \n",
      "          227491475 0     21156845  residential   NaN      Winnebago Street   \n",
      "          227536659 0     21160211    secondary     2     North Port Avenue   \n",
      "          227579522 0     21160211    secondary     2     North Port Avenue   \n",
      "227454455 227538593 0    831149706    secondary   NaN  South Alameda Street   \n",
      "\n",
      "                         oneway  ref reversed      length maxspeed  \\\n",
      "u         v         key                                              \n",
      "227454417 227539516 0     False  NaN    False  111.207782      NaN   \n",
      "          227491475 0     False  NaN     True  239.351195      NaN   \n",
      "          227536659 0     False  NaN    False   91.759789   40 mph   \n",
      "          227579522 0     False  NaN     True  181.553317   40 mph   \n",
      "227454455 227538593 0     False  NaN    False   98.207032      NaN   \n",
      "\n",
      "                                                                  geometry  \\\n",
      "u         v         key                                                      \n",
      "227454417 227539516 0    LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "          227491475 0    LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "          227536659 0    LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "          227579522 0    LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "227454455 227538593 0    LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "                        bridge junction width access  \n",
      "u         v         key                               \n",
      "227454417 227539516 0      NaN      NaN   NaN    NaN  \n",
      "          227491475 0      NaN      NaN   NaN    NaN  \n",
      "          227536659 0      NaN      NaN   NaN    NaN  \n",
      "          227579522 0      NaN      NaN   NaN    NaN  \n",
      "227454455 227538593 0      NaN      NaN   NaN    NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "### Utility Functions ###\n",
    "def read_path_from_file(file_path: str) -> str:\n",
    "    \"\"\"Read OneDrive path from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            path = file.readline().strip()  # Read the first line and strip whitespace\n",
    "        return path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_cache(data, filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Save a GeoDataFrame or Python object to a pickle file in the cache directory.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    print(f\"Saving data to cache: {cache_path}\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_from_cache(filename, cache_dir=\"cache\"):\n",
    "    \"\"\"Load a GeoDataFrame or Python object from a pickle file in the cache directory.\"\"\"\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading data from cache: {cache_path}\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def latlon_to_utm(lat: float, lon: float, epsg: int = 32614):\n",
    "    \"\"\"\n",
    "    Convert latitude and longitude (EPSG:4326) to UTM coordinates (e.g., EPSG:32614).\n",
    "    \"\"\"\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{epsg}\", always_xy=True)\n",
    "    x, y = transformer.transform(lon, lat)  # Transform to target coordinates\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def subsample_data(center_point: tuple, scale: float, blocks: gpd.GeoDataFrame, buildings: gpd.GeoDataFrame, roads: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Subsample blocks, buildings, and roads using a bounding box around a central point.\n",
    "    Ensures proper alignment of CRS before subsampling.\n",
    "    \"\"\"\n",
    "    print(\"- Subsampling data using bounding box...\")\n",
    "\n",
    "    # Default bounding box size: ~10,000 feet (~3,048 meters) on each side\n",
    "    default_bbox_size = 3048  # Half the total size on each dimension, in meters\n",
    "    scaled_bbox_size = default_bbox_size * scale\n",
    "\n",
    "    # Calculate bounding box geometry based on scaled size\n",
    "    center_x, center_y = center_point\n",
    "    bbox = box(\n",
    "        center_x - scaled_bbox_size,  # Min X\n",
    "        center_y - scaled_bbox_size,  # Min Y\n",
    "        center_x + scaled_bbox_size,  # Max X\n",
    "        center_y + scaled_bbox_size   # Max Y\n",
    "    )\n",
    "    print(\"-- Bounding box geometry:\", bbox)\n",
    "\n",
    "    # Debugging: Check CRS alignment for all datasets\n",
    "    print(\"-- Blocks CRS:\", blocks.crs)\n",
    "    print(\"-- Buildings CRS before reprojection:\", buildings.crs)\n",
    "    print(\"-- Roads CRS:\", roads.crs)\n",
    "\n",
    "    # Reproject buildings to match the CRS of the bounding box (EPSG:32614)\n",
    "    if buildings.crs.to_string().lower() != \"epsg:32614\":\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "    print(\"-- Buildings CRS after reprojection:\", buildings.crs)\n",
    "\n",
    "    # Filter each GeoDataFrame by the bounding box intersection\n",
    "    blocks_subset = blocks[blocks.geometry.intersects(bbox)]\n",
    "    buildings_subset = buildings[buildings.geometry.intersects(bbox)]\n",
    "    roads_subset = roads[roads.geometry.intersects(bbox)]\n",
    "\n",
    "    # Debugging: Check if any buildings are retained in the subset\n",
    "    print(\"-- Buildings subset shape after intersects:\", buildings_subset.shape)\n",
    "    if len(buildings_subset) == 0:\n",
    "        print(\"-- Warning: No buildings found within the bounding box.\")\n",
    "        print(\"-- Original buildings dataset sample (after reprojection):\")\n",
    "        print(buildings.geometry.head())  # Inspect original geometries\n",
    "\n",
    "    print(f\"-- Subsampled {len(blocks_subset)} blocks, {len(buildings_subset)} buildings, and {len(roads_subset)} roads.\")\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "def load_data(block_path, osm_boundary_place, cache_dir=\"cache\", subsample_scale=1.0):\n",
    "    \"\"\"\n",
    "    Load blocks, buildings, and roads data from cache (if available), process, and optionally subsample using a scaled bounding box.\n",
    "    Uses only the 'height' column for building heights and outputs the percentage of valid values.\n",
    "    \"\"\"\n",
    "    # Corpus Christi center point from Google Maps\n",
    "    corpus_christi_lat = 27.783611  # Latitude\n",
    "    corpus_christi_lon = -97.414779  # Longitude\n",
    "\n",
    "    print(f\"Converting center coordinates ({corpus_christi_lat}, {corpus_christi_lon}) to UTM...\")\n",
    "    corpus_christi_center_utm = latlon_to_utm(corpus_christi_lat, corpus_christi_lon)  # Convert to UTM (EPSG:32614)\n",
    "    print(f\"Corpus Christi center in UTM (EPSG:32614): {corpus_christi_center_utm}\")\n",
    "\n",
    "    # Check cache for blocks\n",
    "    print(\"- Checking cache for blocks...\")\n",
    "    blocks = load_from_cache(\"blocks.pkl\", cache_dir)\n",
    "    if blocks is None:\n",
    "        print(\"-- Loading block data...\")\n",
    "        blocks = gpd.read_file(block_path).to_crs(epsg=32614)\n",
    "        blocks.loc[:, \"POP20\"] = pd.to_numeric(blocks[\"POP20\"], errors=\"coerce\")  # Explicitly use `.loc`\n",
    "        save_to_cache(blocks, \"blocks.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for buildings\n",
    "    print(\"- Checking cache for buildings...\")\n",
    "    buildings = load_from_cache(\"buildings.pkl\", cache_dir)\n",
    "    if buildings is None:\n",
    "        print(\"-- Fetching building data...\")\n",
    "        buildings = ox.features_from_place(\n",
    "            osm_boundary_place,\n",
    "            tags={\"building\": True, \"building:height\": True, \"building:levels\": True}\n",
    "        )\n",
    "\n",
    "        print(\"-- Raw buildings dataset columns:\")\n",
    "        print(buildings.columns)  # Print available columns for validation\n",
    "        print(\"-- Preview of raw building dataset:\")\n",
    "        print(buildings.head())  # Inspect raw data for possible errors\n",
    "\n",
    "        # Focus only on the 'height' column for building heights\n",
    "        print(\"-- Focusing on the 'height' column for building heights...\")\n",
    "        if \"height\" in buildings.columns:\n",
    "            print(\"-- 'height' column found. Attempting numerical conversion...\")\n",
    "            buildings.loc[:, \"height\"] = pd.to_numeric(buildings[\"height\"], errors=\"coerce\")\n",
    "\n",
    "            # Debugging: Calculate percentage of valid values (>0 and numeric)\n",
    "            total_height_values = len(buildings)\n",
    "            valid_height_values = buildings[\"height\"].dropna().gt(0).sum()\n",
    "            percentage_valid = (valid_height_values / total_height_values) * 100 if total_height_values > 0 else 0\n",
    "            print(f\"-- Valid height values: {valid_height_values} / Total: {total_height_values} ({percentage_valid:.2f}%)\")\n",
    "\n",
    "            print(\"-- Converted 'height' values to numeric. Example values:\")\n",
    "            print(buildings[\"height\"].head())\n",
    "        else:\n",
    "            print(\"-- ERROR: 'height' column is missing in the dataset! Setting height to None.\")\n",
    "            buildings.loc[:, \"height\"] = None\n",
    "\n",
    "        # Reproject buildings to CRS: EPSG:32614\n",
    "        print(\"-- Reprojecting buildings to EPSG:32614...\")\n",
    "        buildings = buildings.to_crs(epsg=32614)\n",
    "\n",
    "        # Save the buildings dataset to cache\n",
    "        save_to_cache(buildings, \"buildings.pkl\", cache_dir)\n",
    "\n",
    "    # Check cache for roads\n",
    "    print(\"- Checking cache for roads...\")\n",
    "    roads = load_from_cache(\"roads.pkl\", cache_dir)\n",
    "    if roads is None:\n",
    "        print(\"-- Fetching road data...\")\n",
    "        roads = ox.graph_to_gdfs(ox.graph_from_place(osm_boundary_place, network_type=\"drive\"), nodes=False)\n",
    "        roads = roads.to_crs(epsg=32614)\n",
    "        save_to_cache(roads, \"roads.pkl\", cache_dir)\n",
    "\n",
    "    # Subsample datasets using the bounding box around Corpus Christi center in UTM\n",
    "    print(\"- Subsampling datasets...\")\n",
    "    blocks_subset, buildings_subset, roads_subset = subsample_data(corpus_christi_center_utm, subsample_scale, blocks, buildings, roads)\n",
    "    print(\"- Data loading complete.\")\n",
    "    \n",
    "    # Calculate building setbacks (distance to nearest road)\n",
    "    print(\"Calculating building setbacks...\")\n",
    "    roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
    "    buildings_subset[\"setback_ft\"] = buildings_subset.geometry.apply(\n",
    "        lambda building_geom: building_geom.distance(roads_union) * 3.28084  # Convert meters to feet\n",
    "    )\n",
    "    return blocks_subset, buildings_subset, roads_subset\n",
    "\n",
    "\n",
    "# Define paths to block shapefile and Corpus Christi boundary\n",
    "one_drive_path = read_path_from_file(\"OneDrive.txt\")\n",
    "block_path = os.path.join(one_drive_path, \"Data\", \"tl_2023_48_tabblock20\", \"tl_2023_48_tabblock20.shp\")\n",
    "osm_boundary_place = \"Corpus Christi, Texas, USA\"\n",
    "\n",
    "# Load data with subsampling\n",
    "print(\"Starting data loading...\")\n",
    "blocks_subset, buildings_subset, roads_subset = load_data(block_path, osm_boundary_place, subsample_scale=1.5)\n",
    "\n",
    "# Inspect subsampled data\n",
    "print(\"-- Blocks subset:\")\n",
    "print(blocks_subset.head())\n",
    "print(\"-- Buildings subset:\")\n",
    "print(buildings_subset.head())\n",
    "print(\"-- Roads subset:\")\n",
    "print(roads_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc4d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in buildings dataset:\n",
      "Index(['geometry', 'addr:state', 'building', 'ele', 'gnis:feature_id', 'name',\n",
      "       'source', 'addr:city', 'addr:housename', 'addr:housenumber',\n",
      "       ...\n",
      "       'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male',\n",
      "       'portable', 'toilets:handwashing', 'capacity', 'size', 'setback_ft'],\n",
      "      dtype='object', length=199)\n",
      "Calculating block metrics...\n",
      "Calculating building area coverage (sq-mile)...\n",
      "Processing building heights...\n",
      "Associating buildings with blocks via spatial join...\n",
      "Aggregating building metrics per block...\n",
      "Calculating population density and other metrics for blocks...\n",
      "Calculating additional block metrics: Building count per square mile and building area percentage...\n",
      "Merging building metrics into block dataset...\n",
      "Filling missing values for block metrics...\n",
      "Block metrics calculated successfully.\n",
      "Block metrics successfully calculated:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    002002      2008  483550020022008   \n",
      "2        48        355    001300      4003  483550013004003   \n",
      "3        48        355    001300      4004  483550013004004   \n",
      "4        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ...  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...   \n",
      "1  1000000US483550020022008  Block2008   G5040    U  20287  ...   \n",
      "2  1000000US483550013004003  Block4003   G5040    U  20287  ...   \n",
      "3  1000000US483550013004004  Block4004   G5040    U  20287  ...   \n",
      "4  1000000US483550013004012  Block4012   G5040    U  20287  ...   \n",
      "\n",
      "                                            geometry  BLOCK_ID   area_sm  \\\n",
      "0  POLYGON ((656383.497 3069802.332, 656388.176 3...    211079  0.013887   \n",
      "1  POLYGON ((656781.143 3070143.675, 656831.652 3...    211080  0.013877   \n",
      "2  POLYGON ((657076.274 3072786.758, 657080.514 3...    211081  0.009445   \n",
      "3  POLYGON ((657029.407 3072699.161, 657058.753 3...    211082  0.006603   \n",
      "4  POLYGON ((657185.832 3072577.964, 657270.53 30...    211083  0.009849   \n",
      "\n",
      "       pop_den   avg_hght  avg_sback  bld_area bld_cnt     bld_ctsm    bld_prc  \n",
      "0  8929.477778  33.216162  54.357708  0.002815    49.0  3528.583961  20.269451  \n",
      "1  7998.964124  32.435641  44.567143  0.002327    47.0  3386.948773  16.770413  \n",
      "2  8152.607692  32.740521  55.002728  0.001837    29.0  3070.462637  19.445682  \n",
      "3     0.000000  32.414699  33.839178  0.002782     1.0   151.448236  42.140090  \n",
      "4  8935.348238  32.753367  61.108800  0.002250    31.0  3147.679493  22.849059  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_block_metrics(blocks_subset: gpd.GeoDataFrame, buildings_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate block-level metrics\n",
    "    \"\"\"\n",
    "    print(\"Available columns in buildings dataset:\")\n",
    "    print(buildings_subset.columns)\n",
    "\n",
    "    # Check if required columns exist in the buildings dataset\n",
    "    required_columns = [\"geometry\", \"height\"]\n",
    "    missing_columns = [col for col in required_columns if col not in buildings_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing required columns in buildings dataset: {missing_columns}\")\n",
    "        print(\"Preview of buildings dataset:\")\n",
    "        print(buildings_subset.head())\n",
    "        raise KeyError(f\"Required columns {missing_columns} are not found in buildings dataset.\")\n",
    "\n",
    "    print(\"Calculating block metrics...\")\n",
    "    blocks_subset[\"BLOCK_ID\"] = blocks_subset.index  # Assign unique ID to each block\n",
    "    buildings_subset[\"build_idx\"] = buildings_subset.index  # Assign unique ID to each building\n",
    "\n",
    "    # Calculate building area (square meters -> square miles)\n",
    "    print(\"Calculating building area coverage (sq-mile)...\")\n",
    "    buildings_subset[\"build_area_sm\"] = buildings_subset.geometry.area * 0.00000038610215855  # Convert area to sq-mile\n",
    "\n",
    "    # Handle building height (convert meters to feet and filter valid heights)\n",
    "    print(\"Processing building heights...\")\n",
    "    buildings_subset[\"height_m\"] = pd.to_numeric(buildings_subset[\"height\"], errors=\"coerce\")\n",
    "    buildings_subset[\"height_ft\"] = buildings_subset[\"height_m\"] * 3.28084  # Convert height to feet\n",
    "    buildings_subset = buildings_subset[buildings_subset[\"height_ft\"] > 0]  # Filter buildings with positive heights\n",
    "        \n",
    "    '''\n",
    "    # Calculate building setbacks (distance to nearest road)\n",
    "    print(\"Calculating building setbacks...\")\n",
    "    roads_union = roads_subset.geometry.unary_union  # Combine all road geometries into one\n",
    "    buildings_subset[\"setback_ft\"] = buildings_subset.geometry.apply(\n",
    "        lambda building_geom: building_geom.distance(roads_union) * 3.28084  # Convert meters to feet\n",
    "    )\n",
    "    '''\n",
    "    # Spatial join to associate buildings with blocks\n",
    "    print(\"Associating buildings with blocks via spatial join...\")\n",
    "    buildings_with_blocks = gpd.sjoin(buildings_subset, blocks_subset, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    # Aggregate metrics per block\n",
    "    print(\"Aggregating building metrics per block...\")\n",
    "    building_metrics = buildings_with_blocks.groupby(\"BLOCK_ID\").agg(\n",
    "        avg_hght=(\"height_ft\", \"mean\"),  # Renamed: Average building height\n",
    "        avg_sback=(\"setback_ft\", \"mean\"),  # Renamed: Average setback distance\n",
    "        bld_area=(\"build_area_sm\", \"sum\"),  # Renamed: Sum building areas\n",
    "        bld_cnt=(\"build_idx\", \"count\"),  # Renamed: Count number of buildings\n",
    "    )\n",
    "\n",
    "    # Calculate population density for blocks\n",
    "    print(\"Calculating population density and other metrics for blocks...\")\n",
    "    blocks_subset[\"area_sm\"] = blocks_subset.geometry.area * 0.00000038610215855  # Convert block area to sq-mile\n",
    "    blocks_subset[\"pop_den\"] = blocks_subset[\"POP20\"] / blocks_subset[\"area_sm\"]  # Renamed: Population density per sq-mile\n",
    "\n",
    "    # Derive additional metrics\n",
    "    print(\"Calculating additional block metrics: Building count per square mile and building area percentage...\")\n",
    "    building_metrics[\"bld_ctsm\"] = building_metrics[\"bld_cnt\"] / blocks_subset[\"area_sm\"]  # Renamed: Building count per sq-mile\n",
    "    building_metrics[\"bld_prc\"] = (building_metrics[\"bld_area\"] / blocks_subset[\"area_sm\"]) * 100  # Renamed: Building area as percentage of block area\n",
    "\n",
    "    # Merge metrics into block dataset\n",
    "    print(\"Merging building metrics into block dataset...\")\n",
    "    blocks_subset = blocks_subset.merge(building_metrics, on=\"BLOCK_ID\", how=\"left\")\n",
    "\n",
    "    # Fill missing values explicitly for columns\n",
    "    print(\"Filling missing values for block metrics...\")\n",
    "    blocks_subset[\"avg_hght\"] = blocks_subset[\"avg_hght\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"avg_sback\"] = blocks_subset[\"avg_sback\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_area\"] = blocks_subset[\"bld_area\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_ctsm\"] = blocks_subset[\"bld_ctsm\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_prc\"] = blocks_subset[\"bld_prc\"].fillna(0)  # Replace NaN with 0\n",
    "    blocks_subset[\"bld_cnt\"] = blocks_subset[\"bld_cnt\"].fillna(0)  # Replace NaN with 0\n",
    "\n",
    "    print(\"Block metrics calculated successfully.\")\n",
    "    return blocks_subset\n",
    "\n",
    "# Calculate block-level metrics\n",
    "blocks_processed = calculate_block_metrics(blocks_subset, buildings_subset, roads_subset)\n",
    "print(\"Block metrics successfully calculated:\")\n",
    "print(blocks_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48683f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculating road metrics with buffer size (for spatial analysis): 50 feet...\n",
      "-- Inspecting blocks structure before processing:\n",
      "Index(['STATEFP20', 'COUNTYFP20', 'TRACTCE20', 'BLOCKCE20', 'GEOID20',\n",
      "       'GEOIDFQ20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20',\n",
      "       'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20', 'HOUSING20', 'POP20',\n",
      "       'geometry', 'BLOCK_ID', 'area_sm', 'pop_den', 'avg_hght', 'avg_sback',\n",
      "       'bld_area', 'bld_cnt', 'bld_ctsm', 'bld_prc'],\n",
      "      dtype='object')\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    002002      2008  483550020022008   \n",
      "2        48        355    001300      4003  483550013004003   \n",
      "3        48        355    001300      4004  483550013004004   \n",
      "4        48        355    001300      4012  483550013004012   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ...  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...   \n",
      "1  1000000US483550020022008  Block2008   G5040    U  20287  ...   \n",
      "2  1000000US483550013004003  Block4003   G5040    U  20287  ...   \n",
      "3  1000000US483550013004004  Block4004   G5040    U  20287  ...   \n",
      "4  1000000US483550013004012  Block4012   G5040    U  20287  ...   \n",
      "\n",
      "                                            geometry  BLOCK_ID   area_sm  \\\n",
      "0  POLYGON ((656383.497 3069802.332, 656388.176 3...    211079  0.013887   \n",
      "1  POLYGON ((656781.143 3070143.675, 656831.652 3...    211080  0.013877   \n",
      "2  POLYGON ((657076.274 3072786.758, 657080.514 3...    211081  0.009445   \n",
      "3  POLYGON ((657029.407 3072699.161, 657058.753 3...    211082  0.006603   \n",
      "4  POLYGON ((657185.832 3072577.964, 657270.53 30...    211083  0.009849   \n",
      "\n",
      "       pop_den   avg_hght  avg_sback  bld_area bld_cnt     bld_ctsm    bld_prc  \n",
      "0  8929.477778  33.216162  54.357708  0.002815    49.0  3528.583961  20.269451  \n",
      "1  7998.964124  32.435641  44.567143  0.002327    47.0  3386.948773  16.770413  \n",
      "2  8152.607692  32.740521  55.002728  0.001837    29.0  3070.462637  19.445682  \n",
      "3     0.000000  32.414699  33.839178  0.002782     1.0   151.448236  42.140090  \n",
      "4  8935.348238  32.753367  61.108800  0.002250    31.0  3147.679493  22.849059  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "- Creating road buffers...\n",
      "-- Road buffers structure after buffering:\n",
      "Index(['osmid', 'highway', 'lanes', 'name', 'oneway', 'ref', 'reversed',\n",
      "       'length', 'maxspeed', 'geometry', 'bridge', 'junction', 'width',\n",
      "       'access', 'road_id', 'buffer_area'],\n",
      "      dtype='object')\n",
      "- Performing spatial join of blocks with road buffers...\n",
      "-- Intersections structure after spatial join:\n",
      "  STATEFP20 COUNTYFP20 TRACTCE20 BLOCKCE20          GEOID20  \\\n",
      "0        48        355    001904      2001  483550019042001   \n",
      "1        48        355    001904      1018  483550019041018   \n",
      "2        48        355    001904      2000  483550019042000   \n",
      "3        48        355    001904      2005  483550019042005   \n",
      "4        48        355    001904      2001  483550019042001   \n",
      "\n",
      "                  GEOIDFQ20     NAME20 MTFCC20 UR20 UACE20  ... reversed  \\\n",
      "0  1000000US483550019042001  Block2001   G5040    U  20287  ...    False   \n",
      "1  1000000US483550019041018  Block1018   G5040    U  20287  ...    False   \n",
      "2  1000000US483550019042000  Block2000   G5040    U  20287  ...    False   \n",
      "3  1000000US483550019042005  Block2005   G5040    U  20287  ...    False   \n",
      "4  1000000US483550019042001  Block2001   G5040    U  20287  ...    False   \n",
      "\n",
      "      length  maxspeed bridge junction  width  access  road_id  buffer_area  \\\n",
      "0  94.165957       NaN    NaN      NaN    NaN     NaN     4390  3592.182272   \n",
      "1  94.165957       NaN    NaN      NaN    NaN     NaN     4390  3592.182272   \n",
      "2  94.165957       NaN    NaN      NaN    NaN     NaN     4390  3592.182272   \n",
      "3  94.165957       NaN    NaN      NaN    NaN     NaN     4390  3592.182272   \n",
      "4  51.078478       NaN    NaN      NaN    NaN     NaN     5328  2283.689421   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((656388.176 3069806.492, 656388.475 3...  \n",
      "1  POLYGON ((656331.076 3069876.455, 656322.38 30...  \n",
      "2  POLYGON ((656338.743 3069888.691, 656339.119 3...  \n",
      "3  POLYGON ((656322.38 3069885.651, 656331.076 30...  \n",
      "4  POLYGON ((656577.893 3070140.495, 656585.777 3...  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "-- Adding road buffer areas for overlap proportion calculation...\n",
      "-- Intersection overlap proportions sample:\n",
      "   BLOCK_ID  road_id  overlap_area  road_buffer_area  overlap_proportion\n",
      "0    211079     4390    193.005975       3592.182272            0.053729\n",
      "1    475983     4390    142.839906       3592.182272            0.039764\n",
      "2    591650     4390   1446.078383       3592.182272            0.402563\n",
      "3    633619     4390   1810.258008       3592.182272            0.503944\n",
      "4    211079     5328   1069.411001       2283.689421            0.468282\n",
      "- Creating overlap dictionary for roads...\n",
      "-- Adding block IDs and overlap percentages columns...\n",
      "- Loading evacuation routes shapefile...\n",
      "-- Flagging roads based on evacuation route overlap...\n",
      "- Weighting metrics for roads...\n",
      "- Aggregating weighted metrics across roads...\n",
      "-- Aggregated road metrics structure:\n",
      "       agg_pop  agg_area     agg_ctsm  agg_bldprc   agg_hght  agg_sback\n",
      "0   179.386868  0.000747   615.723759   10.501223  41.096999  41.270940\n",
      "1   212.881493  0.000385   337.863022    6.376996  26.131773  33.134895\n",
      "2   693.493616  0.000575   817.996603   14.102548  40.959569  43.455655\n",
      "3    58.904260  0.001291   611.881951   15.083942  45.226433  53.047322\n",
      "4  5543.597413  0.002104  2756.273755   23.302067  34.273597  66.820678\n",
      "- Merging metrics into roads dataset...\n",
      "-- Final roads structure:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ... road_id                                          block_ids  \\\n",
      "0  ...       0   [475853, 493225, 609010, 609016, 633726, 642276]   \n",
      "1  ...       1  [475853, 493225, 591609, 604747, 604748, 60901...   \n",
      "2  ...       2   [367249, 475853, 493225, 609015, 609016, 642276]   \n",
      "3  ...       3           [475853, 493225, 609016, 642250, 642276]   \n",
      "4  ...       4                   [316593, 642401, 642402, 642403]   \n",
      "\n",
      "                                       overlap_percs evac_flag      agg_pop  \\\n",
      "0  [0.410884402784681, 0.04926583611313816, 0.047...         0   179.386868   \n",
      "1  [0.021641946929578066, 0.16740453513625067, 0....         0   212.881493   \n",
      "2  [0.03351930391955583, 0.04936305717900521, 0.0...         0   693.493616   \n",
      "3  [0.4247128338513536, 0.48637656963460946, 0.03...         0    58.904260   \n",
      "4  [0.42981149729602536, 0.053945707680300964, 0....         0  5543.597413   \n",
      "\n",
      "   agg_area     agg_ctsm  agg_bldprc   agg_hght  agg_sback  \n",
      "0  0.000747   615.723759   10.501223  41.096999  41.270940  \n",
      "1  0.000385   337.863022    6.376996  26.131773  33.134895  \n",
      "2  0.000575   817.996603   14.102548  40.959569  43.455655  \n",
      "3  0.001291   611.881951   15.083942  45.226433  53.047322  \n",
      "4  0.002104  2756.273755   23.302067  34.273597  66.820678  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "-- Final processed roads sample:\n",
      "       osmid      highway lanes                  name  oneway  ref reversed  \\\n",
      "0   21156845  residential   NaN      Winnebago Street   False  NaN    False   \n",
      "1   21156845  residential   NaN      Winnebago Street   False  NaN     True   \n",
      "2   21160211    secondary     2     North Port Avenue   False  NaN    False   \n",
      "3   21160211    secondary     2     North Port Avenue   False  NaN     True   \n",
      "4  831149706    secondary   NaN  South Alameda Street   False  NaN    False   \n",
      "\n",
      "       length maxspeed                                           geometry  \\\n",
      "0  111.207782      NaN  LINESTRING (656404.034 3076197.804, 656308.077...   \n",
      "1  239.351195      NaN  LINESTRING (656404.034 3076197.804, 656612.629...   \n",
      "2   91.759789   40 mph  LINESTRING (656404.034 3076197.804, 656449.517...   \n",
      "3  181.553317   40 mph  LINESTRING (656404.034 3076197.804, 656315.15 ...   \n",
      "4   98.207032      NaN  LINESTRING (658427.494 3071682.406, 658480.689...   \n",
      "\n",
      "   ... road_id                                          block_ids  \\\n",
      "0  ...       0   [475853, 493225, 609010, 609016, 633726, 642276]   \n",
      "1  ...       1  [475853, 493225, 591609, 604747, 604748, 60901...   \n",
      "2  ...       2   [367249, 475853, 493225, 609015, 609016, 642276]   \n",
      "3  ...       3           [475853, 493225, 609016, 642250, 642276]   \n",
      "4  ...       4                   [316593, 642401, 642402, 642403]   \n",
      "\n",
      "                                       overlap_percs evac_flag      agg_pop  \\\n",
      "0  [0.410884402784681, 0.04926583611313816, 0.047...         0   179.386868   \n",
      "1  [0.021641946929578066, 0.16740453513625067, 0....         0   212.881493   \n",
      "2  [0.03351930391955583, 0.04936305717900521, 0.0...         0   693.493616   \n",
      "3  [0.4247128338513536, 0.48637656963460946, 0.03...         0    58.904260   \n",
      "4  [0.42981149729602536, 0.053945707680300964, 0....         0  5543.597413   \n",
      "\n",
      "   agg_area     agg_ctsm  agg_bldprc   agg_hght  agg_sback  \n",
      "0  0.000747   615.723759   10.501223  41.096999  41.270940  \n",
      "1  0.000385   337.863022    6.376996  26.131773  33.134895  \n",
      "2  0.000575   817.996603   14.102548  40.959569  43.455655  \n",
      "3  0.001291   611.881951   15.083942  45.226433  53.047322  \n",
      "4  0.002104  2756.273755   23.302067  34.273597  66.820678  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_road_metrics(blocks_subset: gpd.GeoDataFrame, roads_subset: gpd.GeoDataFrame, buffer_size_feet=50) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate road-level metrics based on spatial overlap with blocks (weighted aggregation).\n",
    "    Includes Block IDs and Overlap Percentages as columns, and flags roads intersecting evacuation routes using buffered geometry.\n",
    "    \"\"\"\n",
    "    print(f\"- Calculating road metrics with buffer size (for spatial analysis): {buffer_size_feet} feet...\")\n",
    "    \n",
    "    buffer_size_meters = buffer_size_feet * 0.3048  # Convert buffer size to meters\n",
    "\n",
    "    # Blocks subset should maintain the original BLOCK_ID\n",
    "    blocks_subset = blocks_subset.copy()\n",
    "    print(\"-- Inspecting blocks structure before processing:\")\n",
    "    print(blocks_subset.columns)\n",
    "    print(blocks_subset.head())  # Output a sample of blocks_subset\n",
    "\n",
    "    roads_subset = roads_subset.copy().reset_index(drop=True)\n",
    "    roads_subset[\"road_id\"] = roads_subset.index\n",
    "\n",
    "    # Check if required block-level metrics exist\n",
    "    required_columns = [\"area_sm\", \"pop_den\", \"bld_area\", \"bld_ctsm\", \"avg_hght\", \"avg_sback\"]\n",
    "    missing_columns = [col for col in required_columns if col not in blocks_subset.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: The following required block-level metrics are missing: {missing_columns}\")\n",
    "        raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Create buffers for roads\n",
    "    print(\"- Creating road buffers...\")\n",
    "    road_buffers = roads_subset.copy()\n",
    "    road_buffers[\"geometry\"] = roads_subset.geometry.buffer(buffer_size_meters)\n",
    "    road_buffers[\"road_id\"] = roads_subset[\"road_id\"]\n",
    "    road_buffers[\"buffer_area\"] = road_buffers.geometry.area  # Area of the road buffer, for overlap proportion calculation\n",
    "    print(\"-- Road buffers structure after buffering:\")\n",
    "    print(road_buffers.columns)\n",
    "\n",
    "    # Perform spatial join of blocks with road buffers\n",
    "    print(\"- Performing spatial join of blocks with road buffers...\")\n",
    "    intersections = gpd.overlay(blocks_subset, road_buffers, how=\"intersection\")\n",
    "    print(\"-- Intersections structure after spatial join:\")\n",
    "    print(intersections.head())\n",
    "\n",
    "    # Validate road buffer areas and block intersections\n",
    "    print(\"-- Adding road buffer areas for overlap proportion calculation...\")\n",
    "    intersections[\"road_buffer_area\"] = intersections[\"road_id\"].map(road_buffers.set_index(\"road_id\")[\"buffer_area\"])\n",
    "    intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "    intersections[\"overlap_proportion\"] = intersections[\"overlap_area\"] / intersections[\"road_buffer_area\"]\n",
    "    print(\"-- Intersection overlap proportions sample:\")\n",
    "    print(intersections[[\"BLOCK_ID\", \"road_id\", \"overlap_area\", \"road_buffer_area\", \"overlap_proportion\"]].head())\n",
    "\n",
    "    # Create overlap dictionary for roads\n",
    "    print(\"- Creating overlap dictionary for roads...\")\n",
    "    road_block_overlap = (\n",
    "        intersections.groupby(\"road_id\")\n",
    "        .apply(lambda rows: dict(zip(rows[\"BLOCK_ID\"], rows[\"overlap_proportion\"])))\n",
    "        .to_dict()\n",
    "    )\n",
    "    print(\"-- Adding block IDs and overlap percentages columns...\")\n",
    "    # Add block IDs and overlap percentages columns\n",
    "    roads_subset[\"block_ids\"] = [\n",
    "        list(overlap_dict.keys()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "    roads_subset[\"overlap_percs\"] = [\n",
    "        list(overlap_dict.values()) for overlap_dict in road_block_overlap.values()\n",
    "    ]\n",
    "\n",
    "    # Load Evacuation Routes\n",
    "    print(\"- Loading evacuation routes shapefile...\")\n",
    "    evac_path = os.path.join(one_drive_path, \"Data\", \"TxDOT Evacuation Routes AGO.shp\")\n",
    "    evacuation_routes = gpd.read_file(evac_path)\n",
    "\n",
    "    # Ensure both datasets use the same CRS for spatial operations\n",
    "    if evacuation_routes.crs != road_buffers.crs:\n",
    "        evacuation_routes = evacuation_routes.to_crs(road_buffers.crs)\n",
    "\n",
    "    print(\"-- Flagging roads based on evacuation route overlap...\")\n",
    "    def flag_evacuation_routes(road_buffer_geom):\n",
    "        \"\"\"\n",
    "        Flag roads based on overlap with evacuation routes.\n",
    "        0 = No overlap\n",
    "        1 = Major Evacuation Routes\n",
    "        2 = Potential Contraflow\n",
    "        3 = Potential EvacuLanes\n",
    "        \"\"\"\n",
    "        for _, evac_row in evacuation_routes.iterrows():\n",
    "            if road_buffer_geom.intersects(evac_row.geometry):\n",
    "                route_type = evac_row[\"ROUTE_TYPE\"]\n",
    "                if route_type == \"Major Evacuation Routes\":\n",
    "                    return 1\n",
    "                elif route_type == \"Potential Contraflow\":\n",
    "                    return 2\n",
    "                elif route_type == \"Potential EvacuLanes\":\n",
    "                    return 3\n",
    "        return 0  # No overlap\n",
    "\n",
    "    # Use the buffered geometry to calculate evacuation flags\n",
    "    roads_subset[\"evac_flag\"] = road_buffers.geometry.apply(flag_evacuation_routes)\n",
    "\n",
    "    # Calculate weighted metrics for roads\n",
    "    print(\"- Weighting metrics for roads...\")\n",
    "    def compute_weighted_metrics(road_id, overlap_dict):\n",
    "        \"\"\"\n",
    "        Compute weighted metrics for a road using normalized weights for `agg_hght` and `agg_sback`.\n",
    "        \"\"\"\n",
    "        blocks = blocks_subset.set_index(\"BLOCK_ID\").loc[overlap_dict.keys()]\n",
    "        raw_weights = pd.Series({BLOCK_ID: overlap_dict[BLOCK_ID] for BLOCK_ID in blocks.index})\n",
    "        total_weight = raw_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            normalized_weights = raw_weights / total_weight\n",
    "        else:\n",
    "            return pd.Series({\n",
    "                \"agg_pop\": 0,\n",
    "                \"agg_area\": 0,\n",
    "                \"agg_ctsm\": 0,\n",
    "                \"agg_bldprc\": 0,\n",
    "                \"agg_hght\": 0,\n",
    "                \"agg_sback\": 0,\n",
    "            })\n",
    "        valid_blocks_for_hght_sback = blocks[(blocks[\"avg_hght\"] > 0) & (blocks[\"avg_sback\"] > 0)]\n",
    "        aggregated_metrics = {\n",
    "            \"agg_pop\": (blocks[\"pop_den\"] * normalized_weights).sum(),\n",
    "            \"agg_area\": (blocks[\"bld_area\"] * normalized_weights).sum(),\n",
    "            \"agg_ctsm\": (blocks[\"bld_ctsm\"] * normalized_weights).sum(),\n",
    "            \"agg_bldprc\": (blocks[\"bld_prc\"] * normalized_weights).sum(),\n",
    "            \"agg_hght\": (valid_blocks_for_hght_sback[\"avg_hght\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "            \"agg_sback\": (valid_blocks_for_hght_sback[\"avg_sback\"] * normalized_weights.loc[valid_blocks_for_hght_sback.index]).sum()\n",
    "            if not valid_blocks_for_hght_sback.empty else 0,\n",
    "        }\n",
    "        return pd.Series(aggregated_metrics)\n",
    "\n",
    "    print(\"- Aggregating weighted metrics across roads...\")\n",
    "    road_metrics_df = pd.DataFrame([\n",
    "        compute_weighted_metrics(road_id, overlap_dict)\n",
    "        for road_id, overlap_dict in road_block_overlap.items()\n",
    "    ], index=road_block_overlap.keys())\n",
    "    print(\"-- Aggregated road metrics structure:\")\n",
    "    print(road_metrics_df.head())\n",
    "\n",
    "    # Merge aggregated metrics into roads dataset\n",
    "    print(\"- Merging metrics into roads dataset...\")\n",
    "    roads_subset = roads_subset.merge(road_metrics_df, left_on=\"road_id\", right_index=True, how=\"left\")\n",
    "    print(\"-- Final roads structure:\")\n",
    "    print(roads_subset.head())\n",
    "\n",
    "    return roads_subset\n",
    "\n",
    "# Execute road metrics calculation\n",
    "roads_processed = calculate_road_metrics(blocks_processed, roads_subset, buffer_size_feet=50)\n",
    "print(\"-- Final processed roads sample:\")\n",
    "print(roads_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving subsets to shapefiles...\n",
      "- Saving buildings subset...\n",
      "-- Full column names in buildings_subset:\n",
      "  geometry\n",
      "  addr:state\n",
      "  building\n",
      "  ele\n",
      "  gnis:feature_id\n",
      "  name\n",
      "  source\n",
      "  addr:city\n",
      "  addr:housename\n",
      "  addr:housenumber\n",
      "  addr:postcode\n",
      "  addr:street\n",
      "  amenity\n",
      "  brand\n",
      "  brand:wikidata\n",
      "  cuisine\n",
      "  healthcare\n",
      "  healthcare:counselling\n",
      "  height\n",
      "  phone\n",
      "  website\n",
      "  generator:method\n",
      "  generator:output:electricity\n",
      "  generator:source\n",
      "  generator:type\n",
      "  layer\n",
      "  power\n",
      "  shop\n",
      "  opening_hours\n",
      "  ref\n",
      "  wholesale\n",
      "  addr:unit\n",
      "  official_name\n",
      "  takeaway\n",
      "  note\n",
      "  email\n",
      "  museum\n",
      "  tourism\n",
      "  addr:country\n",
      "  alt_name\n",
      "  contact:email\n",
      "  contact:facebook\n",
      "  contact:twitter\n",
      "  man_made\n",
      "  content\n",
      "  office\n",
      "  start_date\n",
      "  building:levels\n",
      "  leisure\n",
      "  sport\n",
      "  wikidata\n",
      "  check_date\n",
      "  fee\n",
      "  fixme\n",
      "  heritage\n",
      "  heritage:operator\n",
      "  historic\n",
      "  ref:nrhp\n",
      "  ship:type\n",
      "  wikipedia\n",
      "  building:part\n",
      "  disused:shop\n",
      "  type\n",
      "  aeroway\n",
      "  operator\n",
      "  short_name\n",
      "  parking\n",
      "  old_name\n",
      "  branch\n",
      "  air_conditioning\n",
      "  drive_through\n",
      "  opening_hours:drive_through\n",
      "  indoor_seating\n",
      "  outdoor_seating\n",
      "  delivery\n",
      "  payment:NFC_mobile_payments\n",
      "  smoking\n",
      "  access\n",
      "  brand:website\n",
      "  brand:wikipedia\n",
      "  denomination\n",
      "  religion\n",
      "  payment:cash\n",
      "  payment:contactless\n",
      "  payment:credit_cards\n",
      "  payment:debit_cards\n",
      "  payment:nfc\n",
      "  website:menu\n",
      "  wheelchair\n",
      "  operator:wikidata\n",
      "  drive_in\n",
      "  compressed_air\n",
      "  toilets\n",
      "  operator:short\n",
      "  operator:type\n",
      "  operator:website\n",
      "  operator:wikipedia\n",
      "  fuel:diesel\n",
      "  fuel:gasoline\n",
      "  self_service\n",
      "  bar\n",
      "  fax\n",
      "  internet_access\n",
      "  internet_access:fee\n",
      "  reservation\n",
      "  rooms\n",
      "  landuse\n",
      "  ref:walmart\n",
      "  atm\n",
      "  shelter_type\n",
      "  roof:shape\n",
      "  roof:levels\n",
      "  diet:chicken\n",
      "  diet:dairy\n",
      "  diet:meat\n",
      "  image\n",
      "  contact:instagram\n",
      "  description\n",
      "  second_hand\n",
      "  screen\n",
      "  bin\n",
      "  toilets:disposal\n",
      "  unisex\n",
      "  social_facility\n",
      "  social_facility:for\n",
      "  beauty\n",
      "  healthcare:speciality\n",
      "  clothes\n",
      "  dispensing\n",
      "  was:shop\n",
      "  check_date:opening_hours\n",
      "  service:vehicle:inspection\n",
      "  service:vehicle:oil_change\n",
      "  contact:foursquare\n",
      "  drinking_water\n",
      "  opening_hours:covid19\n",
      "  highchair\n",
      "  abandoned\n",
      "  automated\n",
      "  payment:coins\n",
      "  telecom\n",
      "  craft\n",
      "  after_school\n",
      "  isced:level\n",
      "  nursery\n",
      "  preschool\n",
      "  abandoned:building\n",
      "  microbrewery\n",
      "  contact:website\n",
      "  not:brand:wikidata\n",
      "  teaching\n",
      "  swimming_pool\n",
      "  bus\n",
      "  public_transport\n",
      "  contact:phone\n",
      "  service:vehicle:car_repair\n",
      "  building:min_level\n",
      "  max_level\n",
      "  min_level\n",
      "  theatre\n",
      "  dance:style\n",
      "  dance:teaching\n",
      "  disused\n",
      "  architect\n",
      "  building:use\n",
      "  diocese\n",
      "  building_1\n",
      "  urgent_care\n",
      "  emergency\n",
      "  lit\n",
      "  bridge\n",
      "  level\n",
      "  community_centre:for\n",
      "  construction_equipment:rental\n",
      "  tool:rental\n",
      "  condo\n",
      "  service:vehicle:car_parts\n",
      "  service:vehicle:new_car_sales\n",
      "  service:vehicle:used_car_sales\n",
      "  bench\n",
      "  government\n",
      "  payment:american_express\n",
      "  payment:mastercard\n",
      "  payment:visa\n",
      "  animal_shelter\n",
      "  animal_shelter:adoption\n",
      "  animal_shelter:release\n",
      "  pets\n",
      "  check_date:opening_hours:drive_through\n",
      "  fuel:octane_87\n",
      "  fuel:octane_89\n",
      "  fuel:octane_93\n",
      "  female\n",
      "  male\n",
      "  portable\n",
      "  toilets:handwashing\n",
      "  capacity\n",
      "  size\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Dropping extraneous columns from buildings_subset: ['addr:state', 'building', 'ele', 'gnis:feature_id', 'name', 'source', 'addr:city', 'addr:housename', 'addr:housenumber', 'addr:postcode', 'addr:street', 'amenity', 'brand', 'brand:wikidata', 'cuisine', 'healthcare', 'healthcare:counselling', 'phone', 'website', 'generator:method', 'generator:output:electricity', 'generator:source', 'generator:type', 'layer', 'power', 'shop', 'opening_hours', 'ref', 'wholesale', 'addr:unit', 'official_name', 'takeaway', 'note', 'email', 'museum', 'tourism', 'addr:country', 'alt_name', 'contact:email', 'contact:facebook', 'contact:twitter', 'man_made', 'content', 'office', 'start_date', 'building:levels', 'leisure', 'sport', 'wikidata', 'check_date', 'fee', 'fixme', 'heritage', 'heritage:operator', 'historic', 'ref:nrhp', 'ship:type', 'wikipedia', 'building:part', 'disused:shop', 'type', 'aeroway', 'operator', 'short_name', 'parking', 'old_name', 'branch', 'air_conditioning', 'drive_through', 'opening_hours:drive_through', 'indoor_seating', 'outdoor_seating', 'delivery', 'payment:NFC_mobile_payments', 'smoking', 'access', 'brand:website', 'brand:wikipedia', 'denomination', 'religion', 'payment:cash', 'payment:contactless', 'payment:credit_cards', 'payment:debit_cards', 'payment:nfc', 'website:menu', 'wheelchair', 'operator:wikidata', 'drive_in', 'compressed_air', 'toilets', 'operator:short', 'operator:type', 'operator:website', 'operator:wikipedia', 'fuel:diesel', 'fuel:gasoline', 'self_service', 'bar', 'fax', 'internet_access', 'internet_access:fee', 'reservation', 'rooms', 'landuse', 'ref:walmart', 'atm', 'shelter_type', 'roof:shape', 'roof:levels', 'diet:chicken', 'diet:dairy', 'diet:meat', 'image', 'contact:instagram', 'description', 'second_hand', 'screen', 'bin', 'toilets:disposal', 'unisex', 'social_facility', 'social_facility:for', 'beauty', 'healthcare:speciality', 'clothes', 'dispensing', 'was:shop', 'check_date:opening_hours', 'service:vehicle:inspection', 'service:vehicle:oil_change', 'contact:foursquare', 'drinking_water', 'opening_hours:covid19', 'highchair', 'abandoned', 'automated', 'payment:coins', 'telecom', 'craft', 'after_school', 'isced:level', 'nursery', 'preschool', 'abandoned:building', 'microbrewery', 'contact:website', 'not:brand:wikidata', 'teaching', 'swimming_pool', 'bus', 'public_transport', 'contact:phone', 'service:vehicle:car_repair', 'building:min_level', 'max_level', 'min_level', 'theatre', 'dance:style', 'dance:teaching', 'disused', 'architect', 'building:use', 'diocese', 'building_1', 'urgent_care', 'emergency', 'lit', 'bridge', 'level', 'community_centre:for', 'construction_equipment:rental', 'tool:rental', 'condo', 'service:vehicle:car_parts', 'service:vehicle:new_car_sales', 'service:vehicle:used_car_sales', 'bench', 'government', 'payment:american_express', 'payment:mastercard', 'payment:visa', 'animal_shelter', 'animal_shelter:adoption', 'animal_shelter:release', 'pets', 'check_date:opening_hours:drive_through', 'fuel:octane_87', 'fuel:octane_89', 'fuel:octane_93', 'female', 'male', 'portable', 'toilets:handwashing', 'capacity', 'size']\n",
      "-- Full column names in buildings_subset after dropping:\n",
      "  geometry\n",
      "  height\n",
      "  setback_ft\n",
      "  build_idx\n",
      "  build_area_sm\n",
      "  height_m\n",
      "  height_ft\n",
      "-- Validating geometries in buildings_subset...\n",
      "ERROR: Invalid geometry type detected in buildings_subset. Expected Polygon or MultiPolygon.\n",
      "-- Invalid geometries: 7\n",
      "-- Attempting to convert invalid geometries to valid Polygon/MultiPolygon...\n",
      "-- After conversion: 31386 valid polygons remaining.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_30644\\3821764773.py:113: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saving blocks processed...\n",
      "-- Full column names in blocks_processed:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  GEOIDFQ20\n",
      "  NAME20\n",
      "  MTFCC20\n",
      "  UR20\n",
      "  UACE20\n",
      "  FUNCSTAT20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  INTPTLAT20\n",
      "  INTPTLON20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "-- Dropping extraneous columns from blocks_processed: ['GEOIDFQ20', 'NAME20', 'MTFCC20', 'UR20', 'UACE20', 'FUNCSTAT20', 'INTPTLAT20', 'INTPTLON20']\n",
      "-- Full column names in blocks_processed after dropping:\n",
      "  STATEFP20\n",
      "  COUNTYFP20\n",
      "  TRACTCE20\n",
      "  BLOCKCE20\n",
      "  GEOID20\n",
      "  ALAND20\n",
      "  AWATER20\n",
      "  HOUSING20\n",
      "  POP20\n",
      "  geometry\n",
      "  BLOCK_ID\n",
      "  area_sm\n",
      "  pop_den\n",
      "  avg_hght\n",
      "  avg_sback\n",
      "  bld_area\n",
      "  bld_cnt\n",
      "  bld_ctsm\n",
      "  bld_prc\n",
      "- Saving roads processed...\n",
      "-- Full column names in roads_processed:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  ref\n",
      "  reversed\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  bridge\n",
      "  junction\n",
      "  width\n",
      "  access\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "-- Dropping extraneous columns from roads_processed: ['ref', 'reversed', 'bridge', 'junction', 'width', 'access']\n",
      "-- Full column names in roads_processed after dropping:\n",
      "  osmid\n",
      "  highway\n",
      "  lanes\n",
      "  name\n",
      "  oneway\n",
      "  length\n",
      "  maxspeed\n",
      "  geometry\n",
      "  road_id\n",
      "  block_ids\n",
      "  overlap_percs\n",
      "  evac_flag\n",
      "  agg_pop\n",
      "  agg_area\n",
      "  agg_ctsm\n",
      "  agg_bldprc\n",
      "  agg_hght\n",
      "  agg_sback\n",
      "-- Trimming values in overlap_percs to 4 decimal places for roads_processed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Local\\Temp\\ipykernel_30644\\3821764773.py:136: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefiles saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael.Barzach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[367222, 367223, 465912, 500640, 500657, 520508, 520635, 520737, 541568, 541569, 541570, 541671, 575979, 591242, 591543, 591545, 591547, 591580, 591608, 591611, 604709, 604835, 604843, 606728, 606753, 606934, 607041, 607042, 607043, 607050, 607052, 608896, 608897, 620980, 620995, 621003, 642476]' of field block_ids has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrames to shapefiles while ensuring the correct geometry column is activated.\n",
    "    Includes functionality to validate geometries only for buildings and trims `overlap_percs` in roads to 4 decimal places.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving subsets to shapefiles...\")\n",
    "\n",
    "    # Suppress warnings about truncated column names for ESRI Shapefiles\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Normalized/laundered field name.*\", category=RuntimeWarning)\n",
    "\n",
    "    def ensure_unique_column_names(df, name):\n",
    "        \"\"\"\n",
    "        Ensure column names in a GeoDataFrame are unique by appending trailing numbers to duplicates.\n",
    "        \"\"\"\n",
    "        duplicates = df.columns[df.columns.duplicated()].unique()\n",
    "        if len(duplicates) > 0:\n",
    "            print(f\"WARNING: Duplicate column names detected in {name}: {duplicates}\")\n",
    "            df = df.rename(columns=lambda x: x[:10])  # Truncate names to 10 characters for ESRI compatibility\n",
    "            # Resolve duplicates by appending a suffix\n",
    "            seen = set()\n",
    "            new_columns = []\n",
    "            for col in df.columns:\n",
    "                if col in seen:\n",
    "                    count = sum([existing.startswith(col) for existing in seen]) + 1\n",
    "                    new_col = f\"{col[:7]}_{count}\"  # Add numeric suffix to resolve duplicates\n",
    "                    print(f\"    Renaming column '{col}' to '{new_col}' to resolve duplication.\")\n",
    "                    new_columns.append(new_col)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "                seen.add(new_columns[-1])\n",
    "            df.columns = new_columns\n",
    "        return df\n",
    "\n",
    "    def print_columns(df, name):\n",
    "        \"\"\"\n",
    "        Print column names in their entirety for a GeoDataFrame.\n",
    "        \"\"\"\n",
    "        print(f\"-- Full column names in {name}:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}\")\n",
    "\n",
    "    def drop_extraneous_columns(df, columns_to_keep, name):\n",
    "        \"\"\"\n",
    "        Drop columns not included in the `columns_to_keep` list, only if they exist in the dataset.\n",
    "        \"\"\"\n",
    "        columns_to_drop = [col for col in df.columns if col not in columns_to_keep]\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Ensure column exists\n",
    "        if columns_to_drop:\n",
    "            print(f\"-- Dropping extraneous columns from {name}: {columns_to_drop}\")\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "        print_columns(df, f\"{name} after dropping\")  # Print columns after dropping\n",
    "        return df\n",
    "\n",
    "    def validate_and_fix_geometries(df, name):\n",
    "        \"\"\"\n",
    "        Validate geometries in a GeoDataFrame, ensuring they are of the correct type.\n",
    "        Filter out invalid geometries or attempt to convert them to `Polygon`/`MultiPolygon`.\n",
    "        Applied **only** to buildings.\n",
    "        \"\"\"\n",
    "        print(f\"-- Validating geometries in {name}...\")\n",
    "        \n",
    "        # Filter valid geometries\n",
    "        valid_types = [\"Polygon\", \"MultiPolygon\"]\n",
    "        \n",
    "        # Separate invalid geometries\n",
    "        invalid_geometries = df[~df.geometry.type.isin(valid_types)]\n",
    "        if len(invalid_geometries) > 0:\n",
    "            print(f\"ERROR: Invalid geometry type detected in {name}. Expected Polygon or MultiPolygon.\")\n",
    "            print(f\"-- Invalid geometries: {len(invalid_geometries)}\")\n",
    "            \n",
    "            # Attempt to convert invalid geometries to polygons\n",
    "            print(\"-- Attempting to convert invalid geometries to valid Polygon/MultiPolygon...\")\n",
    "            def convert_to_polygon(geom):\n",
    "                # Try to buffer as a fix to convert Point/LineString geometries\n",
    "                if geom.is_valid:\n",
    "                    return geom.buffer(0)  # Buffer to create a polygon around invalid geometry\n",
    "                return None  # Skip if geometry cannot be converted\n",
    "                \n",
    "            # Apply conversion\n",
    "            df.loc[~df.geometry.type.isin(valid_types), \"geometry\"] = df.loc[~df.geometry.type.isin(valid_types), \"geometry\"].apply(convert_to_polygon)\n",
    "            \n",
    "            # Remove any geometries that could not be converted\n",
    "            df = df[df.geometry.type.isin(valid_types)]\n",
    "            print(f\"-- After conversion: {len(df)} valid polygons remaining.\")\n",
    "        else:\n",
    "            print(f\"-- All geometries are valid in {name}.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def trim_overlap_percs(df, column_name, name):\n",
    "        \"\"\"\n",
    "        Trim values in the `overlap_percs` field to 4 decimal places.\n",
    "        \"\"\"\n",
    "        print(f\"-- Trimming values in {column_name} to 4 decimal places for {name}...\")\n",
    "        if column_name in df.columns:\n",
    "            df[column_name] = df[column_name].apply(lambda x: [round(value, 4) for value in x])\n",
    "        return df\n",
    "\n",
    "    # Process buildings subset\n",
    "    print(\"- Saving buildings subset...\")\n",
    "    print_columns(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset = drop_extraneous_columns(buildings_subset, columns_to_keep=[\n",
    "        \"geometry\", \"height\", \"setback_ft\", \"build_idx\", \"build_area_sm\", \"height_m\", \"height_ft\"\n",
    "    ], name=\"buildings_subset\")\n",
    "    buildings_subset = validate_and_fix_geometries(buildings_subset, \"buildings_subset\")  # Apply validation/fixing ONLY here\n",
    "    buildings_subset = ensure_unique_column_names(buildings_subset, \"buildings_subset\")\n",
    "    buildings_subset.to_file(f\"{output_dir}/Corpus_Christi_buildings_subset.shp\")\n",
    "\n",
    "    # Process blocks subset\n",
    "    print(\"- Saving blocks processed...\")\n",
    "    print_columns(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed = drop_extraneous_columns(blocks_processed, columns_to_keep=[\n",
    "        \"STATEFP20\", \"COUNTYFP20\", \"TRACTCE20\", \"BLOCKCE20\", \"GEOID20\",\n",
    "        \"ALAND20\", \"AWATER20\", \"HOUSING20\", \"POP20\", \"geometry\", \"BLOCK_ID\", \"area_sm\",\n",
    "        \"pop_den\", \"avg_hght\", \"avg_sback\", \"bld_area\", \"bld_cnt\", \"bld_ctsm\", \"bld_prc\"\n",
    "    ], name=\"blocks_processed\")\n",
    "    blocks_processed = ensure_unique_column_names(blocks_processed, \"blocks_processed\")\n",
    "    blocks_processed.to_file(f\"{output_dir}/Corpus_Christi_blocks_processed.shp\")\n",
    "\n",
    "    # Process roads subset\n",
    "    print(\"- Saving roads processed...\")\n",
    "    print_columns(roads_processed, \"roads_processed\")\n",
    "    roads_processed = drop_extraneous_columns(roads_processed, columns_to_keep=[\n",
    "        \"osmid\", \"highway\", \"lanes\", \"name\", \"oneway\", \"length\", \"maxspeed\", \"geometry\",\n",
    "        \"road_id\", \"agg_pop\", \"agg_area\", \"agg_ctsm\", \"agg_bldprc\", \"agg_hght\", \"agg_sback\",\n",
    "        \"block_ids\", \"overlap_percs\", \"evac_flag\"\n",
    "    ], name=\"roads_processed\")\n",
    "    roads_processed = trim_overlap_percs(roads_processed, \"overlap_percs\", \"roads_processed\")  # Trim `overlap_percs`\n",
    "    roads_processed = ensure_unique_column_names(roads_processed, \"roads_processed\")\n",
    "    roads_processed.to_file(f\"{output_dir}/Corpus_Christi_roads_processed.shp\")\n",
    "\n",
    "    print(\"Shapefiles saved successfully.\")\n",
    "\n",
    "# Save the processed data to shapefiles\n",
    "save_shapefiles(buildings_subset, blocks_processed, roads_processed, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0d494-81e3-4da7-acab-24623d288483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
